[
    {
        "number": 5909,
        "title": "Llama.cpp ./main hangs on line \"llama_new_context_with_model: graph splits (measure): 1\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5909",
        "state": "open",
        "created_at": "2024-03-06T18:12:46Z",
        "user": "wtosborne03",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5906,
        "title": "ggml : use SYS_get_cpu if SYS_getcpu is not defined",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5906",
        "state": "open",
        "created_at": "2024-03-06T16:13:58Z",
        "user": "cebtenzzre",
        "labels": []
    },
    {
        "number": 5905,
        "title": "Mixtral 8x7b QLora not able to convert to gguf after training",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5905",
        "state": "open",
        "created_at": "2024-03-06T15:23:08Z",
        "user": "hammer-mt",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5904,
        "title": "llama.cpp Garbled code",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5904",
        "state": "open",
        "created_at": "2024-03-06T14:43:41Z",
        "user": "dm180",
        "labels": []
    },
    {
        "number": 5902,
        "title": "Support JAISLMHeadModel architecture",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5902",
        "state": "open",
        "created_at": "2024-03-06T11:26:58Z",
        "user": "Omaralsaabi",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5901,
        "title": "[SYCL] fix error when set main gpu to non-zero",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5901",
        "state": "open",
        "created_at": "2024-03-06T09:58:30Z",
        "user": "NeoZhangJianyu",
        "labels": []
    },
    {
        "number": 5899,
        "title": "CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage       CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5899",
        "state": "open",
        "created_at": "2024-03-06T08:39:56Z",
        "user": "start-life",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5898,
        "title": "Free Cublas GPU memory",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5898",
        "state": "open",
        "created_at": "2024-03-06T06:58:00Z",
        "user": "zsogitbe",
        "labels": []
    },
    {
        "number": 5896,
        "title": "server: multimodal - fix misreported prompt and num prompt tokens",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5896",
        "state": "open",
        "created_at": "2024-03-06T04:52:09Z",
        "user": "cjpais",
        "labels": []
    },
    {
        "number": 5891,
        "title": "llama : compute BERT graph with F16 K, V",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5891",
        "state": "open",
        "created_at": "2024-03-05T19:25:35Z",
        "user": "ggerganov",
        "labels": []
    },
    {
        "number": 5890,
        "title": "Feature: A unified configure (and build) process for llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5890",
        "state": "open",
        "created_at": "2024-03-05T17:20:45Z",
        "user": "cebtenzzre",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5887,
        "title": "Training Using WASM ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5887",
        "state": "open",
        "created_at": "2024-03-05T13:52:16Z",
        "user": "saraalrawi",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5886,
        "title": "[SYCL] Add q3_s and q1_s",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5886",
        "state": "open",
        "created_at": "2024-03-05T12:23:27Z",
        "user": "abhilash1910",
        "labels": []
    },
    {
        "number": 5882,
        "title": "server : refactor",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5882",
        "state": "open",
        "created_at": "2024-03-05T09:25:07Z",
        "user": "ggerganov",
        "labels": []
    },
    {
        "number": 5881,
        "title": "Support for torch's Longstorage for Sentance Similiary tasks",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5881",
        "state": "open",
        "created_at": "2024-03-05T08:57:50Z",
        "user": "gevzak",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5880,
        "title": "server: maintain chat completion id for streaming responses",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5880",
        "state": "open",
        "created_at": "2024-03-05T08:41:41Z",
        "user": "mscheong01",
        "labels": []
    },
    {
        "number": 5877,
        "title": "Support speculative decoding in `server` example",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5877",
        "state": "open",
        "created_at": "2024-03-05T02:39:35Z",
        "user": "mscheong01",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5876,
        "title": "Consistent chat completion id in OpenAI compatible chat completion endpoint",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5876",
        "state": "open",
        "created_at": "2024-03-04T23:01:14Z",
        "user": "xyc",
        "labels": [
            "bug",
            "good first issue"
        ]
    },
    {
        "number": 5873,
        "title": "GPU Memory Leak",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5873",
        "state": "open",
        "created_at": "2024-03-04T20:52:20Z",
        "user": "zsogitbe",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5872,
        "title": "Windows 7 support is broken again",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5872",
        "state": "open",
        "created_at": "2024-03-04T20:47:28Z",
        "user": "whoreson",
        "labels": []
    },
    {
        "number": 5870,
        "title": "if use MoE + Ternary, what's happen? ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5870",
        "state": "open",
        "created_at": "2024-03-04T16:06:11Z",
        "user": "qwas982",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5867,
        "title": "IQ3_S: multiplier based code book",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5867",
        "state": "open",
        "created_at": "2024-03-04T07:19:03Z",
        "user": "ikawrakow",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 5865,
        "title": "tips for GPU op profile",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5865",
        "state": "open",
        "created_at": "2024-03-04T03:22:32Z",
        "user": "rnwang04",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5864,
        "title": "Model generated token_id: 0",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5864",
        "state": "open",
        "created_at": "2024-03-04T01:55:25Z",
        "user": "TruongGiangBT",
        "labels": []
    },
    {
        "number": 5863,
        "title": "Server always incorrectly reports 1 for prompt_n, tokens_evaluated, and n_prompt_tokens_processed when using Llava 1.6.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5863",
        "state": "open",
        "created_at": "2024-03-04T01:32:51Z",
        "user": "chigkim",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5859,
        "title": "Provide an example for CMake/CPack variables to create a package",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5859",
        "state": "open",
        "created_at": "2024-03-03T19:39:56Z",
        "user": "raffaeler",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5857,
        "title": "Can you add a windows arm64 support?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5857",
        "state": "open",
        "created_at": "2024-03-03T17:46:13Z",
        "user": "KazbekBrtsiev",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5856,
        "title": "Regressions on IQ3_XXS over time",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5856",
        "state": "open",
        "created_at": "2024-03-03T17:11:32Z",
        "user": "GlasslessPizza",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5854,
        "title": "Server: kvgraphics; log redirection; compound apikey; two minor tweaks",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5854",
        "state": "open",
        "created_at": "2024-03-03T12:21:11Z",
        "user": "pudepiedj",
        "labels": []
    },
    {
        "number": 5852,
        "title": "Question about llama.cpp and llava-cli when used with llava 1.6 for vision:",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5852",
        "state": "open",
        "created_at": "2024-03-03T11:47:12Z",
        "user": "RandomGitUser321",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5850,
        "title": "server: metrics endpoint",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5850",
        "state": "open",
        "created_at": "2024-03-03T08:28:16Z",
        "user": "phymbert",
        "labels": [
            "bug",
            "server/webui"
        ]
    },
    {
        "number": 5848,
        "title": "Multi GPU with Vulkan out of memory issue.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5848",
        "state": "open",
        "created_at": "2024-03-03T03:08:43Z",
        "user": "lastrosade",
        "labels": [
            "bug-unconfirmed",
            "Vulkan"
        ]
    },
    {
        "number": 5846,
        "title": "[CPU Embedding Inference] 2x slower when **Allocated** (not used) memory / RAM is lower.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5846",
        "state": "open",
        "created_at": "2024-03-03T02:45:56Z",
        "user": "PrithivirajDamodaran",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5845,
        "title": "CUDA error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5845",
        "state": "open",
        "created_at": "2024-03-03T02:15:56Z",
        "user": "Vikramardham",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5843,
        "title": "Save/Load Just One Sequence",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5843",
        "state": "open",
        "created_at": "2024-03-03T01:46:36Z",
        "user": "martindevans",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5838,
        "title": "convert-hf: added tokens decoder are not processed",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5838",
        "state": "open",
        "created_at": "2024-03-02T20:31:58Z",
        "user": "dranger003",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5837,
        "title": "server: feature Add Admin key parameter for slots/health/metrics",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5837",
        "state": "open",
        "created_at": "2024-03-02T20:09:58Z",
        "user": "robeyh",
        "labels": []
    },
    {
        "number": 5833,
        "title": "custom calibration data",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5833",
        "state": "open",
        "created_at": "2024-03-02T14:44:44Z",
        "user": "drewskidang",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5831,
        "title": "Issue with Converting llava-med Model to gguf - Error in convert.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5831",
        "state": "open",
        "created_at": "2024-03-02T10:52:29Z",
        "user": "areegtarek",
        "labels": []
    },
    {
        "number": 5828,
        "title": "I fine tuned my model through -ngl,  It doesn't seem to work",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5828",
        "state": "open",
        "created_at": "2024-03-02T03:56:13Z",
        "user": "sohandsome1",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5824,
        "title": "Assume tied weights if lm_head/output weights is missing.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5824",
        "state": "open",
        "created_at": "2024-03-01T20:20:30Z",
        "user": "dmahurin",
        "labels": []
    },
    {
        "number": 5823,
        "title": "persimmon crashes with CUDA: assertion failure `ggml_is_contiguous(src0)`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5823",
        "state": "open",
        "created_at": "2024-03-01T19:27:09Z",
        "user": "cebtenzzre",
        "labels": [
            "bug",
            "model"
        ]
    },
    {
        "number": 5822,
        "title": "How to write a chat template for llama.cpp server?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5822",
        "state": "open",
        "created_at": "2024-03-01T19:04:28Z",
        "user": "AFKler",
        "labels": []
    },
    {
        "number": 5818,
        "title": "quantize to F32/F16/Q8_0 can result in a Q6_K output tensor",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5818",
        "state": "open",
        "created_at": "2024-03-01T16:08:25Z",
        "user": "cebtenzzre",
        "labels": [
            "bug",
            "good first issue"
        ]
    },
    {
        "number": 5816,
        "title": "add @kalomaze's Cubic Sampling with Curve Params",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5816",
        "state": "open",
        "created_at": "2024-03-01T15:13:35Z",
        "user": "igorbarshteyn",
        "labels": []
    },
    {
        "number": 5807,
        "title": "Unable to convert Smaug 72B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5807",
        "state": "open",
        "created_at": "2024-03-01T07:56:24Z",
        "user": "schmorp",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5802,
        "title": "Cannot load model with unicode character in path",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5802",
        "state": "open",
        "created_at": "2024-02-29T22:24:37Z",
        "user": "zbrkic",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5801,
        "title": "Inconsistent Bert Embedding output from embedding.cpp vs llama.cpp server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5801",
        "state": "open",
        "created_at": "2024-02-29T19:10:06Z",
        "user": "tybalex",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5800,
        "title": "Models with activation beacon",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5800",
        "state": "open",
        "created_at": "2024-02-29T19:02:38Z",
        "user": "Bearsaerker",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5798,
        "title": "Models without Vocabulary",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5798",
        "state": "open",
        "created_at": "2024-02-29T14:37:52Z",
        "user": "Xarbirus",
        "labels": []
    },
    {
        "number": 5797,
        "title": "Some logging output not captured with `llama_log_set`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5797",
        "state": "open",
        "created_at": "2024-02-29T13:52:08Z",
        "user": "martindevans",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5793,
        "title": "Server: reuse cached tokens for shifted prompt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5793",
        "state": "open",
        "created_at": "2024-02-29T10:05:43Z",
        "user": "ngxson",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5791,
        "title": "Differences between cgraph->leafs and cgraph->nodes?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5791",
        "state": "open",
        "created_at": "2024-02-29T08:48:52Z",
        "user": "VincentXWD",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5790,
        "title": "Why does my memory keep showing 3%?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5790",
        "state": "open",
        "created_at": "2024-02-29T03:31:42Z",
        "user": "1823616178",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5788,
        "title": "server: index.html issue",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5788",
        "state": "open",
        "created_at": "2024-02-29T02:12:05Z",
        "user": "tdspotts",
        "labels": []
    },
    {
        "number": 5783,
        "title": "Unexpected embeddings using GritLM",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5783",
        "state": "open",
        "created_at": "2024-02-28T22:19:56Z",
        "user": "dranger003",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5782,
        "title": "On certain AMD CPU, AVX512 got enabled with `-march=native` flag, there's no way to disable usage of avx512 at the moment",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5782",
        "state": "open",
        "created_at": "2024-02-28T21:58:20Z",
        "user": "wsxiaoys",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5781,
        "title": "Enable CORS requests on all routes",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5781",
        "state": "open",
        "created_at": "2024-02-28T21:38:29Z",
        "user": "StrangeBytesDev",
        "labels": []
    },
    {
        "number": 5780,
        "title": "Arm AArch64: optimized GEMV and GEMM kernels for q4_0_q8_0, and q8_0_q8_0 quantization",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5780",
        "state": "open",
        "created_at": "2024-02-28T21:08:47Z",
        "user": "Dibakar",
        "labels": []
    },
    {
        "number": 5776,
        "title": "server: error handling",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5776",
        "state": "open",
        "created_at": "2024-02-28T18:38:26Z",
        "user": "z80maniac",
        "labels": []
    },
    {
        "number": 5775,
        "title": "SLICEGPT - Reduce Compute significantly by deleting rows & columns",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5775",
        "state": "open",
        "created_at": "2024-02-28T18:21:24Z",
        "user": "Alumniminium",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5769,
        "title": "CMAKE_OSX_ARCHITECTURES processing",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5769",
        "state": "open",
        "created_at": "2024-02-28T15:03:46Z",
        "user": "Xarbirus",
        "labels": []
    },
    {
        "number": 5765,
        "title": "server : add \"token healing\" support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5765",
        "state": "open",
        "created_at": "2024-02-28T12:10:30Z",
        "user": "CyberShadow",
        "labels": [
            "enhancement",
            "good first issue",
            "server/webui"
        ]
    },
    {
        "number": 5763,
        "title": "llama : add T5 (encoder-decoder) support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5763",
        "state": "open",
        "created_at": "2024-02-28T11:24:59Z",
        "user": "ggerganov",
        "labels": [
            "model"
        ]
    },
    {
        "number": 5762,
        "title": "Clean up server code",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5762",
        "state": "open",
        "created_at": "2024-02-28T10:32:39Z",
        "user": "ngxson",
        "labels": [
            "enhancement",
            "good first issue"
        ]
    },
    {
        "number": 5761,
        "title": "Support BitNet b1.58 ternary models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5761",
        "state": "open",
        "created_at": "2024-02-28T09:41:38Z",
        "user": "igorbarshteyn",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5759,
        "title": "Model replicas on multiple gpus on server?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5759",
        "state": "open",
        "created_at": "2024-02-28T06:22:05Z",
        "user": "duykhanhbk",
        "labels": []
    },
    {
        "number": 5750,
        "title": "[SYCL] Update unsupported quantization types",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5750",
        "state": "open",
        "created_at": "2024-02-27T12:36:46Z",
        "user": "AidanBeltonS",
        "labels": []
    },
    {
        "number": 5748,
        "title": "Build fail - implicit declaration of function \u2018vld1q_u8_x2\u2019 on Armv7-linux with the latest release",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5748",
        "state": "open",
        "created_at": "2024-02-27T09:11:46Z",
        "user": "svilupp",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5745,
        "title": "build(python): Package scripts with pip-0517 compliance",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5745",
        "state": "open",
        "created_at": "2024-02-27T06:34:38Z",
        "user": "ditsuke",
        "labels": []
    },
    {
        "number": 5743,
        "title": "Build failure for Intel CPU on windows with MKL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5743",
        "state": "open",
        "created_at": "2024-02-27T00:34:34Z",
        "user": "ratnampa",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5741,
        "title": "WIP: Add model `merge` example",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5741",
        "state": "open",
        "created_at": "2024-02-26T22:11:32Z",
        "user": "ngxson",
        "labels": [
            "help wanted"
        ]
    },
    {
        "number": 5739,
        "title": "Vulkan for Android",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5739",
        "state": "open",
        "created_at": "2024-02-26T16:56:45Z",
        "user": "riverzhou",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5738,
        "title": "[SYCL] Add support for SYCL Nvidia target",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5738",
        "state": "open",
        "created_at": "2024-02-26T16:49:54Z",
        "user": "AidanBeltonS",
        "labels": []
    },
    {
        "number": 5736,
        "title": "[SYCL] Split device code per kernel",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5736",
        "state": "open",
        "created_at": "2024-02-26T15:30:05Z",
        "user": "AidanBeltonS",
        "labels": []
    },
    {
        "number": 5732,
        "title": "Context length documentation confusion",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5732",
        "state": "open",
        "created_at": "2024-02-26T14:08:11Z",
        "user": "mprudra",
        "labels": []
    },
    {
        "number": 5730,
        "title": "main: port basic LLaVA (multimodal) support from llava-cli",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5730",
        "state": "open",
        "created_at": "2024-02-26T12:47:57Z",
        "user": "Nekotekina",
        "labels": []
    },
    {
        "number": 5727,
        "title": "main : failed to eval",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5727",
        "state": "open",
        "created_at": "2024-02-26T09:18:26Z",
        "user": "ylsdamxssjxxdd",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5719,
        "title": "llama : add llama_kv_cache_compress",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5719",
        "state": "open",
        "created_at": "2024-02-25T20:23:20Z",
        "user": "ggerganov",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 5717,
        "title": "Fine tune data structure ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5717",
        "state": "open",
        "created_at": "2024-02-25T17:40:45Z",
        "user": "Luigi-Ottoboni",
        "labels": []
    },
    {
        "number": 5716,
        "title": "Update to latest \"ggml\" version",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5716",
        "state": "open",
        "created_at": "2024-02-25T17:39:43Z",
        "user": "paulocoutinhox",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5712,
        "title": "HIPBlas AMD Radeon VII under windows. Quits out after \"ggml_backend_sched: failed to allocate graph, reserving\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5712",
        "state": "open",
        "created_at": "2024-02-25T13:02:14Z",
        "user": "moozoo64",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5706,
        "title": "Gemma models quantized using llamacpp not working in lm studio",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5706",
        "state": "open",
        "created_at": "2024-02-25T05:12:46Z",
        "user": "rombodawg",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5701,
        "title": "ROCm(6.0) benchmark failed",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5701",
        "state": "open",
        "created_at": "2024-02-24T14:01:45Z",
        "user": "riverzhou",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5695,
        "title": "Server: add support for \"tool_calls\" (MeetKai/functionary model)",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5695",
        "state": "open",
        "created_at": "2024-02-23T23:53:44Z",
        "user": "ngxson",
        "labels": []
    },
    {
        "number": 5694,
        "title": "Fix a Typo Which Causes It Unable to Compile with Old Glibc",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5694",
        "state": "open",
        "created_at": "2024-02-23T22:10:55Z",
        "user": "PeronGH",
        "labels": []
    },
    {
        "number": 5693,
        "title": "`-ctk q8_0` or `q4_0` cuts inference speed by half",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5693",
        "state": "open",
        "created_at": "2024-02-23T20:38:16Z",
        "user": "Artefact2",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5692,
        "title": "llama : add `retrieval` example",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5692",
        "state": "open",
        "created_at": "2024-02-23T18:46:29Z",
        "user": "ggerganov",
        "labels": [
            "good first issue",
            "examples"
        ]
    },
    {
        "number": 5681,
        "title": "nix: consider moving outputs to legacyPackages for lazier evaluation",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5681",
        "state": "open",
        "created_at": "2024-02-23T11:17:41Z",
        "user": "SomeoneSerge",
        "labels": [
            "nix"
        ]
    },
    {
        "number": 5680,
        "title": "Finetune Codellama 13B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5680",
        "state": "open",
        "created_at": "2024-02-23T11:08:13Z",
        "user": "sanipanwala",
        "labels": []
    },
    {
        "number": 5679,
        "title": "llama : switch to floating-point token positions",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5679",
        "state": "open",
        "created_at": "2024-02-23T10:35:39Z",
        "user": "ggerganov",
        "labels": []
    },
    {
        "number": 5677,
        "title": "lookahead: set parameter W,N,G from environment variable",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5677",
        "state": "open",
        "created_at": "2024-02-23T07:38:53Z",
        "user": "pomoke",
        "labels": []
    },
    {
        "number": 5675,
        "title": "P-Step Truncation Sampling",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5675",
        "state": "open",
        "created_at": "2024-02-23T06:24:58Z",
        "user": "p-e-w",
        "labels": [
            "need feedback"
        ]
    },
    {
        "number": 5674,
        "title": "[SYCL] Support newer non linear quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5674",
        "state": "open",
        "created_at": "2024-02-23T05:40:16Z",
        "user": "akarshanbiswas",
        "labels": [
            "help wanted",
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5673,
        "title": "Update build.zig",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5673",
        "state": "open",
        "created_at": "2024-02-23T04:58:06Z",
        "user": "hazelnutcloud",
        "labels": []
    },
    {
        "number": 5671,
        "title": "Add support for Gemma models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5671",
        "state": "open",
        "created_at": "2024-02-22T22:27:44Z",
        "user": "AIWithShrey",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5669,
        "title": "Freshly converted PLaMo fails assertion: vocab.id_to_token.size() == vocab.token_to_id.size()",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5669",
        "state": "open",
        "created_at": "2024-02-22T19:32:46Z",
        "user": "cebtenzzre",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5664,
        "title": "build(nix): Package gguf-py",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5664",
        "state": "open",
        "created_at": "2024-02-22T15:37:01Z",
        "user": "ditsuke",
        "labels": []
    },
    {
        "number": 5660,
        "title": "Remove logging chats",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5660",
        "state": "open",
        "created_at": "2024-02-22T09:04:29Z",
        "user": "aiaicode",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5659,
        "title": "Add UC Berkleys Large World Models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5659",
        "state": "open",
        "created_at": "2024-02-22T08:30:06Z",
        "user": "Venkman42",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5653,
        "title": "llama : fix K-shift with quantized K (wip)",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5653",
        "state": "open",
        "created_at": "2024-02-21T23:33:19Z",
        "user": "slaren",
        "labels": []
    },
    {
        "number": 5652,
        "title": "llama_kv_cache_seq_shift does not work with cache type q4_0",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5652",
        "state": "open",
        "created_at": "2024-02-21T23:14:10Z",
        "user": "ngxson",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5643,
        "title": "Fix MSVC compile errors",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5643",
        "state": "open",
        "created_at": "2024-02-21T18:49:42Z",
        "user": "uextm",
        "labels": []
    },
    {
        "number": 5641,
        "title": "Vulkan: Double Vram allocation with Gemma models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5641",
        "state": "open",
        "created_at": "2024-02-21T17:37:41Z",
        "user": "stduhpf",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5630,
        "title": "cmake build LLAMA_CUBLAS error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5630",
        "state": "open",
        "created_at": "2024-02-21T12:01:23Z",
        "user": "jerrymatjila",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5626,
        "title": "Why skip rope_freqs, attn_rot_embd modules when serialization to gguf?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5626",
        "state": "open",
        "created_at": "2024-02-21T08:19:14Z",
        "user": "lifelongeeek",
        "labels": []
    },
    {
        "number": 5621,
        "title": "Android OpenCL question ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5621",
        "state": "open",
        "created_at": "2024-02-21T03:28:39Z",
        "user": "anthonyliot",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5620,
        "title": "Implementation of speculative streaming ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5620",
        "state": "open",
        "created_at": "2024-02-21T02:23:09Z",
        "user": "NickNickGo",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5619,
        "title": "llama.cpp with mistral-7b-instruct-v0.2.Q5_K_M.gguf performance comparison between Intel CPU, nVIDIA GPU and Apple M1/M2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5619",
        "state": "open",
        "created_at": "2024-02-21T01:51:56Z",
        "user": "a-b-n-e-o",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5615,
        "title": "convert : get general.name from model dir, not its parent",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5615",
        "state": "open",
        "created_at": "2024-02-20T18:28:09Z",
        "user": "cebtenzzre",
        "labels": []
    },
    {
        "number": 5613,
        "title": "Attempt to fix pre-tokenizer",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5613",
        "state": "open",
        "created_at": "2024-02-20T16:56:39Z",
        "user": "bobqianic",
        "labels": []
    },
    {
        "number": 5612,
        "title": "[RFC] common, server : add top-a sampler",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5612",
        "state": "open",
        "created_at": "2024-02-20T15:09:11Z",
        "user": "Artefact2",
        "labels": []
    },
    {
        "number": 5610,
        "title": "Mixtral 8x7b lora conversion support ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5610",
        "state": "open",
        "created_at": "2024-02-20T14:17:30Z",
        "user": "TaupoB",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5609,
        "title": "docker\u8fd0\u884cserver\u7684\u65f6\u5019\u5f02\u5e38\u9000\u51fa",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5609",
        "state": "open",
        "created_at": "2024-02-20T10:26:45Z",
        "user": "AppleJunJiang",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5608,
        "title": "llama : update the convert-llama2c-to-ggml example",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5608",
        "state": "open",
        "created_at": "2024-02-20T09:50:31Z",
        "user": "ggerganov",
        "labels": [
            "good first issue",
            "testing",
            "refactoring"
        ]
    },
    {
        "number": 5602,
        "title": "Issue with convert-lora-to-ggml.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5602",
        "state": "open",
        "created_at": "2024-02-20T01:34:46Z",
        "user": "Gunnar-Stunnar",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5601,
        "title": "/parallel often produces truncated outputs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5601",
        "state": "open",
        "created_at": "2024-02-20T00:48:38Z",
        "user": "k-gyuhak",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5600,
        "title": "Qwen1.5-1.8B-Chat conversion to gguf causes an error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5600",
        "state": "open",
        "created_at": "2024-02-20T00:26:04Z",
        "user": "anaivebird",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5599,
        "title": "Constrained decoding with BNF grammar fails to work with some tokens",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5599",
        "state": "open",
        "created_at": "2024-02-19T22:27:45Z",
        "user": "KE7",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5596,
        "title": "Orion14b conversion issue",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5596",
        "state": "open",
        "created_at": "2024-02-19T20:31:26Z",
        "user": "sorasoras",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5592,
        "title": "llama cpp server not doing parallel inference for llava when using flags -np and -cb",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5592",
        "state": "open",
        "created_at": "2024-02-19T18:16:43Z",
        "user": "tellsiddh",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5588,
        "title": "Server: add function calling API",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5588",
        "state": "open",
        "created_at": "2024-02-19T13:47:28Z",
        "user": "ngxson",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5586,
        "title": "Please add support for Entropy-ABF",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5586",
        "state": "open",
        "created_at": "2024-02-19T12:00:47Z",
        "user": "shamio",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5583,
        "title": "Vulkan with multimodal models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5583",
        "state": "open",
        "created_at": "2024-02-19T10:39:56Z",
        "user": "ddpasa",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5576,
        "title": "Fix 2 cuda memory leaks in ggml-cuda.cu",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5576",
        "state": "open",
        "created_at": "2024-02-19T01:17:22Z",
        "user": "dhiltgen",
        "labels": []
    },
    {
        "number": 5568,
        "title": "llama : rename n_ctx to kv_size",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5568",
        "state": "open",
        "created_at": "2024-02-18T20:15:27Z",
        "user": "ggerganov",
        "labels": [
            "breaking change",
            "refactoring"
        ]
    },
    {
        "number": 5563,
        "title": "\"Inference Issue with llama.cpp Using Custom Converted qwen1.5 Weights\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5563",
        "state": "open",
        "created_at": "2024-02-18T10:29:22Z",
        "user": "wanshichenguang",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5562,
        "title": "error: implicit declaration of function \u2018getcpu\u2019; did you mean \u2018getopt\u2019? [-Werror=implicit-function-declaration]",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5562",
        "state": "open",
        "created_at": "2024-02-18T09:18:26Z",
        "user": "thistleknot",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5561,
        "title": "Penalty threshold: A mechanism for improving repetition penalties",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5561",
        "state": "open",
        "created_at": "2024-02-18T07:41:53Z",
        "user": "p-e-w",
        "labels": []
    },
    {
        "number": 5560,
        "title": "Vulkan not support ALIBI",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5560",
        "state": "open",
        "created_at": "2024-02-18T03:17:46Z",
        "user": "riverzhou",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5559,
        "title": "Error when converting safe tensors to gguf",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5559",
        "state": "open",
        "created_at": "2024-02-18T03:15:35Z",
        "user": "ieatbeansbruh",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5556,
        "title": "Windows Defender detects Virus in Multiple zip files.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5556",
        "state": "open",
        "created_at": "2024-02-17T20:42:18Z",
        "user": "erikpro008",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5554,
        "title": "CUDA: switch tile sizes based on binary version",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5554",
        "state": "open",
        "created_at": "2024-02-17T20:32:04Z",
        "user": "JohannesGaessler",
        "labels": []
    },
    {
        "number": 5552,
        "title": "ggml_vulkan: Error: Missing op: ARGSORT",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5552",
        "state": "open",
        "created_at": "2024-02-17T19:56:17Z",
        "user": "Rotoslider",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5547,
        "title": "SYCL build failed",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5547",
        "state": "open",
        "created_at": "2024-02-17T11:48:36Z",
        "user": "DDXDB",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5545,
        "title": "Llava generates gibberish on the Vulkan backend",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5545",
        "state": "open",
        "created_at": "2024-02-17T09:38:24Z",
        "user": "stduhpf",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5541,
        "title": "Segmentation Fault 11 on M2 Ultra 192GB when offloading more than 110GB into Metal",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5541",
        "state": "open",
        "created_at": "2024-02-17T05:41:19Z",
        "user": "SomeOddCodeGuy",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5540,
        "title": "Kumpute not offloading any gpu layers",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5540",
        "state": "open",
        "created_at": "2024-02-16T19:55:23Z",
        "user": "userbox020",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5539,
        "title": "prompt parameter in ./server api not working properly ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5539",
        "state": "open",
        "created_at": "2024-02-16T19:43:34Z",
        "user": "14041999ai",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5537,
        "title": "error: implicit declaration of function \u2018getcpu\u2019",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5537",
        "state": "open",
        "created_at": "2024-02-16T15:28:59Z",
        "user": "davesworking",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5535,
        "title": "common, examples, llama : optimize using reserve if possible",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5535",
        "state": "open",
        "created_at": "2024-02-16T14:01:15Z",
        "user": "GermanAizek",
        "labels": []
    },
    {
        "number": 5534,
        "title": "OpenBLAS Linux : more thread = less performances",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5534",
        "state": "open",
        "created_at": "2024-02-16T13:35:08Z",
        "user": "SuperUserNameMan",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5524,
        "title": "Wrong detection count threads in NUMA configuration in Windows",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5524",
        "state": "open",
        "created_at": "2024-02-16T11:04:01Z",
        "user": "GermanAizek",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5523,
        "title": "Today's llama fails to compile for android on windows",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5523",
        "state": "open",
        "created_at": "2024-02-16T11:02:08Z",
        "user": "ValleZ",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5520,
        "title": "Why are the results different?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5520",
        "state": "open",
        "created_at": "2024-02-16T06:49:58Z",
        "user": "giocafe",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5519,
        "title": "HipBlas with multiple Architectures ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5519",
        "state": "open",
        "created_at": "2024-02-16T05:49:45Z",
        "user": "userbox020",
        "labels": []
    },
    {
        "number": 5513,
        "title": "[SYCL] GGML_ASSERT issue when running llama.cpp with SYCL on A770",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5513",
        "state": "open",
        "created_at": "2024-02-15T17:52:38Z",
        "user": "aahouzi",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5511,
        "title": "examples : constantify lambda variables",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5511",
        "state": "open",
        "created_at": "2024-02-15T14:02:01Z",
        "user": "GermanAizek",
        "labels": []
    },
    {
        "number": 5510,
        "title": "Suspicious multiplier parameter in benchmark-matmult",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5510",
        "state": "open",
        "created_at": "2024-02-15T13:36:35Z",
        "user": "GermanAizek",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5508,
        "title": "Stricter definition constantify for C compiler",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5508",
        "state": "open",
        "created_at": "2024-02-15T13:26:06Z",
        "user": "GermanAizek",
        "labels": []
    },
    {
        "number": 5503,
        "title": "configure prints 'Unknown architecture' on FreeBSD 14.0 amd64",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5503",
        "state": "open",
        "created_at": "2024-02-15T11:28:38Z",
        "user": "yurivict",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5499,
        "title": "phi-2/LLMA2 gpu accerleration of pre-attention layer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5499",
        "state": "open",
        "created_at": "2024-02-15T06:47:58Z",
        "user": "pure-water",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5498,
        "title": "Some memory management bugs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5498",
        "state": "open",
        "created_at": "2024-02-14T23:39:20Z",
        "user": "An0nie",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5497,
        "title": "Server system prompt parameter bug",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5497",
        "state": "open",
        "created_at": "2024-02-14T23:34:02Z",
        "user": "An0nie",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5492,
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5492",
        "state": "open",
        "created_at": "2024-02-14T16:35:42Z",
        "user": "sorasoras",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5486,
        "title": "Poor performance with unquantized FP16 models on Vulkan backend",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5486",
        "state": "open",
        "created_at": "2024-02-14T07:58:59Z",
        "user": "deiteris",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5485,
        "title": "fix(server): infinite loop to inference",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5485",
        "state": "open",
        "created_at": "2024-02-13T23:44:38Z",
        "user": "snowyu",
        "labels": []
    },
    {
        "number": 5481,
        "title": "\"Illegal instruction\" on armv7/raspberry pi 2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5481",
        "state": "open",
        "created_at": "2024-02-13T17:21:01Z",
        "user": "rchan96",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5480,
        "title": "Low performance with Sycl Backend",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5480",
        "state": "open",
        "created_at": "2024-02-13T16:49:52Z",
        "user": "chsasank",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5479,
        "title": "lookup: complement data from context with general text statistics",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5479",
        "state": "open",
        "created_at": "2024-02-13T13:33:50Z",
        "user": "JohannesGaessler",
        "labels": []
    },
    {
        "number": 5476,
        "title": "Integrate ZLUDA for AMD CUDA",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5476",
        "state": "open",
        "created_at": "2024-02-13T09:40:29Z",
        "user": "GermanAizek",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5467,
        "title": " SYCL backend error PI_ERROR_INVALID_WORK_GROUP_SIZE on iGPU UHD 770",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5467",
        "state": "open",
        "created_at": "2024-02-12T18:33:25Z",
        "user": "fakezeta",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5464,
        "title": "Deepseek coder merge",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5464",
        "state": "open",
        "created_at": "2024-02-12T12:22:53Z",
        "user": "jaggzh",
        "labels": [
            "help wanted",
            "good first issue"
        ]
    },
    {
        "number": 5463,
        "title": "not able to convert custom finetune ohi2 model into gguf,  ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5463",
        "state": "open",
        "created_at": "2024-02-12T12:17:56Z",
        "user": "akashAD98",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5462,
        "title": "lookup: use hashmaps, select most frequent tokens, abort draft early if no good candidates",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5462",
        "state": "open",
        "created_at": "2024-02-12T11:48:14Z",
        "user": "JohannesGaessler",
        "labels": []
    },
    {
        "number": 5461,
        "title": "general.name wrong and unchangeable",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5461",
        "state": "open",
        "created_at": "2024-02-12T11:44:08Z",
        "user": "StefanDanielSchwarz",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5459,
        "title": "qwen 1.5 Beta 1.8B  output incoherently",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5459",
        "state": "open",
        "created_at": "2024-02-12T10:44:12Z",
        "user": "sorasoras",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5447,
        "title": "Get chat_template from a server endpoint.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5447",
        "state": "open",
        "created_at": "2024-02-10T23:27:50Z",
        "user": "lastrosade",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5445,
        "title": "GGML_ASSERT with Vulkan backend and Mixtral 8x7B model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5445",
        "state": "open",
        "created_at": "2024-02-10T22:27:56Z",
        "user": "deiteris",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5442,
        "title": "imatrix stops at \"compute_imatrix: tokenizing the input ..\"?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5442",
        "state": "open",
        "created_at": "2024-02-10T17:35:35Z",
        "user": "PhoenixtheII",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5441,
        "title": "Vulkan Device memory allocation failed (ErrorOutOfDeviceMemory )",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5441",
        "state": "open",
        "created_at": "2024-02-10T17:34:34Z",
        "user": "Eliastrt",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5439,
        "title": "Unable to build llama.cpp on Intel DevCloud",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5439",
        "state": "open",
        "created_at": "2024-02-10T12:49:32Z",
        "user": "AlexFierro9",
        "labels": [
            "bug-unconfirmed",
            "Intel GPU"
        ]
    },
    {
        "number": 5438,
        "title": "Trying to add main --no-prompt-echo",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5438",
        "state": "open",
        "created_at": "2024-02-10T09:51:14Z",
        "user": "jaggzh",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5436,
        "title": "Memory leak in Mac Metal ggml_metal_graph_compute",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5436",
        "state": "open",
        "created_at": "2024-02-10T05:45:46Z",
        "user": "irbull",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5429,
        "title": "Add CUDA option to use the max release threshold for the default memory pool",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5429",
        "state": "open",
        "created_at": "2024-02-09T11:01:47Z",
        "user": "YavorGIvanov",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 5413,
        "title": "Fuse matrix multiplication + SiLU",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5413",
        "state": "open",
        "created_at": "2024-02-08T13:34:25Z",
        "user": "JohannesGaessler",
        "labels": []
    },
    {
        "number": 5410,
        "title": "GPU Performance Data Point via Vulkan ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5410",
        "state": "open",
        "created_at": "2024-02-08T10:58:07Z",
        "user": "pure-water",
        "labels": [
            "enhancement",
            "Vulkan"
        ]
    },
    {
        "number": 5408,
        "title": "Add support for OLMo",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5408",
        "state": "open",
        "created_at": "2024-02-08T10:45:06Z",
        "user": "lastrosade",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5406,
        "title": "Problem connecting VSCode (Continue) to the server LlamaCpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5406",
        "state": "open",
        "created_at": "2024-02-08T10:27:26Z",
        "user": "mike2003",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5403,
        "title": "convert models to gguf: explain how to implement additional model architecture ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5403",
        "state": "open",
        "created_at": "2024-02-08T06:58:05Z",
        "user": "zwilch",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5398,
        "title": "Static lookup decoding",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5398",
        "state": "open",
        "created_at": "2024-02-07T20:46:05Z",
        "user": "JohannesGaessler",
        "labels": [
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 5396,
        "title": "I found a regression in speed with latest update with WestSeverus 7B DPO GGUF",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5396",
        "state": "open",
        "created_at": "2024-02-07T19:23:54Z",
        "user": "sprappcom",
        "labels": [
            "need more info"
        ]
    },
    {
        "number": 5390,
        "title": "[Request/Enhancment]1-bit quants",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5390",
        "state": "open",
        "created_at": "2024-02-07T13:21:01Z",
        "user": "benxh1995",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5387,
        "title": "I created some CSS for a slighly different design of the web UI ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5387",
        "state": "open",
        "created_at": "2024-02-07T11:45:12Z",
        "user": "flatsiedatsie",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5385,
        "title": "fix bug make prompt with image always being default",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5385",
        "state": "open",
        "created_at": "2024-02-07T08:46:33Z",
        "user": "DrewZt",
        "labels": [
            "need feedback"
        ]
    },
    {
        "number": 5380,
        "title": "Vulkan backend does not reload model into vram if kicked out by other uses. Massive slowdown.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5380",
        "state": "open",
        "created_at": "2024-02-07T03:45:05Z",
        "user": "superkuh",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5376,
        "title": "Missing ggml-metal.metal file during shared library build",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5376",
        "state": "open",
        "created_at": "2024-02-06T22:04:27Z",
        "user": "hankluo6",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5372,
        "title": "Documentation: Host sample updated for IPv4+IPv6.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5372",
        "state": "open",
        "created_at": "2024-02-06T19:21:16Z",
        "user": "jboero",
        "labels": []
    },
    {
        "number": 5369,
        "title": "GBNF \"regex\" character range e.g. [a-z]{min, max}",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5369",
        "state": "open",
        "created_at": "2024-02-06T17:08:47Z",
        "user": "SaulMoonves",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5365,
        "title": "Support for Sparse MoE models like Camelidae and Sparsetral",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5365",
        "state": "open",
        "created_at": "2024-02-06T15:00:12Z",
        "user": "candre23",
        "labels": [
            "enhancement",
            "good first issue"
        ]
    },
    {
        "number": 5364,
        "title": "Implementation of Exponential Function ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5364",
        "state": "open",
        "created_at": "2024-02-06T14:41:53Z",
        "user": "saraalrawi",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5359,
        "title": "Strange output when running mixtral",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5359",
        "state": "open",
        "created_at": "2024-02-06T10:00:26Z",
        "user": "sirus20x6",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5357,
        "title": "Add Nv/AMD sycl target build cmd",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5357",
        "state": "open",
        "created_at": "2024-02-06T08:36:18Z",
        "user": "abhilash1910",
        "labels": []
    },
    {
        "number": 5356,
        "title": "Vulkan generated targets and shader organization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5356",
        "state": "open",
        "created_at": "2024-02-06T02:30:51Z",
        "user": "bandoti",
        "labels": [
            "enhancement",
            "Vulkan"
        ]
    },
    {
        "number": 5355,
        "title": "clBLAST builds only output \"######...\" regression since the end of December 2023 (CPU still good, old commit clBLAST still good)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5355",
        "state": "open",
        "created_at": "2024-02-06T02:01:17Z",
        "user": "superkuh",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5350,
        "title": "Uknown format .bin",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5350",
        "state": "open",
        "created_at": "2024-02-05T19:35:49Z",
        "user": "ente0v1",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5345,
        "title": "Can't find config.json",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5345",
        "state": "open",
        "created_at": "2024-02-05T13:46:58Z",
        "user": "felipeblin",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5344,
        "title": "After building the model successfully using make command when i tried to run the model for inference getting error model load failed why?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5344",
        "state": "open",
        "created_at": "2024-02-05T13:43:43Z",
        "user": "amitjaiswal396",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5340,
        "title": "Documentation: Server Example Load Balancing Architecture",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5340",
        "state": "open",
        "created_at": "2024-02-05T10:36:30Z",
        "user": "jboero",
        "labels": []
    },
    {
        "number": 5337,
        "title": "Server: Slow prompt processing speed",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5337",
        "state": "open",
        "created_at": "2024-02-05T09:22:46Z",
        "user": "Dampfinchen",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5336,
        "title": "Server: 1 token prompt eval with multimodal",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5336",
        "state": "open",
        "created_at": "2024-02-05T09:15:30Z",
        "user": "Borobo",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5335,
        "title": "response dont see underscore ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5335",
        "state": "open",
        "created_at": "2024-02-05T08:43:11Z",
        "user": "Tasarinan",
        "labels": [
            "need more info",
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5333,
        "title": "Simple chat templates",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5333",
        "state": "open",
        "created_at": "2024-02-05T08:35:30Z",
        "user": "auriocus",
        "labels": []
    },
    {
        "number": 5331,
        "title": "When I tried to convert the Qwen-VL-chat model to gguf, an error occurred: `Can not map tensor \u2018transformer.visual.positional_embedding\u2019. What is the reason?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5331",
        "state": "open",
        "created_at": "2024-02-05T06:37:00Z",
        "user": "bingal",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5328,
        "title": "llama : support Mamba Selective State Space Models",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5328",
        "state": "open",
        "created_at": "2024-02-05T01:09:14Z",
        "user": "compilade",
        "labels": []
    },
    {
        "number": 5324,
        "title": "Dual GPU performance regression After #4606",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5324",
        "state": "open",
        "created_at": "2024-02-04T18:27:32Z",
        "user": "Ph0rk0z",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5323,
        "title": "Vulkan backend performance is relatively slower at certain quants",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5323",
        "state": "open",
        "created_at": "2024-02-04T15:33:51Z",
        "user": "Nindaleth",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5317,
        "title": "Please support bert based models.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5317",
        "state": "open",
        "created_at": "2024-02-04T04:56:06Z",
        "user": "superchargez",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5316,
        "title": "Centos9 \u7f16\u8bd1\u63d0\u793aunsupported instruction `vpdpbusd'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5316",
        "state": "open",
        "created_at": "2024-02-04T01:40:52Z",
        "user": "lxh111",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5314,
        "title": "Completion results change with the amount of memory available on a Mac?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5314",
        "state": "open",
        "created_at": "2024-02-03T21:08:22Z",
        "user": "pgorzelany",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5312,
        "title": "Building with NDK",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5312",
        "state": "open",
        "created_at": "2024-02-03T18:06:30Z",
        "user": "Jeximo",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5308,
        "title": "Question: does n_predict have an unreasonably large default value?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5308",
        "state": "open",
        "created_at": "2024-02-03T15:18:17Z",
        "user": "channeladam",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5306,
        "title": "Impossible to quantize llamantino-2 70b to gguf",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5306",
        "state": "open",
        "created_at": "2024-02-03T14:53:01Z",
        "user": "ivanfioravanti",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5299,
        "title": "Vulkan @ ARM V8 architecture",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5299",
        "state": "open",
        "created_at": "2024-02-03T09:32:11Z",
        "user": "pure-water",
        "labels": []
    },
    {
        "number": 5296,
        "title": "Add Feature Temp-Lora",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5296",
        "state": "open",
        "created_at": "2024-02-03T04:51:02Z",
        "user": "limoncc",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5292,
        "title": "Coalescence: Potential large speedups in generation for constrained grammars",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5292",
        "state": "open",
        "created_at": "2024-02-02T21:12:16Z",
        "user": "Azeirah",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5290,
        "title": "Faster IQ3_XSS AVX2 text generation",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5290",
        "state": "open",
        "created_at": "2024-02-02T17:58:16Z",
        "user": "Dampfinchen",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5280,
        "title": "AMD ROCm problem: GPU is constantly running at 100%",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5280",
        "state": "open",
        "created_at": "2024-02-02T09:31:30Z",
        "user": "MichaelFomenko",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5279,
        "title": "SAM compatibility with convert.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5279",
        "state": "open",
        "created_at": "2024-02-02T09:01:28Z",
        "user": "JPonsa",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5278,
        "title": "Issues of converting safetensors to gguf with convert.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5278",
        "state": "open",
        "created_at": "2024-02-02T08:44:45Z",
        "user": "592319702",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5276,
        "title": "MiniCPM 2b model support?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5276",
        "state": "open",
        "created_at": "2024-02-02T08:06:39Z",
        "user": "KnutJaegersberg",
        "labels": [
            "enhancement",
            "good first issue"
        ]
    },
    {
        "number": 5272,
        "title": "Excessively slow prompt processing time with 70B partially offloaded in SYCL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5272",
        "state": "open",
        "created_at": "2024-02-02T04:38:28Z",
        "user": "Jacoby1218",
        "labels": [
            "bug-unconfirmed",
            "Intel GPU"
        ]
    },
    {
        "number": 5271,
        "title": "Build option -DLLAMA_SYCL_F16=ON is ignored on Windows when building with SYCL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5271",
        "state": "open",
        "created_at": "2024-02-02T03:53:07Z",
        "user": "Jacoby1218",
        "labels": [
            "bug-unconfirmed",
            "Intel GPU"
        ]
    },
    {
        "number": 5269,
        "title": "cs_co_variant(): LLVM ERROR stack trace with Vulkan on mesa (llvmpipe)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5269",
        "state": "open",
        "created_at": "2024-02-02T01:06:47Z",
        "user": "themanyone",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5266,
        "title": "Llava 34b model is working with llamacpp?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5266",
        "state": "open",
        "created_at": "2024-02-01T22:01:45Z",
        "user": "mirek190",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5264,
        "title": "wrong docker run instructions",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5264",
        "state": "open",
        "created_at": "2024-02-01T17:18:57Z",
        "user": "fedecompa",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5259,
        "title": "Vulkan multi or selectable GPU?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5259",
        "state": "open",
        "created_at": "2024-02-01T16:04:33Z",
        "user": "Ph0rk0z",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5251,
        "title": "calm2-7b-chat model : unable to load model ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5251",
        "state": "open",
        "created_at": "2024-02-01T07:24:18Z",
        "user": "ShikouMochizuki",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5249,
        "title": "Support for split GGUF checkpoints",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5249",
        "state": "open",
        "created_at": "2024-02-01T02:11:17Z",
        "user": "AlpinDale",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5247,
        "title": "Can't get server to run with more than 6 slots",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5247",
        "state": "open",
        "created_at": "2024-01-31T22:10:12Z",
        "user": "vashat",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 5246,
        "title": "`server` stops processing requests after the empty prompt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5246",
        "state": "open",
        "created_at": "2024-01-31T17:32:06Z",
        "user": "z80maniac",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5239,
        "title": "llama : refactor the llm.build_xxx functions",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5239",
        "state": "open",
        "created_at": "2024-01-31T12:55:44Z",
        "user": "ggerganov",
        "labels": [
            "good first issue",
            "refactoring"
        ]
    },
    {
        "number": 5237,
        "title": "Issues with running Llama.cpp on Raspberry Pi 5 with Vulkan.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5237",
        "state": "open",
        "created_at": "2024-01-31T11:04:06Z",
        "user": "Fujiwari",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5235,
        "title": " [Errno -2] Name or service not known",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5235",
        "state": "open",
        "created_at": "2024-01-31T09:51:09Z",
        "user": "Al2a2d2m",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5234,
        "title": "Custom fine-tuned DeepSeek coder model unable to be quantized to Fp16 ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5234",
        "state": "open",
        "created_at": "2024-01-31T07:54:11Z",
        "user": "jackshiwl",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5225,
        "title": "Thread hanging issues after the sched_yield changes / Intel CPU slowdowns related to E-cores",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5225",
        "state": "open",
        "created_at": "2024-01-30T19:56:58Z",
        "user": "kalomaze",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5220,
        "title": "ggml : move constant tables into a common header",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5220",
        "state": "open",
        "created_at": "2024-01-30T17:14:18Z",
        "user": "ggerganov",
        "labels": [
            "refactoring"
        ]
    },
    {
        "number": 5218,
        "title": "APIs for rolling back sampler tokens",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5218",
        "state": "open",
        "created_at": "2024-01-30T16:04:42Z",
        "user": "l3utterfly",
        "labels": []
    },
    {
        "number": 5215,
        "title": "llama : create llamax library",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5215",
        "state": "open",
        "created_at": "2024-01-30T13:01:06Z",
        "user": "ggerganov",
        "labels": [
            "refactoring"
        ]
    },
    {
        "number": 5214,
        "title": "llama : move the sampling API from common into llama lib",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5214",
        "state": "open",
        "created_at": "2024-01-30T12:44:03Z",
        "user": "ggerganov",
        "labels": [
            "refactoring"
        ]
    },
    {
        "number": 5213,
        "title": "Must the number of expertrs in Mixtral-MoE be a pow of 2 ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5213",
        "state": "open",
        "created_at": "2024-01-30T10:50:28Z",
        "user": "ZhangEnmao",
        "labels": []
    },
    {
        "number": 5210,
        "title": "Unable to get reproducible results",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5210",
        "state": "open",
        "created_at": "2024-01-30T08:57:42Z",
        "user": "DivyaKanniah",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5207,
        "title": "Feature Request: run large gguf file in low RAM machine",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5207",
        "state": "open",
        "created_at": "2024-01-30T08:19:23Z",
        "user": "liangDarwin2",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5197,
        "title": "Metal implementation on Matrix multiplication",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5197",
        "state": "open",
        "created_at": "2024-01-29T18:44:41Z",
        "user": "DerrickYLJ",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5192,
        "title": "How can I \"regenerate the last n tokens\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5192",
        "state": "open",
        "created_at": "2024-01-29T10:13:03Z",
        "user": "l3utterfly",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5186,
        "title": "Subtle Vulkan shader compilation bug when running on Adreno GPUs (Samsung Galaxy S23 Ultra)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5186",
        "state": "open",
        "created_at": "2024-01-29T07:08:45Z",
        "user": "l3utterfly",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5185,
        "title": "rogue inference on quantized gguf ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5185",
        "state": "open",
        "created_at": "2024-01-29T05:45:04Z",
        "user": "amygbAI",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5183,
        "title": "\u3010Aarch64 GGML_F16 is slower than GGML_F32\u3011ggml_vec_dot_f16's perf is slower when enable ARM_FEATURE_FP16_VECTOR_ARITHMETIC ON",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5183",
        "state": "open",
        "created_at": "2024-01-29T03:36:55Z",
        "user": "XiaotaoChen",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5177,
        "title": "Unable to convert safetensor fine tuned model deepseek to gguf with convert.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5177",
        "state": "open",
        "created_at": "2024-01-28T19:59:08Z",
        "user": "JesseGuerrero",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5174,
        "title": "add support for polylm-13b",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5174",
        "state": "open",
        "created_at": "2024-01-28T14:06:11Z",
        "user": "aspwow",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5171,
        "title": "Bad file descsriptor in building with `make` in w64devkit on Windows 11",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5171",
        "state": "open",
        "created_at": "2024-01-28T10:59:35Z",
        "user": "VIS-WA",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5170,
        "title": "Finetune from text",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5170",
        "state": "open",
        "created_at": "2024-01-28T10:19:55Z",
        "user": "zamalex",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5166,
        "title": "Segfault when running KL Divergence on both Windows & WSL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5166",
        "state": "open",
        "created_at": "2024-01-28T07:19:39Z",
        "user": "kalomaze",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5165,
        "title": "Frequent breakage of Mixtral based models using ROCM",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5165",
        "state": "open",
        "created_at": "2024-01-28T04:45:54Z",
        "user": "gnawzie",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5163,
        "title": " Error: could not load cache",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5163",
        "state": "open",
        "created_at": "2024-01-28T03:46:46Z",
        "user": "amyweiwu",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5156,
        "title": "convert-hf-to-gguf.py Qwen-72B-Chat model get Killed result",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5156",
        "state": "open",
        "created_at": "2024-01-27T03:02:05Z",
        "user": "lmxin123",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 5153,
        "title": "[FEATURE REQUEST] q9_0 or q10_0 quantization ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5153",
        "state": "open",
        "created_at": "2024-01-26T22:13:14Z",
        "user": "kalomaze",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5150,
        "title": "Performance decrease after using latest llama.cpp branch vs previous branch with ggml format",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5150",
        "state": "open",
        "created_at": "2024-01-26T17:31:00Z",
        "user": "aahouzi",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5144,
        "title": " make LLAMA_CUBLAS=1",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5144",
        "state": "open",
        "created_at": "2024-01-26T14:20:50Z",
        "user": "OMN-LEGACY",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5143,
        "title": "feat: Introduce new GGUFValueType.OBJ virtual type\ud83c\udf20",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5143",
        "state": "open",
        "created_at": "2024-01-26T13:34:30Z",
        "user": "snowyu",
        "labels": []
    },
    {
        "number": 5142,
        "title": "Architecture \"LlamaForCausalLM\" not supported",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5142",
        "state": "open",
        "created_at": "2024-01-26T12:45:54Z",
        "user": "lmxin123",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5140,
        "title": "CUDA: assert when using batch size less than 129",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5140",
        "state": "open",
        "created_at": "2024-01-26T11:24:14Z",
        "user": "ikawrakow",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 5139,
        "title": "Can not map tensor 'transformer.h.0.attn.c_attn.g_idx'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5139",
        "state": "open",
        "created_at": "2024-01-26T09:58:22Z",
        "user": "lmxin123",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5135,
        "title": "Help: FileNotFoundError: spm vocab not found",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5135",
        "state": "open",
        "created_at": "2024-01-26T04:02:10Z",
        "user": "xjj210130",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5134,
        "title": "Run on GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5134",
        "state": "open",
        "created_at": "2024-01-26T03:39:07Z",
        "user": "AreckOVO",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5128,
        "title": "I can't compile llama on my 5700xt. PLEASE, HELP ME! XD",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5128",
        "state": "open",
        "created_at": "2024-01-25T23:02:49Z",
        "user": "selvaunidi",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5120,
        "title": "Support VLM WebSight (aka \"screenshot2html\") model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5120",
        "state": "open",
        "created_at": "2024-01-24T23:57:40Z",
        "user": "niansa",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5119,
        "title": "Adding topic steering on layers ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5119",
        "state": "open",
        "created_at": "2024-01-24T20:46:09Z",
        "user": "benxh1995",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5112,
        "title": "A special token '\\u0000' will cause an assert error in 'llm_load_vocab'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5112",
        "state": "open",
        "created_at": "2024-01-24T14:24:39Z",
        "user": "SolenoidWGT",
        "labels": []
    },
    {
        "number": 5110,
        "title": "Non-GPU Support of LLMLingua",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5110",
        "state": "open",
        "created_at": "2024-01-24T13:42:37Z",
        "user": "sathyapriyaa13",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5108,
        "title": "ggml_vec_dot_f16's perf is slower servely when enable ARM_FEATURE_FP16_VECTOR_ARITHMETIC on Android",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5108",
        "state": "open",
        "created_at": "2024-01-24T11:31:35Z",
        "user": "hayyaw",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5106,
        "title": "adding support for linux binaries",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5106",
        "state": "open",
        "created_at": "2024-01-24T06:43:40Z",
        "user": "DarkReaperBoy",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5102,
        "title": "CUDA error: an illegal memory access was encountered",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5102",
        "state": "open",
        "created_at": "2024-01-23T23:02:35Z",
        "user": "puyuanOT",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5097,
        "title": "Support for XGLM models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5097",
        "state": "open",
        "created_at": "2024-01-23T10:32:47Z",
        "user": "Stypox",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5089,
        "title": "ggllm tensor printf debug function",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5089",
        "state": "open",
        "created_at": "2024-01-22T23:24:56Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 5080,
        "title": "Server enhancements - grammar segfault and helper titles.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5080",
        "state": "open",
        "created_at": "2024-01-22T15:54:27Z",
        "user": "jboero",
        "labels": []
    },
    {
        "number": 5079,
        "title": " Intel\u00ae Core\u2122 Ultra processors NPU  Support ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5079",
        "state": "open",
        "created_at": "2024-01-22T14:15:28Z",
        "user": "mofanke",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5075,
        "title": "Wrong detect all threads on multisocket motherboards on Windows",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5075",
        "state": "open",
        "created_at": "2024-01-22T10:19:28Z",
        "user": "GermanAizek",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5070,
        "title": "ggml : support `bs > 512` for Metal `ggml_mul_mat_id`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5070",
        "state": "open",
        "created_at": "2024-01-22T01:34:58Z",
        "user": "stewartoallen",
        "labels": [
            "enhancement",
            "good first issue",
            "macos"
        ]
    },
    {
        "number": 5069,
        "title": "Question : I found out that somehow generating with server differs to using regular /main",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5069",
        "state": "open",
        "created_at": "2024-01-22T01:28:45Z",
        "user": "x4080",
        "labels": []
    },
    {
        "number": 5067,
        "title": "Investigate: performance with Intel OneAPI (MKL)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5067",
        "state": "open",
        "created_at": "2024-01-22T00:14:21Z",
        "user": "ngxson",
        "labels": []
    },
    {
        "number": 5048,
        "title": "[Feature Request]: have TinyBlast Support for llama.cpp build. (which is very helpful for AMD user)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5048",
        "state": "open",
        "created_at": "2024-01-20T10:15:35Z",
        "user": "hiepxanh",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5040,
        "title": "Include `eos_token` and `bos_token` from `tokenizer_config.json` for chat templating",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5040",
        "state": "open",
        "created_at": "2024-01-19T20:31:05Z",
        "user": "abetlen",
        "labels": [
            "enhancement",
            "help wanted"
        ]
    },
    {
        "number": 5034,
        "title": "Baichuan2-7B-Chat model converted to ggml-model-q4_0.gguf, AI answer does not stop automatically when inference is made",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5034",
        "state": "open",
        "created_at": "2024-01-19T06:52:34Z",
        "user": "Lyzin",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5031,
        "title": "convert.py couldn't convert internlm2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5031",
        "state": "open",
        "created_at": "2024-01-19T00:52:07Z",
        "user": "gaord",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5026,
        "title": "Allow oversubscription of GPU memory through cudaMallocManaged on cuBLAS builds for systems like GH200",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5026",
        "state": "open",
        "created_at": "2024-01-18T21:57:55Z",
        "user": "vultj",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5022,
        "title": "How does 6bit, 5bit and 3bit work on NVIDIA GPU?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5022",
        "state": "open",
        "created_at": "2024-01-18T18:37:39Z",
        "user": "hy-chen",
        "labels": []
    },
    {
        "number": 5021,
        "title": "ggml : add Flash Attention",
        "url": "https://github.com/ggerganov/llama.cpp/pull/5021",
        "state": "open",
        "created_at": "2024-01-18T17:06:57Z",
        "user": "ggerganov",
        "labels": [
            "help wanted",
            "performance"
        ]
    },
    {
        "number": 5014,
        "title": "When use the GPU, llama-cpp-python[server] keeps returning #",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5014",
        "state": "open",
        "created_at": "2024-01-18T05:34:45Z",
        "user": "felix1982",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5012,
        "title": "The arguments/parameters/help should be dynamic per example",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5012",
        "state": "open",
        "created_at": "2024-01-18T02:10:18Z",
        "user": "cmp-nct",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 5009,
        "title": "Using convert.py with a fine tuned phi-2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5009",
        "state": "open",
        "created_at": "2024-01-17T21:14:08Z",
        "user": "FiveTechSoft",
        "labels": []
    },
    {
        "number": 5005,
        "title": "Build with AMD AOCC + AOCL (CPU only)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5005",
        "state": "open",
        "created_at": "2024-01-17T18:03:09Z",
        "user": "GANJAC",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5004,
        "title": "Convert NF4 Transformer model to GGML/GGUF",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5004",
        "state": "open",
        "created_at": "2024-01-17T17:50:02Z",
        "user": "puyuanOT",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 5000,
        "title": "Tasks queue logic doesn't seem to be logical",
        "url": "https://github.com/ggerganov/llama.cpp/issues/5000",
        "state": "open",
        "created_at": "2024-01-17T14:24:18Z",
        "user": "tikikun",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4998,
        "title": "unsupported op 'MUL_MAT'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4998",
        "state": "open",
        "created_at": "2024-01-17T12:12:18Z",
        "user": "NeevJewalkar",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4989,
        "title": "Enabling `cache_prompt` on completion request fills KV cache quickly ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4989",
        "state": "open",
        "created_at": "2024-01-16T21:05:14Z",
        "user": "BruceMacD",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4985,
        "title": "Tokenizer should remember prevously converted tokens",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4985",
        "state": "open",
        "created_at": "2024-01-16T18:09:46Z",
        "user": "Urammar",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4977,
        "title": "new 2-Bit quants don't work with CLBlast Backend ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4977",
        "state": "open",
        "created_at": "2024-01-16T13:41:36Z",
        "user": "stduhpf",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4976,
        "title": "[FEATURE REQUEST] - \"Dithering\" to improve quantization results at inference time",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4976",
        "state": "open",
        "created_at": "2024-01-16T13:32:03Z",
        "user": "kalomaze",
        "labels": [
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 4973,
        "title": "Adreno gpu run crash",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4973",
        "state": "open",
        "created_at": "2024-01-16T12:12:12Z",
        "user": "java63940",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4967,
        "title": "Android GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4967",
        "state": "open",
        "created_at": "2024-01-16T07:48:54Z",
        "user": "AreckOVO",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4965,
        "title": "[REGRESSION] 1.5-1.75x RAM overhead since e7e4df0 (or 53ae0dd, to be exact)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4965",
        "state": "open",
        "created_at": "2024-01-16T02:49:09Z",
        "user": "arzeth",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4962,
        "title": "Can not install llama.cpp with cuBlas using the latest code.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4962",
        "state": "open",
        "created_at": "2024-01-15T18:37:44Z",
        "user": "IvoryTower800",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4953,
        "title": "Add support Tiny Llava on llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4953",
        "state": "open",
        "created_at": "2024-01-15T07:29:07Z",
        "user": "circuluspibo",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4946,
        "title": "VRAM leak during prompt processing",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4946",
        "state": "open",
        "created_at": "2024-01-14T23:27:48Z",
        "user": "Artefact2",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4944,
        "title": "Installing llama.cpp on virtualbox",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4944",
        "state": "open",
        "created_at": "2024-01-14T20:18:58Z",
        "user": "marcpre",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4942,
        "title": "Mixtral-like models crash with Clblast on warming up after backend update with partial offloading.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4942",
        "state": "open",
        "created_at": "2024-01-14T18:39:41Z",
        "user": "MaggotHATE",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4940,
        "title": "Issue in  convert-lora-to-ggml.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4940",
        "state": "open",
        "created_at": "2024-01-14T17:09:41Z",
        "user": "ragesh2000",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4939,
        "title": "Seg fault (core dumped)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4939",
        "state": "open",
        "created_at": "2024-01-14T16:15:38Z",
        "user": "willmil11",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4928,
        "title": "terminate called after throwing an instance of 'std::bad_alloc'   what():  std::bad_alloc",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4928",
        "state": "open",
        "created_at": "2024-01-14T09:19:56Z",
        "user": "zamalex",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4925,
        "title": "can' quantize deekseek model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4925",
        "state": "open",
        "created_at": "2024-01-14T07:19:27Z",
        "user": "dotyuu",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4921,
        "title": "Wrong interpretation of the drive letter Q under Windows if the location of the model is copied from Windows Security tab in file properties.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4921",
        "state": "open",
        "created_at": "2024-01-13T22:26:46Z",
        "user": "Nexesenex",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4912,
        "title": "llama : add phixtral support",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4912",
        "state": "open",
        "created_at": "2024-01-13T12:22:12Z",
        "user": "ggerganov",
        "labels": []
    },
    {
        "number": 4899,
        "title": "Ensure System Prompt Is ALWAYS In Cache",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4899",
        "state": "open",
        "created_at": "2024-01-12T15:07:26Z",
        "user": "Nate687",
        "labels": []
    },
    {
        "number": 4892,
        "title": "[Bug] windows build 1833 opencl version",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4892",
        "state": "open",
        "created_at": "2024-01-12T11:36:41Z",
        "user": "FNsi",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4887,
        "title": "mixtral\u9519\u8bef",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4887",
        "state": "open",
        "created_at": "2024-01-12T06:05:08Z",
        "user": "Anorid",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4885,
        "title": "Is soft prompting enabled or is there a plan to do so?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4885",
        "state": "open",
        "created_at": "2024-01-12T03:29:07Z",
        "user": "robhaslinger",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4884,
        "title": "Connection to the server API fails and website returns an empty response when using Docker on Windows with WSL2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4884",
        "state": "open",
        "created_at": "2024-01-12T02:21:37Z",
        "user": "Celarye",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4880,
        "title": "Llava-cli crashes Mac and reboots since commit ce18d727",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4880",
        "state": "open",
        "created_at": "2024-01-11T17:31:25Z",
        "user": "chigkim",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4868,
        "title": "[Feat]: Add Tokenizer Metadata in tokenizer.json to gguf Format for Enhanced llama.cpp Capabilities",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4868",
        "state": "open",
        "created_at": "2024-01-11T04:51:43Z",
        "user": "snowyu",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4859,
        "title": "server unable to render code blocks when \"Show Probabilities\" option is active",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4859",
        "state": "open",
        "created_at": "2024-01-10T15:30:30Z",
        "user": "Huge",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4858,
        "title": "convert.py: Outfile default name change and additional metadata support",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4858",
        "state": "open",
        "created_at": "2024-01-10T14:53:01Z",
        "user": "mofosyne",
        "labels": []
    },
    {
        "number": 4857,
        "title": "allow loading from ram location ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4857",
        "state": "open",
        "created_at": "2024-01-10T14:51:04Z",
        "user": "IMbackK",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4851,
        "title": "CI: nix flake update: no permission to create a PR",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4851",
        "state": "open",
        "created_at": "2024-01-10T01:52:06Z",
        "user": "SomeoneSerge",
        "labels": [
            "nix"
        ]
    },
    {
        "number": 4848,
        "title": "param.path_model is a Path() not a string",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4848",
        "state": "open",
        "created_at": "2024-01-09T23:50:15Z",
        "user": "mofosyne",
        "labels": []
    },
    {
        "number": 4843,
        "title": "Request: Allow for adjustments at the layer-level, for a practically two-fold increase in LLM handling ability by prompters",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4843",
        "state": "open",
        "created_at": "2024-01-09T19:57:09Z",
        "user": "9a9o",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4831,
        "title": "Query Regarding Hugging Face llama2 7b Chat Model: GPU Segmentation Fault",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4831",
        "state": "open",
        "created_at": "2024-01-09T06:53:54Z",
        "user": "keyword1983",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 4830,
        "title": "Baby-llama.cpp report bus error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4830",
        "state": "open",
        "created_at": "2024-01-09T03:16:07Z",
        "user": "YangZyyyy",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4824,
        "title": " Add support for MobileLLaMA/MobileVLM models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4824",
        "state": "open",
        "created_at": "2024-01-08T16:27:22Z",
        "user": "Foul-Tarnished",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4823,
        "title": "Support for LLMLingua",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4823",
        "state": "open",
        "created_at": "2024-01-08T15:52:51Z",
        "user": "TechnotechGit",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4819,
        "title": "how to use GGML-Tips-&-Tricks in new code\uff1f",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4819",
        "state": "open",
        "created_at": "2024-01-08T05:38:22Z",
        "user": "2213601279",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4816,
        "title": "oneAPI, intel icx/icpx: Fix compile warnings \"explicit comparison with infinity in fast floating point mode\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4816",
        "state": "open",
        "created_at": "2024-01-07T22:34:53Z",
        "user": "stolsvik",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4811,
        "title": "A chatbot-ui based ui for llama.cpp server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4811",
        "state": "open",
        "created_at": "2024-01-07T15:42:53Z",
        "user": "yportne13",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4808,
        "title": "How to prepare&run the model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4808",
        "state": "open",
        "created_at": "2024-01-07T12:43:07Z",
        "user": "DerrickYLJ",
        "labels": []
    },
    {
        "number": 4803,
        "title": "redhat7.7 compilation error: ggml.c:89:23: fatal error: stdatomic.h: No such file or directory  #include <stdatomic.h>",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4803",
        "state": "open",
        "created_at": "2024-01-07T03:31:05Z",
        "user": "chuanzhubin",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4802,
        "title": "Issues with network access of server cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4802",
        "state": "open",
        "created_at": "2024-01-06T20:07:39Z",
        "user": "RuslanKovalyov",
        "labels": []
    },
    {
        "number": 4801,
        "title": "CUDA: int8 tensor core matrix multiplication",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4801",
        "state": "open",
        "created_at": "2024-01-06T19:10:00Z",
        "user": "JohannesGaessler",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 4793,
        "title": "Build Error on macOS: 'CoreGraphics/CGEventSource.h' File Not Found",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4793",
        "state": "open",
        "created_at": "2024-01-06T07:58:40Z",
        "user": "shanemcl",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4792,
        "title": "MPI issue on the Nvidia Jetson Cluster",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4792",
        "state": "open",
        "created_at": "2024-01-06T07:16:49Z",
        "user": "shahizat",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4791,
        "title": "Failed to train-text-from-scratch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4791",
        "state": "open",
        "created_at": "2024-01-06T06:47:11Z",
        "user": "notooth1",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4789,
        "title": "Crash When Running Obsidian 3B Q6 Multimodal.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4789",
        "state": "open",
        "created_at": "2024-01-06T01:39:23Z",
        "user": "bubbabug",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4782,
        "title": "HQQ quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4782",
        "state": "open",
        "created_at": "2024-01-05T10:52:53Z",
        "user": "loretoparisi",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4781,
        "title": "redhat7.7 compilation error: ggml.c:89:23: fatal error: stdatomic.h: No such file or directory",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4781",
        "state": "open",
        "created_at": "2024-01-05T09:40:16Z",
        "user": "chuanzhubin",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4778,
        "title": "Token healing (under 40 LOC)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4778",
        "state": "open",
        "created_at": "2024-01-04T20:14:53Z",
        "user": "Ayenem",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4776,
        "title": "Display model in use inside 'server' HTML GUI? Preferably with params.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4776",
        "state": "open",
        "created_at": "2024-01-04T16:52:07Z",
        "user": "stolsvik",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4774,
        "title": "Android & CLBlast: Output garbage when trying to offload last layer of Llama-2-7b to GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4774",
        "state": "open",
        "created_at": "2024-01-04T13:41:01Z",
        "user": "Msyu1020",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4772,
        "title": "Tokenizer: wrong token ids for OpenChat and Starling models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4772",
        "state": "open",
        "created_at": "2024-01-04T11:31:24Z",
        "user": "jndiogo",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4771,
        "title": "OpenAI incompatible image handling in server multimodal",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4771",
        "state": "open",
        "created_at": "2024-01-04T11:24:44Z",
        "user": "gelim",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4770,
        "title": "CUDA error: unknown error when offloading to gfx1035",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4770",
        "state": "open",
        "created_at": "2024-01-04T10:09:37Z",
        "user": "tdavie",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4769,
        "title": "How to build high performance llama.cpp on Windows on ARM device?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4769",
        "state": "open",
        "created_at": "2024-01-04T10:06:18Z",
        "user": "Billzhong2022",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4763,
        "title": "[MULTI-GPU][AMD/HIP5.7] Using the -mg param is critical",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4763",
        "state": "open",
        "created_at": "2024-01-03T20:59:11Z",
        "user": "stejpet",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4762,
        "title": "CUDA error: invalid device function when compiling and running for amd gfx 1032",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4762",
        "state": "open",
        "created_at": "2024-01-03T18:23:12Z",
        "user": "nasawyer7",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4759,
        "title": "implement sparsity prediction to significantly reduce memory footprint",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4759",
        "state": "open",
        "created_at": "2024-01-03T15:30:42Z",
        "user": "dmahurin",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4755,
        "title": "Quantization of transformer state for matrix-vector products potentially causes numerical accuracy issues",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4755",
        "state": "open",
        "created_at": "2024-01-03T12:30:13Z",
        "user": "cafaxo",
        "labels": []
    },
    {
        "number": 4753,
        "title": "unknown format error while trying to convert opp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4753",
        "state": "open",
        "created_at": "2024-01-03T10:38:53Z",
        "user": "jnpushkar1507",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4752,
        "title": "Fix CUDA diag_mask_inf tests with LLAMA_FAST",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4752",
        "state": "open",
        "created_at": "2024-01-03T10:36:33Z",
        "user": "JohannesGaessler",
        "labels": []
    },
    {
        "number": 4748,
        "title": "The completion stopped but the output is not complete?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4748",
        "state": "open",
        "created_at": "2024-01-03T07:25:32Z",
        "user": "flowermlh",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4744,
        "title": "Question : Is it normal when the prompt size is large, the result quality will be degraded ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4744",
        "state": "open",
        "created_at": "2024-01-02T23:04:26Z",
        "user": "x4080",
        "labels": []
    },
    {
        "number": 4728,
        "title": "Inconsistent Results when using Server and LlamaGuard",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4728",
        "state": "open",
        "created_at": "2024-01-01T19:27:09Z",
        "user": "adrianliechti",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4724,
        "title": "Finetuning doesn't seem to use GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4724",
        "state": "open",
        "created_at": "2024-01-01T17:42:22Z",
        "user": "tom-adsfund",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4720,
        "title": "Key-value-only GGUFs to help with unit testing models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4720",
        "state": "open",
        "created_at": "2023-12-31T21:34:26Z",
        "user": "postmasters",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4719,
        "title": "CUDA: parallelize graph evaluation across multiple streams",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4719",
        "state": "open",
        "created_at": "2023-12-31T17:15:55Z",
        "user": "JohannesGaessler",
        "labels": []
    },
    {
        "number": 4718,
        "title": "in situ auto-Frankenmerges",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4718",
        "state": "open",
        "created_at": "2023-12-31T15:46:56Z",
        "user": "semiring",
        "labels": [
            "enhancement",
            "good first issue"
        ]
    },
    {
        "number": 4717,
        "title": "Making embed and norm layers trainable during LoRA fine-tuning in llama cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4717",
        "state": "open",
        "created_at": "2023-12-31T15:23:48Z",
        "user": "RonanKMcGovern",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4716,
        "title": "Weird performance difference between Ryzen 5900X and 7950X",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4716",
        "state": "open",
        "created_at": "2023-12-31T10:39:17Z",
        "user": "nathanodle",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4712,
        "title": "ggml-cuda : VRAM from batched processing should be freed (9-27% of GPU VRAM are occupied that way)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4712",
        "state": "open",
        "created_at": "2023-12-31T05:12:44Z",
        "user": "cmp-nct",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4708,
        "title": "Can llama.cpp handle image output? e.g. llava-interactive",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4708",
        "state": "open",
        "created_at": "2023-12-30T19:54:00Z",
        "user": "husnoo",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4706,
        "title": "Huge slow down when combine -cb and -np",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4706",
        "state": "open",
        "created_at": "2023-12-30T18:04:49Z",
        "user": "sorasoras",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4704,
        "title": "Add support for DR\u00b5GS",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4704",
        "state": "open",
        "created_at": "2023-12-30T17:23:47Z",
        "user": "Chanka0",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4703,
        "title": "Error doing a full fine-tune with train-text-from-scratch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4703",
        "state": "open",
        "created_at": "2023-12-30T17:16:57Z",
        "user": "RonanKMcGovern",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4701,
        "title": "Support AutoAWQ in `awq-py`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4701",
        "state": "open",
        "created_at": "2023-12-30T15:46:17Z",
        "user": "casper-hansen",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4699,
        "title": "[BUG] cmake build with CUDAToolkit 10.0.130 encounter nvcc error: Unknown option 'Wno-pedantic'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4699",
        "state": "open",
        "created_at": "2023-12-30T15:03:04Z",
        "user": "XiaotaoChen",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4694,
        "title": "add missing completion params to chat - Issue#4429",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4694",
        "state": "open",
        "created_at": "2023-12-30T03:36:57Z",
        "user": "peturparkur",
        "labels": []
    },
    {
        "number": 4689,
        "title": "mixtral-fusion-4x7b-instruct-v0.1.Q4_K_M.gguf",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4689",
        "state": "open",
        "created_at": "2023-12-29T17:52:33Z",
        "user": "erlebach",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4688,
        "title": "Mixtral expert offloading and speculative loading",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4688",
        "state": "open",
        "created_at": "2023-12-29T17:50:35Z",
        "user": "nivibilla",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4686,
        "title": "Strange phenomena during model output",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4686",
        "state": "open",
        "created_at": "2023-12-29T14:38:40Z",
        "user": "hell99dkx",
        "labels": []
    },
    {
        "number": 4685,
        "title": "llama.cpp not compiling",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4685",
        "state": "open",
        "created_at": "2023-12-29T14:21:52Z",
        "user": "5hayanB",
        "labels": []
    },
    {
        "number": 4684,
        "title": "DRAFT: Issue#4638 General prompt template handling",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4684",
        "state": "open",
        "created_at": "2023-12-29T12:45:22Z",
        "user": "peturparkur",
        "labels": []
    },
    {
        "number": 4683,
        "title": "Using custom prompt template in server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4683",
        "state": "open",
        "created_at": "2023-12-29T12:37:47Z",
        "user": "peturparkur",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4679,
        "title": "Getting KeyError: 'U8' error in the `convert.py`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4679",
        "state": "open",
        "created_at": "2023-12-29T05:23:44Z",
        "user": "yukiarimo",
        "labels": []
    },
    {
        "number": 4678,
        "title": "New UI and FreePascal/Delphi bindings",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4678",
        "state": "open",
        "created_at": "2023-12-29T03:16:03Z",
        "user": "ortegaalfredo",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4677,
        "title": "How do I properly terminate it in interactive mode to save the prompt cache on exit?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4677",
        "state": "open",
        "created_at": "2023-12-28T23:38:22Z",
        "user": "takosalad",
        "labels": []
    },
    {
        "number": 4667,
        "title": "[FEATURE REQUEST] - Make MoE offloading offload real layers rather than groups of layers",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4667",
        "state": "open",
        "created_at": "2023-12-28T15:02:16Z",
        "user": "kalomaze",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4661,
        "title": "[Question] How can i quantize converted lora adaptor (ggml) as q4?  ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4661",
        "state": "open",
        "created_at": "2023-12-28T06:30:01Z",
        "user": "allzero-kwon",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4659,
        "title": "[Question] Plans to parallelize GPU matrix execution in ggml_cuda_op_mul_mat?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4659",
        "state": "open",
        "created_at": "2023-12-28T02:39:41Z",
        "user": "zzningxp",
        "labels": []
    },
    {
        "number": 4656,
        "title": "Need to be able to assign to each GPU user-defined number of layers.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4656",
        "state": "open",
        "created_at": "2023-12-27T19:47:19Z",
        "user": "phalexo",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4655,
        "title": "TensorRT-LLM Inference Optimizer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4655",
        "state": "open",
        "created_at": "2023-12-27T19:21:55Z",
        "user": "ngoiyaeric",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4652,
        "title": "Please add GGUF to pth conversion.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4652",
        "state": "open",
        "created_at": "2023-12-27T10:15:47Z",
        "user": "Pavelzelen",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4645,
        "title": "LoRA Model Support and Full Fine-tune options",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4645",
        "state": "open",
        "created_at": "2023-12-26T14:51:42Z",
        "user": "RonanKMcGovern",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4644,
        "title": "Add BAAI/Emu 2 in llamacpp?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4644",
        "state": "open",
        "created_at": "2023-12-26T14:29:22Z",
        "user": "paryska99",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4634,
        "title": "Model does not appear to have a file named config.json",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4634",
        "state": "open",
        "created_at": "2023-12-25T22:45:36Z",
        "user": "minh282906",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4627,
        "title": "Make the code easily reusable as a library",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4627",
        "state": "open",
        "created_at": "2023-12-25T11:40:18Z",
        "user": "Anton-V-K",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4626,
        "title": "Build is broken when MKL and Cuda enabled at the same time on Windows",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4626",
        "state": "open",
        "created_at": "2023-12-25T10:23:15Z",
        "user": "realiti4",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4624,
        "title": "`llama_decode` is significantly slower if `n_tokens > 1` ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4624",
        "state": "open",
        "created_at": "2023-12-24T23:05:48Z",
        "user": "apoorvumang",
        "labels": [
            "performance",
            "macos",
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4621,
        "title": "llama_init_from_file: failed to load model: Value too large for defined data type",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4621",
        "state": "open",
        "created_at": "2023-12-24T17:11:14Z",
        "user": "quarinteen",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4616,
        "title": "finetuned lora crashes with clblast",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4616",
        "state": "open",
        "created_at": "2023-12-24T08:01:55Z",
        "user": "bigfatbrowncat",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4615,
        "title": "Phi-2 Quantization of QLoRA model Fails",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4615",
        "state": "open",
        "created_at": "2023-12-24T05:41:18Z",
        "user": "TKDKid1000",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4614,
        "title": "HPX Support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4614",
        "state": "open",
        "created_at": "2023-12-24T03:23:30Z",
        "user": "ct-clmsn",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4613,
        "title": "initial import of hpx support",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4613",
        "state": "open",
        "created_at": "2023-12-24T03:16:03Z",
        "user": "ct-clmsn",
        "labels": []
    },
    {
        "number": 4612,
        "title": "Add flake app to run openai proxy",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4612",
        "state": "open",
        "created_at": "2023-12-23T22:53:41Z",
        "user": "ParetoOptimalDev",
        "labels": [
            "nix"
        ]
    },
    {
        "number": 4611,
        "title": "Mixtral Experts are initialized from Mistral 7b - Low Rank conversion possible?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4611",
        "state": "open",
        "created_at": "2023-12-23T19:07:56Z",
        "user": "kalomaze",
        "labels": [
            "enhancement",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 4602,
        "title": "CUDA: fixed peer access toggle synchronization",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4602",
        "state": "open",
        "created_at": "2023-12-22T14:33:54Z",
        "user": "JohannesGaessler",
        "labels": []
    },
    {
        "number": 4598,
        "title": "Power save mode for server --unload-timeout 120",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4598",
        "state": "open",
        "created_at": "2023-12-22T12:14:09Z",
        "user": "zwilch",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4595,
        "title": "The program of `server`  can start, but does not listen port",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4595",
        "state": "open",
        "created_at": "2023-12-22T10:09:20Z",
        "user": "qingchuwudi",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4583,
        "title": "single client multi-prompt hangs on server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4583",
        "state": "open",
        "created_at": "2023-12-22T03:38:19Z",
        "user": "jxy",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4580,
        "title": "Support for conversion of falcon-rw-1b to gguf format",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4580",
        "state": "open",
        "created_at": "2023-12-22T03:20:02Z",
        "user": "Govind-S-B",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4575,
        "title": "make process hangs if LLAMA_CUBLAS=1, at the line that includes the file scripts/get-flags.mk for the second time",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4575",
        "state": "open",
        "created_at": "2023-12-21T21:18:17Z",
        "user": "yigber",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4574,
        "title": "llama : integer type consistency in `llama.h`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4574",
        "state": "open",
        "created_at": "2023-12-21T19:55:14Z",
        "user": "MarcusDunn",
        "labels": [
            "enhancement",
            "good first issue",
            "refactoring"
        ]
    },
    {
        "number": 4571,
        "title": "Initial import of OpenSHMEM support",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4571",
        "state": "open",
        "created_at": "2023-12-21T18:39:32Z",
        "user": "ct-clmsn",
        "labels": []
    },
    {
        "number": 4570,
        "title": "OpenSHMEM support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4570",
        "state": "open",
        "created_at": "2023-12-21T18:38:59Z",
        "user": "ct-clmsn",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4563,
        "title": "CUDA error 719",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4563",
        "state": "open",
        "created_at": "2023-12-21T13:59:35Z",
        "user": "Dyke-F",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4557,
        "title": "Some bugs in new server UI (pr #4236)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4557",
        "state": "open",
        "created_at": "2023-12-21T02:25:50Z",
        "user": "mounta11n",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4550,
        "title": "f\"Found multiple models in {path}, not sure which to pick: {files}\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4550",
        "state": "open",
        "created_at": "2023-12-20T19:38:06Z",
        "user": "drewskidang",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4547,
        "title": "LlamaCPP for local model usage",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4547",
        "state": "open",
        "created_at": "2023-12-20T13:07:24Z",
        "user": "4entertainment",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4544,
        "title": "Using a word like lim\u00f3n on the prompt makes llama.cpp crash",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4544",
        "state": "open",
        "created_at": "2023-12-20T09:11:33Z",
        "user": "FiveTechSoft",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4543,
        "title": "[Review] Merge PowerInfer with llama.cpp mainline",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4543",
        "state": "open",
        "created_at": "2023-12-20T08:02:29Z",
        "user": "chsasank",
        "labels": []
    },
    {
        "number": 4542,
        "title": "PowerInfer faster Inference than llama cpp.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4542",
        "state": "open",
        "created_at": "2023-12-20T03:54:15Z",
        "user": "nivibilla",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4537,
        "title": "API for manipulating token-level input embeddings",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4537",
        "state": "open",
        "created_at": "2023-12-19T20:49:10Z",
        "user": "ringohoffman",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4532,
        "title": "Inference is dead slow on A100, A6000 within last 2 weeks. <1t/s. Using GBNF grammars.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4532",
        "state": "open",
        "created_at": "2023-12-19T09:48:11Z",
        "user": "e-p-armstrong",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4530,
        "title": "KeyError: 'transformer.wte.weight'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4530",
        "state": "open",
        "created_at": "2023-12-19T08:07:12Z",
        "user": "h9-tect",
        "labels": []
    },
    {
        "number": 4526,
        "title": "Batch processing should use a currently-missing batch dimension for all tensors",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4526",
        "state": "open",
        "created_at": "2023-12-18T19:20:28Z",
        "user": "AutonomicPerfectionist",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4524,
        "title": "Adding better line editing in interactive mode",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4524",
        "state": "open",
        "created_at": "2023-12-18T16:26:38Z",
        "user": "Slider2k",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4521,
        "title": "Does llama.cpp works in 32 bits ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4521",
        "state": "open",
        "created_at": "2023-12-18T09:59:39Z",
        "user": "FiveTechSoft",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4518,
        "title": "Possible ideas for speeding up CPU inference with Mixtral (KV cache prioritization)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4518",
        "state": "open",
        "created_at": "2023-12-18T05:43:16Z",
        "user": "kalomaze",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4515,
        "title": "Add support for ViP-LLaVA?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4515",
        "state": "open",
        "created_at": "2023-12-17T23:37:20Z",
        "user": "mu-cai",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4512,
        "title": "Mixtral 8x7B didnt work with server.exe ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4512",
        "state": "open",
        "created_at": "2023-12-17T19:50:00Z",
        "user": "williamgomez71",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4510,
        "title": "LLAMA_MAX_NODES needs to be increased when running 220B Saily model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4510",
        "state": "open",
        "created_at": "2023-12-17T19:05:34Z",
        "user": "dillfrescott",
        "labels": []
    },
    {
        "number": 4504,
        "title": "Support for CUDA/HIP on OpenCL/Level0 via chipStar",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4504",
        "state": "open",
        "created_at": "2023-12-17T01:05:32Z",
        "user": "ColonelPhantom",
        "labels": []
    },
    {
        "number": 4503,
        "title": "models generating gibberish on hipblas",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4503",
        "state": "open",
        "created_at": "2023-12-16T23:17:56Z",
        "user": "SuperPou1",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4502,
        "title": "issue running mixtrall",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4502",
        "state": "open",
        "created_at": "2023-12-16T18:57:37Z",
        "user": "alienatorZ",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4498,
        "title": "Crippled Performance on Multi Gpu Due to Loading onto RAM",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4498",
        "state": "open",
        "created_at": "2023-12-16T12:59:06Z",
        "user": "Noobville1345",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4497,
        "title": "server: make completions endpoint follow openai api just like chatcompletion",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4497",
        "state": "open",
        "created_at": "2023-12-16T12:22:38Z",
        "user": "nyxkrage",
        "labels": [
            "good first issue",
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4496,
        "title": "whats the right way the use llama.cpp from a Windows app ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4496",
        "state": "open",
        "created_at": "2023-12-16T11:25:35Z",
        "user": "FiveTechSoft",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4493,
        "title": "Failed to convert Llama-v2 models ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4493",
        "state": "open",
        "created_at": "2023-12-16T08:47:53Z",
        "user": "HighTemplar-wjiang",
        "labels": [
            "bug",
            "high priority"
        ]
    },
    {
        "number": 4492,
        "title": "Invalid `grammar` crashes the entire `server` instead of returning Null or ignoring it",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4492",
        "state": "open",
        "created_at": "2023-12-16T05:08:45Z",
        "user": "ibehnam",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4491,
        "title": "deprecate llama_batch_get_one and llama_get_logits",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4491",
        "state": "open",
        "created_at": "2023-12-16T03:15:15Z",
        "user": "cebtenzzre",
        "labels": []
    },
    {
        "number": 4487,
        "title": "Please add llama.cui to the list of UI in Readme",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4487",
        "state": "open",
        "created_at": "2023-12-15T19:59:13Z",
        "user": "dspasyuk",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4481,
        "title": "Add `make` instructions for Intel MKL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4481",
        "state": "open",
        "created_at": "2023-12-14T23:46:51Z",
        "user": "vn971",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4479,
        "title": "Sending in tokens one at a time vs all at once gives different logits",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4479",
        "state": "open",
        "created_at": "2023-12-14T21:29:48Z",
        "user": "Phylliida",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4476,
        "title": "server: Completion of pre-tokenized prompt is broken",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4476",
        "state": "open",
        "created_at": "2023-12-14T19:18:13Z",
        "user": "shibe2",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 4475,
        "title": " The terminal disconnects llava-v1.5-7b-q4-server.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4475",
        "state": "open",
        "created_at": "2023-12-14T19:16:58Z",
        "user": "arhun",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4473,
        "title": "Increase prediction length maximum in server mode web UI",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4473",
        "state": "open",
        "created_at": "2023-12-14T16:58:22Z",
        "user": "0xAlcibiades",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4471,
        "title": "Provided docker images don't support -np -cb  parameters",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4471",
        "state": "open",
        "created_at": "2023-12-14T15:19:16Z",
        "user": "alexvt-amz",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4470,
        "title": "Min-p Mixtral routing",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4470",
        "state": "open",
        "created_at": "2023-12-14T14:13:36Z",
        "user": "someone13574",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4463,
        "title": "Adding MistralForCausalLM architecture to convert-hf-to-gguf.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4463",
        "state": "open",
        "created_at": "2023-12-14T10:00:26Z",
        "user": "moo-aly",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4458,
        "title": "Surprising CPU/GPU divergences at temperature 0 since a6fc554e",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4458",
        "state": "open",
        "created_at": "2023-12-14T03:44:59Z",
        "user": "mrdomino",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4455,
        "title": "Smart layer offload selection for Mixtral",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4455",
        "state": "open",
        "created_at": "2023-12-13T23:20:18Z",
        "user": "someone13574",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4452,
        "title": "CUDA OOM for 6bit Mixtral 8x7B Model on 2x3090",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4452",
        "state": "open",
        "created_at": "2023-12-13T21:44:52Z",
        "user": "razorback16",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4451,
        "title": "CLBlast support for Mixtral",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4451",
        "state": "open",
        "created_at": "2023-12-13T20:45:30Z",
        "user": "paralin",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4442,
        "title": "Model is split between multiple GPU's even after explicitly specifying main gpu",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4442",
        "state": "open",
        "created_at": "2023-12-13T15:14:09Z",
        "user": "Abhi011999",
        "labels": []
    },
    {
        "number": 4440,
        "title": "Cannot run Llama.cpp on Apple Silicon M3 Max",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4440",
        "state": "open",
        "created_at": "2023-12-13T13:47:46Z",
        "user": "dineshdharme",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4439,
        "title": "Run Mixtral-8x7b-instruct with Llama.cpp: Could not load Llama model from path",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4439",
        "state": "open",
        "created_at": "2023-12-13T13:26:16Z",
        "user": "weissenbacherpwc",
        "labels": []
    },
    {
        "number": 4432,
        "title": "How to include the installation parameters and make BLAS arguments in the requirements.txt?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4432",
        "state": "open",
        "created_at": "2023-12-13T06:09:17Z",
        "user": "AayushSameerShah",
        "labels": []
    },
    {
        "number": 4429,
        "title": "Add `completion` server parameters to `v1/chat/completions` ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4429",
        "state": "open",
        "created_at": "2023-12-12T17:32:11Z",
        "user": "LiamNiisan",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4425,
        "title": "Quantizing V cache not working yet",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4425",
        "state": "open",
        "created_at": "2023-12-12T09:40:08Z",
        "user": "CISC",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4422,
        "title": "make Breakpoints can't be stopped\uff1bcmake segmentation fault has occurred",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4422",
        "state": "open",
        "created_at": "2023-12-12T02:02:37Z",
        "user": "18222901154",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4421,
        "title": "unable to build for armeabi-v7a",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4421",
        "state": "open",
        "created_at": "2023-12-12T01:02:17Z",
        "user": "itsPreto",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4419,
        "title": "Add Support for ise-uiuc/Magicoder-S-DS-6.7B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4419",
        "state": "open",
        "created_at": "2023-12-11T23:07:27Z",
        "user": "apcameron",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4415,
        "title": "Implement Attention Buckets",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4415",
        "state": "open",
        "created_at": "2023-12-11T21:29:53Z",
        "user": "Silver267",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4413,
        "title": "Better description for flags related to prompt template",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4413",
        "state": "open",
        "created_at": "2023-12-11T20:04:15Z",
        "user": "chigkim",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4409,
        "title": "Cannot compile with openblas on Windows",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4409",
        "state": "open",
        "created_at": "2023-12-11T14:53:22Z",
        "user": "dillfrescott",
        "labels": []
    },
    {
        "number": 4400,
        "title": "UI Improvements for Server",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4400",
        "state": "open",
        "created_at": "2023-12-10T02:53:03Z",
        "user": "xydac",
        "labels": []
    },
    {
        "number": 4399,
        "title": "[Build] Offer WASM in release?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4399",
        "state": "open",
        "created_at": "2023-12-09T23:48:41Z",
        "user": "FNsi",
        "labels": []
    },
    {
        "number": 4395,
        "title": "cuBLAS error 15 at ggml-cuda.cu:7956: the requested functionality is not supported",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4395",
        "state": "open",
        "created_at": "2023-12-09T18:26:37Z",
        "user": "envolution",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4391,
        "title": "support EAGLE models (new speculative model architecture faster than Medusa)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4391",
        "state": "open",
        "created_at": "2023-12-09T11:32:48Z",
        "user": "BarfingLemurs",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4387,
        "title": "Enhancement: We need CogVLM support - extremely good image and text analysis, feels like a multi generational step forward.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4387",
        "state": "open",
        "created_at": "2023-12-09T01:16:22Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 4386,
        "title": "Add QuIP Sharp Support - New Quantization method with amazing 2-bit performance ( close to 16 bit )",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4386",
        "state": "open",
        "created_at": "2023-12-09T01:01:51Z",
        "user": "errorsandwarnings",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4385,
        "title": "Recoverable Error Handling",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4385",
        "state": "open",
        "created_at": "2023-12-09T00:24:30Z",
        "user": "martindevans",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4384,
        "title": "[Question] GPU vs Metal performance & Seeding models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4384",
        "state": "open",
        "created_at": "2023-12-09T00:04:31Z",
        "user": "aramcheck",
        "labels": []
    },
    {
        "number": 4378,
        "title": "llava: batch inference",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4378",
        "state": "open",
        "created_at": "2023-12-08T15:32:23Z",
        "user": "Borobo",
        "labels": []
    },
    {
        "number": 4377,
        "title": "High inference time ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4377",
        "state": "open",
        "created_at": "2023-12-08T13:00:56Z",
        "user": "adeelhasan19",
        "labels": []
    },
    {
        "number": 4365,
        "title": "Running convert fails with BadZipFile (Bad CRC-32)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4365",
        "state": "open",
        "created_at": "2023-12-07T16:55:30Z",
        "user": "itsJoKr",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4363,
        "title": "Implementing TokenMonster tokenizer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4363",
        "state": "open",
        "created_at": "2023-12-07T15:59:19Z",
        "user": "Sovenok-Hacker",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4360,
        "title": "Errors w/ BPE tokenizers (GGML_ASSERT: llama.cpp:2029: codepoints_from_utf8(word).size() > 0 and more)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4360",
        "state": "open",
        "created_at": "2023-12-07T13:09:21Z",
        "user": "lhl",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4355,
        "title": "Fix typos in code.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4355",
        "state": "open",
        "created_at": "2023-12-07T05:37:16Z",
        "user": "richardkiss",
        "labels": []
    },
    {
        "number": 4353,
        "title": "Possible of implementing mamba ssm",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4353",
        "state": "open",
        "created_at": "2023-12-07T04:36:19Z",
        "user": "tikikun",
        "labels": [
            "enhancement",
            "help wanted"
        ]
    },
    {
        "number": 4352,
        "title": "llama.cpp inference too long",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4352",
        "state": "open",
        "created_at": "2023-12-07T00:24:17Z",
        "user": "wzg-zhuo",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4349,
        "title": "Json integration into console commands",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4349",
        "state": "open",
        "created_at": "2023-12-06T19:04:46Z",
        "user": "MaggotHATE",
        "labels": []
    },
    {
        "number": 4346,
        "title": "Compilation Error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4346",
        "state": "open",
        "created_at": "2023-12-06T08:07:12Z",
        "user": "mitesh741",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4339,
        "title": "Separation of declarations in llama.cpp ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4339",
        "state": "open",
        "created_at": "2023-12-05T14:45:02Z",
        "user": "jmikedupont2",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4338,
        "title": "llm_build_context::build_<X>() functions refactor",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4338",
        "state": "open",
        "created_at": "2023-12-05T14:15:15Z",
        "user": "jmikedupont2",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4337,
        "title": "[Feature request] Adding Support for \"XVERSE\" model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4337",
        "state": "open",
        "created_at": "2023-12-05T11:09:22Z",
        "user": "aspwow",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4336,
        "title": "Add GPU support for training",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4336",
        "state": "open",
        "created_at": "2023-12-05T09:30:30Z",
        "user": "gocursor",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4333,
        "title": "Grammar based decoding doesn't work with DeepSeek models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4333",
        "state": "open",
        "created_at": "2023-12-05T01:55:41Z",
        "user": "Ghatage",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4331,
        "title": "Qwen-72B-Chat conversion script does not treat <|im_start|> and <|im_end|> correctly.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4331",
        "state": "open",
        "created_at": "2023-12-04T23:16:13Z",
        "user": "Noeda",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4326,
        "title": "\"main : failed to eval\" when LLM produces a long output",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4326",
        "state": "open",
        "created_at": "2023-12-04T16:01:57Z",
        "user": "l29ah",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4316,
        "title": "Support for the new 450 language translation models from Google T5X \"madlad\" - apparently Apache-2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4316",
        "state": "open",
        "created_at": "2023-12-04T00:10:01Z",
        "user": "cmp-nct",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4315,
        "title": "Memory estimation utilities",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4315",
        "state": "open",
        "created_at": "2023-12-03T22:33:18Z",
        "user": "giladgd",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4313,
        "title": "A step ahead supporting NousResearch/Nous-Hermes-2-Vision",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4313",
        "state": "open",
        "created_at": "2023-12-03T19:54:26Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 4311,
        "title": "Adding Tests for GGUFWriter Class",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4311",
        "state": "open",
        "created_at": "2023-12-03T19:31:30Z",
        "user": "mendax0110",
        "labels": []
    },
    {
        "number": 4305,
        "title": "./main -m Qwen-14B-Chat/ggml-model-q4_0.gguf -n 128  Segmentation fault ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4305",
        "state": "open",
        "created_at": "2023-12-03T07:48:39Z",
        "user": "Minami-su",
        "labels": []
    },
    {
        "number": 4302,
        "title": "persimmon : simplify rope logic",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4302",
        "state": "open",
        "created_at": "2023-12-03T03:39:31Z",
        "user": "Galunid",
        "labels": []
    },
    {
        "number": 4301,
        "title": "Add AVX_VNNI support for intel x86 processors",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4301",
        "state": "open",
        "created_at": "2023-12-03T01:35:50Z",
        "user": "freebie1101",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4300,
        "title": "train-text-from-scratch oom (in tokenizer?) ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4300",
        "state": "open",
        "created_at": "2023-12-03T01:16:26Z",
        "user": "tezlm",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4295,
        "title": "Create api_like_OAI.sh",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4295",
        "state": "open",
        "created_at": "2023-12-02T16:38:05Z",
        "user": "mounta11n",
        "labels": []
    },
    {
        "number": 4293,
        "title": "Feature Request: Implementation of Supervised Fine-Tuning in llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4293",
        "state": "open",
        "created_at": "2023-12-02T10:13:21Z",
        "user": "Drincann",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4291,
        "title": "Local LLM ISSUE GGML_ASSERT",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4291",
        "state": "open",
        "created_at": "2023-12-02T09:16:38Z",
        "user": "CoderTom314",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4288,
        "title": "[ BUG ] `cache_prompt = false` doesn't disable cache in `server`. I want to be able to **disable cache**",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4288",
        "state": "open",
        "created_at": "2023-12-02T00:33:54Z",
        "user": "ibehnam",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4286,
        "title": "Speculative Decoding?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4286",
        "state": "open",
        "created_at": "2023-12-01T18:18:26Z",
        "user": "akumaburn",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4282,
        "title": "SIGSEGV when trying to launch llama.cpp server on Intel MacBook ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4282",
        "state": "open",
        "created_at": "2023-12-01T10:30:58Z",
        "user": "chameleon-lizard",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4271,
        "title": "Error compiling CUDA kernels",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4271",
        "state": "open",
        "created_at": "2023-11-30T17:53:09Z",
        "user": "savant117",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4270,
        "title": "ggml_metal_init: default.metallib not found... Segmentation fault: 11",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4270",
        "state": "open",
        "created_at": "2023-11-30T16:27:34Z",
        "user": "rm-rf-etc",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4269,
        "title": "Context splitting",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4269",
        "state": "open",
        "created_at": "2023-11-30T16:25:48Z",
        "user": "candre23",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4268,
        "title": "llama print timeout occurs too soon",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4268",
        "state": "open",
        "created_at": "2023-11-30T15:40:00Z",
        "user": "joshknnd1982",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4267,
        "title": "Input Temperature & Output Temperature",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4267",
        "state": "open",
        "created_at": "2023-11-30T13:14:13Z",
        "user": "kalomaze",
        "labels": []
    },
    {
        "number": 4263,
        "title": "Linking against the Swift package directly gives unresolved symbols: LLAMA_BUILD_NUMBER and LLAMA_COMMIT",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4263",
        "state": "open",
        "created_at": "2023-11-30T07:00:34Z",
        "user": "l3utterfly",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4257,
        "title": "Seg fault when fine tuning in AWS g5 machine with Nvidia A10 card",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4257",
        "state": "open",
        "created_at": "2023-11-29T17:58:21Z",
        "user": "r78v10a07",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4251,
        "title": "how to understand kv_cache ring-buffer?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4251",
        "state": "open",
        "created_at": "2023-11-29T06:51:38Z",
        "user": "yancaoweidaode",
        "labels": []
    },
    {
        "number": 4249,
        "title": "Gibberish output when all layers offloaded if llamacpp is built with a normal make (Tesla P40) ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4249",
        "state": "open",
        "created_at": "2023-11-28T22:58:09Z",
        "user": "Dirky14",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4247,
        "title": "How can I access the string belonging to a token ID during a sampling function?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4247",
        "state": "open",
        "created_at": "2023-11-28T20:20:01Z",
        "user": "kalomaze",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4245,
        "title": "server OAI API does not handle image_data in multimodal",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4245",
        "state": "open",
        "created_at": "2023-11-28T10:53:36Z",
        "user": "edwios",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4244,
        "title": "MPI run on M1 Max",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4244",
        "state": "open",
        "created_at": "2023-11-28T09:58:37Z",
        "user": "ageorgios",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4243,
        "title": "Launching Server With Parameters",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4243",
        "state": "open",
        "created_at": "2023-11-28T04:03:11Z",
        "user": "chigkim",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4238,
        "title": "Clarification for multi GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4238",
        "state": "open",
        "created_at": "2023-11-27T16:55:58Z",
        "user": "jholla-atx",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4236,
        "title": "Server UI improvements",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4236",
        "state": "open",
        "created_at": "2023-11-27T13:07:22Z",
        "user": "mounta11n",
        "labels": [
            "need feedback"
        ]
    },
    {
        "number": 4234,
        "title": "adding support for tiva (text-image-video-audio) models-- NExTGPT-7B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4234",
        "state": "open",
        "created_at": "2023-11-27T04:29:09Z",
        "user": "itsPreto",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4233,
        "title": "cuda : tweak mm stride to double perf on P40 + GTX 970",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4233",
        "state": "open",
        "created_at": "2023-11-27T03:51:54Z",
        "user": "cebtenzzre",
        "labels": []
    },
    {
        "number": 4218,
        "title": "llama : speed-up grammar sampling",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4218",
        "state": "open",
        "created_at": "2023-11-25T17:04:06Z",
        "user": "ggerganov",
        "labels": [
            "performance",
            "refactoring"
        ]
    },
    {
        "number": 4216,
        "title": "server : improvements and maintenance",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4216",
        "state": "open",
        "created_at": "2023-11-25T09:57:53Z",
        "user": "ggerganov",
        "labels": [
            "help wanted",
            "refactoring",
            "server/webui"
        ]
    },
    {
        "number": 4212,
        "title": "CUDA error 700 at ggml-cuda.cu:6963: an illegal memory access was encountered for finetune using cuda",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4212",
        "state": "open",
        "created_at": "2023-11-25T01:55:26Z",
        "user": "jooray",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4206,
        "title": "terminate running deepseek models with gbnf grammars",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4206",
        "state": "open",
        "created_at": "2023-11-24T18:25:49Z",
        "user": "54rt1n",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4201,
        "title": "Server slowing down with each request (requests are identical)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4201",
        "state": "open",
        "created_at": "2023-11-24T12:15:58Z",
        "user": "ruped",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 4200,
        "title": "Why avx512 is not used",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4200",
        "state": "open",
        "created_at": "2023-11-24T11:45:54Z",
        "user": "hhy3",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4199,
        "title": "KeyError: 'I8' when trying to convert finetuned 8bit model to GGUF",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4199",
        "state": "open",
        "created_at": "2023-11-24T11:14:58Z",
        "user": "Lue-C",
        "labels": []
    },
    {
        "number": 4188,
        "title": "Endless Responses and Imaginary Conversations in llama.cpp Prompted Models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4188",
        "state": "open",
        "created_at": "2023-11-23T20:21:03Z",
        "user": "jovanlopez32",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4185,
        "title": "update_slots : failed to decode the batch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4185",
        "state": "open",
        "created_at": "2023-11-23T14:28:28Z",
        "user": "rvandernoort",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4184,
        "title": "error: unknown argument: --logits-all",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4184",
        "state": "open",
        "created_at": "2023-11-23T12:45:19Z",
        "user": "AntonioZC666",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 4176,
        "title": "Dynamic matrix tile quantization and fine-tune w vector approximation.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4176",
        "state": "open",
        "created_at": "2023-11-22T22:11:02Z",
        "user": "chadbrewbaker",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4166,
        "title": "Produced text tokens do not always match with token top_logprobs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4166",
        "state": "open",
        "created_at": "2023-11-22T08:22:48Z",
        "user": "tomkouwenhoven",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4165,
        "title": "GPTQ / ExLlamaV2 (EXL2) quantisation",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4165",
        "state": "open",
        "created_at": "2023-11-22T06:08:43Z",
        "user": "0xdevalias",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4162,
        "title": "pkg-config: could not find package 'openblas'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4162",
        "state": "open",
        "created_at": "2023-11-22T05:52:38Z",
        "user": "BingoZZBZZ",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4161,
        "title": "pkg-config: could not find package 'openblas'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4161",
        "state": "open",
        "created_at": "2023-11-22T05:51:13Z",
        "user": "BingoZZBZZ",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4149,
        "title": "[Feature Request] Dynamically update grammar rules during generation",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4149",
        "state": "open",
        "created_at": "2023-11-20T18:30:37Z",
        "user": "AlienKevin",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4145,
        "title": "error: inlining failed in call to \u2018always_inline\u2019 \u2018_mm256_fmadd_ps\u2019: target specific option mismatch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4145",
        "state": "open",
        "created_at": "2023-11-20T08:36:04Z",
        "user": "Fyutong",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4132,
        "title": "adding image generation capabilities?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4132",
        "state": "open",
        "created_at": "2023-11-19T03:37:29Z",
        "user": "itsPreto",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4123,
        "title": "does not compile on CUDA 10 anymore",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4123",
        "state": "open",
        "created_at": "2023-11-18T10:38:49Z",
        "user": "whoreson",
        "labels": []
    },
    {
        "number": 4120,
        "title": "Could not find tokenizer.model in modified Mistral model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4120",
        "state": "open",
        "created_at": "2023-11-18T00:44:54Z",
        "user": "fakerybakery",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4119,
        "title": "Windows cuda gpu compilation issue: nvcc fatal",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4119",
        "state": "open",
        "created_at": "2023-11-18T00:00:15Z",
        "user": "alando46",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4111,
        "title": "Llama 2 70b quantizes in way that's superior for GQA; Mistral 7b is missing that optimization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4111",
        "state": "open",
        "created_at": "2023-11-17T10:31:41Z",
        "user": "kalomaze",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4109,
        "title": "Finetuning on AMD GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4109",
        "state": "open",
        "created_at": "2023-11-17T01:46:47Z",
        "user": "fakerybakery",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4106,
        "title": "Token to piece (#1)",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4106",
        "state": "open",
        "created_at": "2023-11-17T01:17:01Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 4105,
        "title": "Update api_like_OAI.py server script to support text in mixed-mode data",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4105",
        "state": "open",
        "created_at": "2023-11-17T00:12:28Z",
        "user": "varon",
        "labels": []
    },
    {
        "number": 4103,
        "title": "server: system prompt makes generated text incoherent",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4103",
        "state": "open",
        "created_at": "2023-11-16T17:48:59Z",
        "user": "z80maniac",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4099,
        "title": "Compilation error on Nvidia Jetson Nano",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4099",
        "state": "open",
        "created_at": "2023-11-16T11:56:41Z",
        "user": "rvandernoort",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4098,
        "title": "LlamaCpp - Output Lenght with RetrievalQA-chain from Langchain",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4098",
        "state": "open",
        "created_at": "2023-11-16T08:47:49Z",
        "user": "weissenbacherpwc",
        "labels": []
    },
    {
        "number": 4093,
        "title": "YaRN : correction to GPT-NeoX implementation",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4093",
        "state": "open",
        "created_at": "2023-11-15T22:19:50Z",
        "user": "cebtenzzre",
        "labels": []
    },
    {
        "number": 4085,
        "title": "metal : compile-time kernel args and params",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4085",
        "state": "open",
        "created_at": "2023-11-15T11:09:39Z",
        "user": "ggerganov",
        "labels": [
            "performance",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 4072,
        "title": "warning: failed to mlock in Docker",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4072",
        "state": "open",
        "created_at": "2023-11-14T14:46:07Z",
        "user": "sengiv",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4070,
        "title": "Update gpt2 preprocess and add deepseek coder preprocess",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4070",
        "state": "open",
        "created_at": "2023-11-14T09:51:12Z",
        "user": "DOGEwbx",
        "labels": []
    },
    {
        "number": 4066,
        "title": "Segfault in grammar",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4066",
        "state": "open",
        "created_at": "2023-11-13T21:12:26Z",
        "user": "jmikedupont2",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4065,
        "title": "segfault with ebnf",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4065",
        "state": "open",
        "created_at": "2023-11-13T20:50:19Z",
        "user": "jmikedupont2",
        "labels": []
    },
    {
        "number": 4063,
        "title": "struct gpt_params should not use LLAMA_MAX_DEVICES",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4063",
        "state": "open",
        "created_at": "2023-11-13T18:32:39Z",
        "user": "Displacer",
        "labels": []
    },
    {
        "number": 4062,
        "title": "gguf-py readme example fixes for keys: general.architecture and general.alignment",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4062",
        "state": "open",
        "created_at": "2023-11-13T17:47:06Z",
        "user": "jay-johnson",
        "labels": []
    },
    {
        "number": 4059,
        "title": "can't use embedding model for PDF chunk vectorize",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4059",
        "state": "open",
        "created_at": "2023-11-13T12:41:36Z",
        "user": "greywolf0324",
        "labels": []
    },
    {
        "number": 4047,
        "title": "Requesting less than 3 tokens from the `server` generates more tokens than needed",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4047",
        "state": "open",
        "created_at": "2023-11-12T11:42:41Z",
        "user": "z80maniac",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4044,
        "title": "Langchain LlamaCpp return empty response when hit multiple times",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4044",
        "state": "open",
        "created_at": "2023-11-12T04:28:58Z",
        "user": "muhammadfhadli1453",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4038,
        "title": "Adept Persimmon Models not working with CUDA Acceleration",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4038",
        "state": "open",
        "created_at": "2023-11-11T14:46:23Z",
        "user": "maddes8cht",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 4035,
        "title": "llama_get_kv_cache_token_count() deprecation hurts debugging - suggested API enhancement",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4035",
        "state": "open",
        "created_at": "2023-11-11T09:26:24Z",
        "user": "WeirdConstructor",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4034,
        "title": "chat in multi-modal resets the prompt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4034",
        "state": "open",
        "created_at": "2023-11-11T07:17:24Z",
        "user": "m-a-sch",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 4030,
        "title": "Segmentation fault after model load on ROCm multi-gpu, multi-gfx",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4030",
        "state": "open",
        "created_at": "2023-11-11T01:01:05Z",
        "user": "xangelix",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4028,
        "title": "Fix bug where POST /infill doesn't work without prompt argument",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4028",
        "state": "open",
        "created_at": "2023-11-10T20:29:06Z",
        "user": "jonastemplestein",
        "labels": []
    },
    {
        "number": 4027,
        "title": "POST /infill on server example returns empty content unless non-empty \"prompt\" is passed in",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4027",
        "state": "open",
        "created_at": "2023-11-10T20:15:48Z",
        "user": "jonastemplestein",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4023,
        "title": "Fine-tuned model where k-quantization produces gibberish, other quantization (q4_0, q5_0) works",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4023",
        "state": "open",
        "created_at": "2023-11-10T15:21:22Z",
        "user": "matthiasdg",
        "labels": []
    },
    {
        "number": 4016,
        "title": "Finetune GPU Utilization fell to 0%",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4016",
        "state": "open",
        "created_at": "2023-11-10T08:17:21Z",
        "user": "L16H7",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4012,
        "title": "added lto",
        "url": "https://github.com/ggerganov/llama.cpp/pull/4012",
        "state": "open",
        "created_at": "2023-11-09T21:20:07Z",
        "user": "chadbrewbaker",
        "labels": []
    },
    {
        "number": 4006,
        "title": "Configuration for NVidia Jetson Xavier SOM required Makefile change",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4006",
        "state": "open",
        "created_at": "2023-11-09T11:24:58Z",
        "user": "mehmetalianil",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4005,
        "title": "State copy or set messes up results",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4005",
        "state": "open",
        "created_at": "2023-11-09T11:01:39Z",
        "user": "niansa",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 4004,
        "title": "Metal issues with Intel Mac",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4004",
        "state": "open",
        "created_at": "2023-11-09T10:37:39Z",
        "user": "zhangchn",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4001,
        "title": "WizardCoder Inference accuracy dropped a lot compared to fastchat or vllm",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4001",
        "state": "open",
        "created_at": "2023-11-09T02:13:09Z",
        "user": "sanigochien",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 4000,
        "title": "Override EOS token / Specify multiple EOS tokens",
        "url": "https://github.com/ggerganov/llama.cpp/issues/4000",
        "state": "open",
        "created_at": "2023-11-09T01:44:48Z",
        "user": "j7z",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3997,
        "title": "System Prompts Not Loading From -spf",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3997",
        "state": "open",
        "created_at": "2023-11-08T21:49:42Z",
        "user": "ActuallyTaylor",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3993,
        "title": "How to send Prompt in falcon-180b model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3993",
        "state": "open",
        "created_at": "2023-11-08T15:58:29Z",
        "user": "Prashantsaini25",
        "labels": []
    },
    {
        "number": 3992,
        "title": "Can't run inference with lora extensions produced by finetune",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3992",
        "state": "open",
        "created_at": "2023-11-08T15:18:49Z",
        "user": "gbecigneul",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3991,
        "title": "Stuck loading VRAM ROCm multi gpu",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3991",
        "state": "open",
        "created_at": "2023-11-08T10:22:50Z",
        "user": "bojak83318",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3990,
        "title": "cuda release binary for win uses BMI2 ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3990",
        "state": "open",
        "created_at": "2023-11-08T08:41:30Z",
        "user": "slavonnet",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3989,
        "title": "LlamaCpp: Set max_tokens",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3989",
        "state": "open",
        "created_at": "2023-11-08T08:34:44Z",
        "user": "weissenbacherpwc",
        "labels": []
    },
    {
        "number": 3985,
        "title": "Save Chat History into New Prompts",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3985",
        "state": "open",
        "created_at": "2023-11-08T03:48:09Z",
        "user": "Crear12",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3984,
        "title": "contrastive: PoC for improving reasoning via contrastive decoding",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3984",
        "state": "open",
        "created_at": "2023-11-08T01:18:30Z",
        "user": "trabbart",
        "labels": []
    },
    {
        "number": 3980,
        "title": "Grammar sampler implementation causes non-trivial token speed degradation",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3980",
        "state": "open",
        "created_at": "2023-11-07T14:53:48Z",
        "user": "kalomaze",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3979,
        "title": "openai-like api endpoint doesn't work well as before in b1382",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3979",
        "state": "open",
        "created_at": "2023-11-07T14:20:27Z",
        "user": "gaord",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3978,
        "title": "enhancement: replace \"invalid magic characters\" text with something more descriptive ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3978",
        "state": "open",
        "created_at": "2023-11-07T10:41:39Z",
        "user": "staviq",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3973,
        "title": "Error C2026 when building ggml-opencl.cpp with MSVC",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3973",
        "state": "open",
        "created_at": "2023-11-06T21:22:11Z",
        "user": "stduhpf",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3971,
        "title": "[Potential issue] Does it matter that the probabilities are not being renormalized before Temperature is called?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3971",
        "state": "open",
        "created_at": "2023-11-06T17:47:27Z",
        "user": "kalomaze",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3964,
        "title": "http://localhost:6800/jsonrpc",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3964",
        "state": "open",
        "created_at": "2023-11-05T19:00:15Z",
        "user": "Yazed550",
        "labels": [
            "invalid",
            "need more info"
        ]
    },
    {
        "number": 3960,
        "title": "ggml : become thread-safe",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3960",
        "state": "open",
        "created_at": "2023-11-05T15:56:19Z",
        "user": "ggerganov",
        "labels": [
            "refactoring"
        ]
    },
    {
        "number": 3957,
        "title": "GGUF endianness cannot be determined from GGUF itself",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3957",
        "state": "open",
        "created_at": "2023-11-05T14:00:47Z",
        "user": "philpax",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3953,
        "title": "converting LORA to ggml to gguf",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3953",
        "state": "open",
        "created_at": "2023-11-05T09:56:19Z",
        "user": "xcottos",
        "labels": []
    },
    {
        "number": 3948,
        "title": "Finetuning in chat-mode and with input-output not yet supported",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3948",
        "state": "open",
        "created_at": "2023-11-05T00:21:51Z",
        "user": "gbecigneul",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3936,
        "title": "ggml_opencl error -1 on Intel Raptor Lake-P [Iris Xe Graphics]",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3936",
        "state": "open",
        "created_at": "2023-11-03T16:29:41Z",
        "user": "ParetoOptimalDev",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3934,
        "title": "Changing prompt(template) in the server web interface does not affect generation when using multimodal mode",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3934",
        "state": "open",
        "created_at": "2023-11-03T16:06:08Z",
        "user": "Answer-is-not-42",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3929,
        "title": "CUDA/HIP stream usage on ROCm causes constant 100% GPU load - support disabling streams on ROCm",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3929",
        "state": "open",
        "created_at": "2023-11-03T11:33:21Z",
        "user": "Googulator",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3919,
        "title": "Possible Speculative Decoding Bug",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3919",
        "state": "open",
        "created_at": "2023-11-02T20:28:39Z",
        "user": "williammm001",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3911,
        "title": "finetune -ngl not offloading to GPU,  Metal M2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3911",
        "state": "open",
        "created_at": "2023-11-02T14:16:35Z",
        "user": "QueryType",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3900,
        "title": "Vocab size mismatch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3900",
        "state": "open",
        "created_at": "2023-11-02T07:32:11Z",
        "user": "eswarthammana",
        "labels": []
    },
    {
        "number": 3896,
        "title": "70b models on rocm endless prompt processing / gpu errors",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3896",
        "state": "open",
        "created_at": "2023-11-02T02:19:40Z",
        "user": "ChristophHaag",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3894,
        "title": "Mac Metal: error on ggml_metal_init",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3894",
        "state": "open",
        "created_at": "2023-11-02T00:10:01Z",
        "user": "spencekim",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3892,
        "title": "Illegal instruction in librocblas.so.3.0",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3892",
        "state": "open",
        "created_at": "2023-11-01T19:37:19Z",
        "user": "tranquillity-codes",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3883,
        "title": "Starting the model using. main has context understanding ability, but deploying the model using. server does not have context understanding ability",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3883",
        "state": "open",
        "created_at": "2023-11-01T10:40:07Z",
        "user": "948024326",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3880,
        "title": "Can't compile \"llama.cpp/ggml-quants.c\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3880",
        "state": "open",
        "created_at": "2023-11-01T03:34:20Z",
        "user": "ByerRA",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3878,
        "title": "Malformed grammar crashes the server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3878",
        "state": "open",
        "created_at": "2023-10-31T22:54:32Z",
        "user": "dkogut1996",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3867,
        "title": "Mistral & Sliding Window Attention - GGUF ROPE accomodation.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3867",
        "state": "open",
        "created_at": "2023-10-31T08:25:25Z",
        "user": "SabinStargem",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3865,
        "title": "Support `mmap` when using LORAs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3865",
        "state": "open",
        "created_at": "2023-10-31T05:27:17Z",
        "user": "l3utterfly",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3860,
        "title": "GPU NOT used during \"normal generation\" when ONE LAYER offloaded (But GPU used in prompt evaluation)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3860",
        "state": "open",
        "created_at": "2023-10-30T13:46:49Z",
        "user": "Nate687",
        "labels": []
    },
    {
        "number": 3852,
        "title": "Finetune produces unusable LoRA for Mistral model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3852",
        "state": "open",
        "created_at": "2023-10-29T20:26:20Z",
        "user": "maxxk",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3851,
        "title": "Windows CUDA Immediately Exits",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3851",
        "state": "open",
        "created_at": "2023-10-29T17:49:41Z",
        "user": "JeremyBickel",
        "labels": []
    },
    {
        "number": 3850,
        "title": "Dynamic Range Calibration",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3850",
        "state": "open",
        "created_at": "2023-10-29T17:06:21Z",
        "user": "eswarthammana",
        "labels": []
    },
    {
        "number": 3846,
        "title": "Bug: min value is set to 0 tor super block quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3846",
        "state": "open",
        "created_at": "2023-10-29T10:06:32Z",
        "user": "shlevi-microsoft",
        "labels": []
    },
    {
        "number": 3845,
        "title": "OpenCL: Pass src0 offset as kernel argument instead of global offset",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3845",
        "state": "open",
        "created_at": "2023-10-29T09:01:24Z",
        "user": "shibe2",
        "labels": []
    },
    {
        "number": 3832,
        "title": "Assertion failed: ggml_allocr_is_measure(alloc) in Debug Configuration",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3832",
        "state": "open",
        "created_at": "2023-10-28T13:26:08Z",
        "user": "dmilos",
        "labels": [
            "bug-unconfirmed"
        ]
    },
    {
        "number": 3824,
        "title": "gguf-py not in flake.nix?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3824",
        "state": "open",
        "created_at": "2023-10-28T04:32:14Z",
        "user": "zk395",
        "labels": [
            "enhancement",
            "nix"
        ]
    },
    {
        "number": 3821,
        "title": "Server parallelization works - but now refuses with 'slot unavailable' rather than entering requests into a queue.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3821",
        "state": "open",
        "created_at": "2023-10-27T22:51:21Z",
        "user": "jaballski",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3816,
        "title": "cuda : speed-up by using CUBLAS_COMPUTE_32F instead of CUBLAS_COMPUTE_16F",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3816",
        "state": "open",
        "created_at": "2023-10-27T15:46:41Z",
        "user": "ggerganov",
        "labels": [
            "demo",
            "Nvidia GPU"
        ]
    },
    {
        "number": 3811,
        "title": "Server completion endpoint receive embeddings",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3811",
        "state": "open",
        "created_at": "2023-10-27T11:49:34Z",
        "user": "Johnz86",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3808,
        "title": "When I used the tool to quantify the chatglm model, the following error was reported",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3808",
        "state": "open",
        "created_at": "2023-10-27T02:51:16Z",
        "user": "whisky-12",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3807,
        "title": "Tokens being skipped in Swift",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3807",
        "state": "open",
        "created_at": "2023-10-27T01:04:46Z",
        "user": "EthanLipnik",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3802,
        "title": "Possible wrong implementation of beam search",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3802",
        "state": "open",
        "created_at": "2023-10-26T21:14:45Z",
        "user": "shenjiangqiu",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3801,
        "title": "Segfault when using simple self-referential grammar on M2 Mac",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3801",
        "state": "open",
        "created_at": "2023-10-26T19:18:20Z",
        "user": "jonastemplestein",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3799,
        "title": "Finetuning not using Metal/Apple Silicon GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3799",
        "state": "open",
        "created_at": "2023-10-26T15:53:19Z",
        "user": "fakerybakery",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3796,
        "title": "How to stock prompt's result into txt file ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3796",
        "state": "open",
        "created_at": "2023-10-26T13:30:50Z",
        "user": "oceanedruenne",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3794,
        "title": "Strange results when CLBlast and Metal are both enabled and ngl > 1",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3794",
        "state": "open",
        "created_at": "2023-10-26T10:56:10Z",
        "user": "akx",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3789,
        "title": "Unable to finetune Bloom",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3789",
        "state": "open",
        "created_at": "2023-10-26T01:51:12Z",
        "user": "fakerybakery",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3783,
        "title": "#3746 introduces error in convert-mpt-hf-to-gguf.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3783",
        "state": "open",
        "created_at": "2023-10-25T20:43:21Z",
        "user": "maddes8cht",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3780,
        "title": "b1428 OOM error on 3x P40 setup",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3780",
        "state": "open",
        "created_at": "2023-10-25T18:33:52Z",
        "user": "quarterturn",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3778,
        "title": "Support for LLAMA V.1.5 Multimodal LLMs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3778",
        "state": "open",
        "created_at": "2023-10-25T15:21:44Z",
        "user": "Dyl777",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3777,
        "title": "AMX isa Native addition",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3777",
        "state": "open",
        "created_at": "2023-10-25T12:34:02Z",
        "user": "abhilash1910",
        "labels": []
    },
    {
        "number": 3775,
        "title": "Inconsistent Handling of gguf_writer.add_name in convert-to-gguf.py Scripts",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3775",
        "state": "open",
        "created_at": "2023-10-25T11:34:43Z",
        "user": "maddes8cht",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3773,
        "title": "Inconsistencies in Rope Dimension Count Specification in convert-to-gguf.py Scripts",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3773",
        "state": "open",
        "created_at": "2023-10-25T08:27:11Z",
        "user": "maddes8cht",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3772,
        "title": "multi-gpu inference produces broken output",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3772",
        "state": "open",
        "created_at": "2023-10-25T06:35:00Z",
        "user": "nih23",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3769,
        "title": "suggestion for simpler implementation ggml_vec_dot_q4_K_q8_K ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3769",
        "state": "open",
        "created_at": "2023-10-24T22:17:45Z",
        "user": "shlevi-microsoft",
        "labels": []
    },
    {
        "number": 3759,
        "title": "Finetuning on tensors with type 'f16' is not yet supported.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3759",
        "state": "open",
        "created_at": "2023-10-24T11:19:24Z",
        "user": "Shookapic",
        "labels": []
    },
    {
        "number": 3756,
        "title": "Loading model too verbose",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3756",
        "state": "open",
        "created_at": "2023-10-24T02:32:12Z",
        "user": "aiaicode",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3752,
        "title": "Trouble running llama.cpp compiled for OpenMPI",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3752",
        "state": "open",
        "created_at": "2023-10-23T22:14:38Z",
        "user": "Jdo300",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3744,
        "title": "server unable to load model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3744",
        "state": "open",
        "created_at": "2023-10-23T14:06:45Z",
        "user": "jonty-esterhuizen",
        "labels": []
    },
    {
        "number": 3741,
        "title": "I want to add my PDF also in GGUF model and serve via llama-cpp-server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3741",
        "state": "open",
        "created_at": "2023-10-23T11:01:17Z",
        "user": "Prashantsaini25",
        "labels": []
    },
    {
        "number": 3738,
        "title": "server: Cache is not reused between completions by default.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3738",
        "state": "open",
        "created_at": "2023-10-23T08:21:08Z",
        "user": "shibe2",
        "labels": []
    },
    {
        "number": 3736,
        "title": "when support Qwen-7b",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3736",
        "state": "open",
        "created_at": "2023-10-23T05:09:23Z",
        "user": "AppleJunJiang",
        "labels": []
    },
    {
        "number": 3735,
        "title": "[User] when I use the server command to start as a service , error: Segmentation fault (core dumped)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3735",
        "state": "open",
        "created_at": "2023-10-23T03:57:17Z",
        "user": "whisky-12",
        "labels": []
    },
    {
        "number": 3732,
        "title": "CausalLM: Llama + vocab.json BPE tokenizer = `error loading model: cannot find tokenizer merges in model file`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3732",
        "state": "open",
        "created_at": "2023-10-22T21:30:13Z",
        "user": "TheBloke",
        "labels": [
            "model"
        ]
    },
    {
        "number": 3723,
        "title": "server: Default web UI erroneously inteprets markdown special characters inside code blocks",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3723",
        "state": "open",
        "created_at": "2023-10-22T06:40:53Z",
        "user": "jrichey98",
        "labels": []
    },
    {
        "number": 3717,
        "title": "Server executable should not consume cpu/gpu power when idle.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3717",
        "state": "open",
        "created_at": "2023-10-21T19:34:38Z",
        "user": "moritz-pv",
        "labels": []
    },
    {
        "number": 3716,
        "title": "Loading a model with a big context crashes",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3716",
        "state": "open",
        "created_at": "2023-10-21T18:55:38Z",
        "user": "giladgd",
        "labels": []
    },
    {
        "number": 3714,
        "title": "[LoRA] Support safetensors lora",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3714",
        "state": "open",
        "created_at": "2023-10-21T13:27:55Z",
        "user": "Ph0rk0z",
        "labels": []
    },
    {
        "number": 3713,
        "title": "[LoRA] Falcon merges still don't work. Ideas as to why?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3713",
        "state": "open",
        "created_at": "2023-10-21T13:25:41Z",
        "user": "Ph0rk0z",
        "labels": []
    },
    {
        "number": 3712,
        "title": "segmentation fault on 0.2.11 when just starting generated",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3712",
        "state": "open",
        "created_at": "2023-10-21T13:16:51Z",
        "user": "wzg-zhuo",
        "labels": []
    },
    {
        "number": 3709,
        "title": "[User] Using server.exe, but the server.exe crash",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3709",
        "state": "open",
        "created_at": "2023-10-21T10:20:19Z",
        "user": "lenwang1994",
        "labels": []
    },
    {
        "number": 3707,
        "title": "Error with convert-finetune-checkpoint-to-gguf.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3707",
        "state": "open",
        "created_at": "2023-10-21T04:37:04Z",
        "user": "Drael64",
        "labels": []
    },
    {
        "number": 3705,
        "title": "special token handling sometimes produces garbage output with AMD ROCM/HIP",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3705",
        "state": "open",
        "created_at": "2023-10-21T02:19:36Z",
        "user": "hansejo",
        "labels": [
            "generation quality",
            "AMD GPU"
        ]
    },
    {
        "number": 3701,
        "title": "Prompt evaluation performance regression in llama.cpp on RDNA3 with HSA_OVERRIDE_GFX_VERSION=11.0.1 vs 11.0.0",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3701",
        "state": "open",
        "created_at": "2023-10-20T17:47:41Z",
        "user": "Googulator",
        "labels": []
    },
    {
        "number": 3694,
        "title": "How to enable OpenCL with llama.cpp in Android App?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3694",
        "state": "open",
        "created_at": "2023-10-20T08:26:58Z",
        "user": "DavdGao",
        "labels": []
    },
    {
        "number": 3692,
        "title": "When I run :main -m D:/LLM/Mytest/llama.cpp/models/L-13B/ggml-model-f16.gguf -n 128;llm_load_vocab: mismatch in special tokens definition ( 889/55296 vs 259/55296 ).",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3692",
        "state": "open",
        "created_at": "2023-10-20T04:57:57Z",
        "user": "OuXiao514",
        "labels": []
    },
    {
        "number": 3689,
        "title": "toy LlamaCppEmbeddings causes ggml allocation failure",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3689",
        "state": "open",
        "created_at": "2023-10-19T20:11:40Z",
        "user": "devzzzero",
        "labels": []
    },
    {
        "number": 3688,
        "title": "console: fix getwchar failing when LC_ALL undefined",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3688",
        "state": "open",
        "created_at": "2023-10-19T19:47:17Z",
        "user": "staviq",
        "labels": []
    },
    {
        "number": 3687,
        "title": "Documentation to understand the implementation details",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3687",
        "state": "open",
        "created_at": "2023-10-19T17:44:34Z",
        "user": "Darshvino",
        "labels": []
    },
    {
        "number": 3675,
        "title": "[feature] Exempt arbitrary tokens from penalties",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3675",
        "state": "open",
        "created_at": "2023-10-19T08:11:50Z",
        "user": "shibe2",
        "labels": []
    },
    {
        "number": 3671,
        "title": "examples/train-text-from-scratch with cublas build results in alloc error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3671",
        "state": "open",
        "created_at": "2023-10-19T01:00:31Z",
        "user": "Corallus-Caninus",
        "labels": []
    },
    {
        "number": 3670,
        "title": "CMakeLists.txt has no fallback for generating build-info.h if .git is missing from source zip files",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3670",
        "state": "open",
        "created_at": "2023-10-18T23:30:12Z",
        "user": "bobqianic",
        "labels": [
            "bug",
            "build"
        ]
    },
    {
        "number": 3664,
        "title": "[feature] Make space insertion optional in SentencePiece tokenizer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3664",
        "state": "open",
        "created_at": "2023-10-18T07:40:44Z",
        "user": "shibe2",
        "labels": []
    },
    {
        "number": 3659,
        "title": "[User] qwen vl support and comparative performance vs llava",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3659",
        "state": "open",
        "created_at": "2023-10-18T04:29:56Z",
        "user": "Kerushii",
        "labels": []
    },
    {
        "number": 3654,
        "title": "api_like_OAI.py different GPT-GUIs hanging in response",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3654",
        "state": "open",
        "created_at": "2023-10-17T15:48:47Z",
        "user": "ahoepf",
        "labels": []
    },
    {
        "number": 3649,
        "title": "Speculative Decoding is slower than expected on A100",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3649",
        "state": "open",
        "created_at": "2023-10-17T04:22:20Z",
        "user": "LiuXiaoxuanPKU",
        "labels": []
    },
    {
        "number": 3647,
        "title": "./server endpoint doesn't support --rope-scale parameter",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3647",
        "state": "open",
        "created_at": "2023-10-16T20:47:24Z",
        "user": "IridiumMaster",
        "labels": []
    },
    {
        "number": 3644,
        "title": "[User] Unable to Finetune Llama 2 70B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3644",
        "state": "open",
        "created_at": "2023-10-16T14:29:16Z",
        "user": "mrroll",
        "labels": []
    },
    {
        "number": 3642,
        "title": "examples/finetune: mlock option not supported",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3642",
        "state": "open",
        "created_at": "2023-10-16T10:08:58Z",
        "user": "Corallus-Caninus",
        "labels": []
    },
    {
        "number": 3640,
        "title": "New models Sequelbox/StellarBright and ValiantLabs/ShiningValiant cannot be converted due to \"Unexpected tensor name: model.layers.0.self_attn.q_proj.lora_A.default.weight\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3640",
        "state": "open",
        "created_at": "2023-10-15T21:29:11Z",
        "user": "TheBloke",
        "labels": []
    },
    {
        "number": 3638,
        "title": "[User] Hitting special characters keys on AZERTY keyboards starts infinite generation in interactive mode",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3638",
        "state": "open",
        "created_at": "2023-10-15T19:41:09Z",
        "user": "BakaKiller",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3637,
        "title": "Segmentation fault when running llava",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3637",
        "state": "open",
        "created_at": "2023-10-15T15:04:24Z",
        "user": "michaelbogdan",
        "labels": []
    },
    {
        "number": 3630,
        "title": "[Question] WARP_SIZE as 64 for MI GPUs?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3630",
        "state": "open",
        "created_at": "2023-10-15T09:14:05Z",
        "user": "jammm",
        "labels": []
    },
    {
        "number": 3628,
        "title": "ci : fix Docker workflow",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3628",
        "state": "open",
        "created_at": "2023-10-15T06:37:47Z",
        "user": "ggerganov",
        "labels": [
            "help wanted",
            "build"
        ]
    },
    {
        "number": 3611,
        "title": "Building C object CMakeFiles/ggml.dir/ggml.c.o failed",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3611",
        "state": "open",
        "created_at": "2023-10-13T05:09:03Z",
        "user": "PingEnLu",
        "labels": []
    },
    {
        "number": 3609,
        "title": "\u534e\u4e3a\u624b\u673a\uff0ctermux \u4e0amake\u540e\uff0c\u751f\u6210\u7684chat.sh\u6587\u4ef6\uff0c\u4e0d\u5728scripts/llama-cpp\u76ee\u5f55\u4e0b",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3609",
        "state": "open",
        "created_at": "2023-10-13T00:46:18Z",
        "user": "ewwerpm",
        "labels": []
    },
    {
        "number": 3608,
        "title": "windows10\u7cfb\u7edf\uff0ccmake\u7f16\u8bd1\u7684\u65f6\u5019\uff0c\u51fa\u95ee\u9898",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3608",
        "state": "open",
        "created_at": "2023-10-13T00:32:02Z",
        "user": "ewwerpm",
        "labels": []
    },
    {
        "number": 3604,
        "title": "MPT: tokenization crashes",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3604",
        "state": "open",
        "created_at": "2023-10-12T21:07:35Z",
        "user": "jploski",
        "labels": []
    },
    {
        "number": 3602,
        "title": "multimodal - Improve LLaVA model accuracy and performance",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3602",
        "state": "open",
        "created_at": "2023-10-12T19:34:59Z",
        "user": "monatis",
        "labels": []
    },
    {
        "number": 3594,
        "title": "Sparsity support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3594",
        "state": "open",
        "created_at": "2023-10-12T04:53:25Z",
        "user": "BDHU",
        "labels": []
    },
    {
        "number": 3593,
        "title": "Running Lllava in interactive mode just Quits after generating response without waiting for next prompt.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3593",
        "state": "open",
        "created_at": "2023-10-12T04:23:58Z",
        "user": "chigkim",
        "labels": []
    },
    {
        "number": 3583,
        "title": "Unable to convert Mistral-7B-OpenOrca to GGUF",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3583",
        "state": "open",
        "created_at": "2023-10-11T13:57:42Z",
        "user": "Nate687",
        "labels": []
    },
    {
        "number": 3569,
        "title": "[User] Switch to semvar",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3569",
        "state": "open",
        "created_at": "2023-10-10T13:51:10Z",
        "user": "magnusviri",
        "labels": []
    },
    {
        "number": 3566,
        "title": "Memory Leak Issue in LLaMA Model on Nvidia Jetson Xavier NX (Arm Architecture)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3566",
        "state": "open",
        "created_at": "2023-10-10T05:22:42Z",
        "user": "CYOT",
        "labels": []
    },
    {
        "number": 3565,
        "title": "Layer skipping/self-speculation demo",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3565",
        "state": "open",
        "created_at": "2023-10-10T01:08:48Z",
        "user": "KerfuffleV2",
        "labels": [
            "research \ud83d\udd2c",
            "demo"
        ]
    },
    {
        "number": 3561,
        "title": "How to make RLHF?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3561",
        "state": "open",
        "created_at": "2023-10-09T19:27:40Z",
        "user": "yukiarimo",
        "labels": []
    },
    {
        "number": 3551,
        "title": "internlm-chat-7b can convert to gguf but can not run",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3551",
        "state": "open",
        "created_at": "2023-10-09T03:08:15Z",
        "user": "lizhiling12345",
        "labels": []
    },
    {
        "number": 3547,
        "title": "[Janek] export-lora error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3547",
        "state": "open",
        "created_at": "2023-10-08T13:49:29Z",
        "user": "janek1969",
        "labels": []
    },
    {
        "number": 3531,
        "title": "model: refact-1_6B-fim unable to load model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3531",
        "state": "open",
        "created_at": "2023-10-07T17:48:12Z",
        "user": "nortekax",
        "labels": []
    },
    {
        "number": 3520,
        "title": "Unable to use more than 1 CUDA device",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3520",
        "state": "open",
        "created_at": "2023-10-07T03:11:28Z",
        "user": "ScarlettZebra",
        "labels": []
    },
    {
        "number": 3519,
        "title": "[Lora] Cannot convert baichuan2 qlora to GGUF",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3519",
        "state": "open",
        "created_at": "2023-10-07T03:08:38Z",
        "user": "lizhiling12345",
        "labels": []
    },
    {
        "number": 3511,
        "title": "[User] Crash: Undefined symbol \"csinl\" in convert.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3511",
        "state": "open",
        "created_at": "2023-10-06T20:07:04Z",
        "user": "lyjia",
        "labels": []
    },
    {
        "number": 3502,
        "title": "llama.cpp BPE tokenization of wiki.test does not match the HF tokenization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3502",
        "state": "open",
        "created_at": "2023-10-06T13:42:05Z",
        "user": "ggerganov",
        "labels": []
    },
    {
        "number": 3501,
        "title": "Cuda 12 support assumptions for Maxwell architecture is Wrong ! ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3501",
        "state": "open",
        "created_at": "2023-10-06T13:17:39Z",
        "user": "canaxx",
        "labels": []
    },
    {
        "number": 3483,
        "title": "[Feature Request] Dynamic temperature sampling for better coherence / creativity",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3483",
        "state": "open",
        "created_at": "2023-10-05T02:23:01Z",
        "user": "kalomaze",
        "labels": []
    },
    {
        "number": 3469,
        "title": "ci : add Apple silicon (M1) macOS runners",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3469",
        "state": "open",
        "created_at": "2023-10-04T11:21:05Z",
        "user": "ggerganov",
        "labels": [
            "good first issue",
            "testing"
        ]
    },
    {
        "number": 3467,
        "title": "I cannot convert Llama2-13b-hf model to ggml FP16 format",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3467",
        "state": "open",
        "created_at": "2023-10-04T08:52:49Z",
        "user": "Lucien20000118",
        "labels": []
    },
    {
        "number": 3442,
        "title": "convert.py fails on LongLORA tokenizer model, with mismatched vocab lengths",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3442",
        "state": "open",
        "created_at": "2023-10-02T18:06:44Z",
        "user": "willcallender",
        "labels": []
    },
    {
        "number": 3441,
        "title": "Support Jais-13b-chat bilingual model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3441",
        "state": "open",
        "created_at": "2023-10-02T17:41:59Z",
        "user": "infojunkie",
        "labels": [
            "model"
        ]
    },
    {
        "number": 3440,
        "title": "[User] Implement Streaming LLM - Make the inference more efficient",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3440",
        "state": "open",
        "created_at": "2023-10-02T17:38:36Z",
        "user": "errorsandwarnings",
        "labels": []
    },
    {
        "number": 3438,
        "title": "iOS swift package seems to use internal symbols: cblas_sgemm from NEWLAPACK, causing apps that use this to be unable to be published to the App Store",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3438",
        "state": "open",
        "created_at": "2023-10-02T12:59:02Z",
        "user": "l3utterfly",
        "labels": []
    },
    {
        "number": 3433,
        "title": "simple convert scripts run out of tmpfs space on Linux",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3433",
        "state": "open",
        "created_at": "2023-10-02T01:28:24Z",
        "user": "cebtenzzre",
        "labels": []
    },
    {
        "number": 3431,
        "title": "Add basic support for function calls in oai python server",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3431",
        "state": "open",
        "created_at": "2023-10-01T21:49:26Z",
        "user": "xaedes",
        "labels": [
            "\ud83e\udd99."
        ]
    },
    {
        "number": 3422,
        "title": "[User] AMD GPU slower than CPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3422",
        "state": "open",
        "created_at": "2023-10-01T05:57:21Z",
        "user": "oliverhu",
        "labels": [
            "performance",
            "AMD GPU"
        ]
    },
    {
        "number": 3414,
        "title": "[User] Metal regression \u2013 CodeLlama 34B 4_K_M won't run via Metal on 32\u00a0GB Apple M1 Max since `ec89379`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3414",
        "state": "open",
        "created_at": "2023-09-30T11:46:09Z",
        "user": "laurids-reichardt",
        "labels": []
    },
    {
        "number": 3398,
        "title": "Win11-WSL2/Ubuntu22.04/Cuda12/RTX4070 not compilng due ggml-cuda.cu(6125): function \"log2\" ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3398",
        "state": "open",
        "created_at": "2023-09-29T11:58:39Z",
        "user": "valentinsavenko",
        "labels": []
    },
    {
        "number": 3393,
        "title": "Wish to convert flan-t5 model to GGUF format",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3393",
        "state": "open",
        "created_at": "2023-09-29T09:26:27Z",
        "user": "niranjanakella",
        "labels": []
    },
    {
        "number": 3384,
        "title": "[User] Regression with CodeLlama 7B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3384",
        "state": "open",
        "created_at": "2023-09-28T20:01:22Z",
        "user": "AutonomicPerfectionist",
        "labels": [
            "need feedback"
        ]
    },
    {
        "number": 3377,
        "title": "llama : support sliding window attention",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3377",
        "state": "open",
        "created_at": "2023-09-28T12:12:40Z",
        "user": "ggerganov",
        "labels": [
            "performance"
        ]
    },
    {
        "number": 3373,
        "title": "Add JapaneseStableLM Support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3373",
        "state": "open",
        "created_at": "2023-09-28T01:28:24Z",
        "user": "azulika",
        "labels": []
    },
    {
        "number": 3365,
        "title": "llama : revisit using flash attention for prompt processing (a.k.a. prefil) + GPU implementation",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3365",
        "state": "open",
        "created_at": "2023-09-27T15:21:17Z",
        "user": "ggerganov",
        "labels": [
            "performance",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 3361,
        "title": "quantize seems to exit with errorlevel 0 on errors",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3361",
        "state": "open",
        "created_at": "2023-09-27T11:54:01Z",
        "user": "maddes8cht",
        "labels": [
            "good first issue"
        ]
    },
    {
        "number": 3356,
        "title": "Bad Output after runnig the train-text-from-scratch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3356",
        "state": "open",
        "created_at": "2023-09-27T08:05:38Z",
        "user": "Tantawi1",
        "labels": []
    },
    {
        "number": 3355,
        "title": "Convert.py throws ValueError - unknown format",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3355",
        "state": "open",
        "created_at": "2023-09-27T07:40:36Z",
        "user": "enahsor",
        "labels": []
    },
    {
        "number": 3352,
        "title": "How can I create Embeddings?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3352",
        "state": "open",
        "created_at": "2023-09-27T06:23:29Z",
        "user": "manel00",
        "labels": []
    },
    {
        "number": 3339,
        "title": "[User] main program built by cmake crashed due to Illegal instruction",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3339",
        "state": "open",
        "created_at": "2023-09-26T12:42:27Z",
        "user": "chenqiny",
        "labels": []
    },
    {
        "number": 3338,
        "title": "Confusing tokenizer API",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3338",
        "state": "open",
        "created_at": "2023-09-26T09:04:21Z",
        "user": "ortegaalfredo",
        "labels": []
    },
    {
        "number": 3337,
        "title": "Merge with qwen.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3337",
        "state": "open",
        "created_at": "2023-09-26T02:50:19Z",
        "user": "arch-btw",
        "labels": []
    },
    {
        "number": 3335,
        "title": "How to run the single example in llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3335",
        "state": "open",
        "created_at": "2023-09-26T02:09:06Z",
        "user": "Tr-buaa",
        "labels": []
    },
    {
        "number": 3334,
        "title": "[MPI] Add support for per-node options, thread counts, and layer allocations",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3334",
        "state": "open",
        "created_at": "2023-09-26T00:24:10Z",
        "user": "AutonomicPerfectionist",
        "labels": []
    },
    {
        "number": 3326,
        "title": "convert.py incorrectly detects LLaMAv1 65B as a LLaMAv2 model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3326",
        "state": "open",
        "created_at": "2023-09-24T14:02:55Z",
        "user": "city96",
        "labels": [
            "good first issue"
        ]
    },
    {
        "number": 3325,
        "title": "Debug intermediate layers of tensor compute values ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3325",
        "state": "open",
        "created_at": "2023-09-24T11:35:55Z",
        "user": "nlpcat",
        "labels": []
    },
    {
        "number": 3310,
        "title": "[Model Support Request] btlm-3b-8k-base",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3310",
        "state": "open",
        "created_at": "2023-09-22T18:46:28Z",
        "user": "yhyu13",
        "labels": []
    },
    {
        "number": 3305,
        "title": "[User] GGML_ASSERT: /opt/projects/llama.cpp/ggml.c:4796: view_src == NULL || data_size + view_offs <= ggml_nbytes(view_src) Aborted",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3305",
        "state": "open",
        "created_at": "2023-09-22T03:13:45Z",
        "user": "yilong2001",
        "labels": [
            "need more info"
        ]
    },
    {
        "number": 3303,
        "title": "Grammars from other projects",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3303",
        "state": "open",
        "created_at": "2023-09-21T21:43:21Z",
        "user": "tom-adsfund",
        "labels": []
    },
    {
        "number": 3294,
        "title": "Request: Nougat OCR Integration",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3294",
        "state": "open",
        "created_at": "2023-09-21T06:29:29Z",
        "user": "OhadRubin",
        "labels": [
            "help wanted",
            "model"
        ]
    },
    {
        "number": 3293,
        "title": "GPT-NeoX has only minimal inference support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3293",
        "state": "open",
        "created_at": "2023-09-21T04:50:58Z",
        "user": "cebtenzzre",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 3291,
        "title": "I am experiencing difficulties importing the llama_cpp package.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3291",
        "state": "open",
        "created_at": "2023-09-21T03:49:52Z",
        "user": "Hapluckyy",
        "labels": []
    },
    {
        "number": 3287,
        "title": "Inconsistent tokenization with examples/server tokenize endpoint",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3287",
        "state": "open",
        "created_at": "2023-09-20T22:37:29Z",
        "user": "IsaacDynamo",
        "labels": []
    },
    {
        "number": 3279,
        "title": "Quantization of Q4_K and Q5_K fail with \"illegal hardware instruction\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3279",
        "state": "open",
        "created_at": "2023-09-20T15:23:04Z",
        "user": "thilomichael",
        "labels": []
    },
    {
        "number": 3278,
        "title": "Contrastive Decoding Improves Reasoning in Large Language Models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3278",
        "state": "open",
        "created_at": "2023-09-20T14:50:32Z",
        "user": "logikstate",
        "labels": []
    },
    {
        "number": 3274,
        "title": "[Discuss] Dynamic Graph Building / Execution API",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3274",
        "state": "open",
        "created_at": "2023-09-20T09:17:17Z",
        "user": "wsxiaoys",
        "labels": []
    },
    {
        "number": 3269,
        "title": "[Enhancement Proposal] Single Model Speculative Sampling (+ possible future dynamic bit-depth models)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3269",
        "state": "open",
        "created_at": "2023-09-19T19:52:39Z",
        "user": "tysam-code",
        "labels": []
    },
    {
        "number": 3266,
        "title": "Fix static build",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3266",
        "state": "open",
        "created_at": "2023-09-19T16:57:37Z",
        "user": "WilliamTambellini",
        "labels": []
    },
    {
        "number": 3253,
        "title": "[Q] Use llama.cpp with Kaggle TPUs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3253",
        "state": "open",
        "created_at": "2023-09-18T17:43:11Z",
        "user": "NightMachinery",
        "labels": []
    },
    {
        "number": 3244,
        "title": "rocm: Automatically build with separate compiler",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3244",
        "state": "open",
        "created_at": "2023-09-18T03:24:39Z",
        "user": "danielzgtg",
        "labels": []
    },
    {
        "number": 3235,
        "title": "A100 SMX initialization error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3235",
        "state": "open",
        "created_at": "2023-09-17T20:50:29Z",
        "user": "bryfry",
        "labels": []
    },
    {
        "number": 3234,
        "title": "llama : store non-RoPEd K cache",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3234",
        "state": "open",
        "created_at": "2023-09-17T20:50:12Z",
        "user": "ggerganov",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 3229,
        "title": "metal : simplify kernel arguments using a struct",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3229",
        "state": "open",
        "created_at": "2023-09-17T17:10:35Z",
        "user": "ggerganov",
        "labels": [
            "good first issue",
            "refactoring"
        ]
    },
    {
        "number": 3225,
        "title": "Discrepencies between max position embeddings and max sequence length",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3225",
        "state": "open",
        "created_at": "2023-09-17T04:31:28Z",
        "user": "Nexesenex",
        "labels": []
    },
    {
        "number": 3219,
        "title": "log.h improvements",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3219",
        "state": "open",
        "created_at": "2023-09-16T17:41:25Z",
        "user": "staviq",
        "labels": []
    },
    {
        "number": 3208,
        "title": "What would it take to support DeciLM?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3208",
        "state": "open",
        "created_at": "2023-09-16T04:02:56Z",
        "user": "l3utterfly",
        "labels": [
            "model"
        ]
    },
    {
        "number": 3204,
        "title": "Blenderbot Support?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3204",
        "state": "open",
        "created_at": "2023-09-16T01:10:06Z",
        "user": "fakerybakery",
        "labels": [
            "enhancement",
            "model"
        ]
    },
    {
        "number": 3183,
        "title": "Error when loading Llama model from file",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3183",
        "state": "open",
        "created_at": "2023-09-14T22:47:25Z",
        "user": "beyhangl",
        "labels": []
    },
    {
        "number": 3181,
        "title": "prevent markdownish from modifying text inside code block",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3181",
        "state": "open",
        "created_at": "2023-09-14T20:59:46Z",
        "user": "nathan-sixnines",
        "labels": []
    },
    {
        "number": 3172,
        "title": "ERROR: Failed building wheel for llama-cpp-python usig cmake",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3172",
        "state": "open",
        "created_at": "2023-09-14T17:17:52Z",
        "user": "icecoldt369",
        "labels": []
    },
    {
        "number": 3171,
        "title": "Getting an error during the llama-cpp-python build, looks like llama.cpp build issue.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3171",
        "state": "open",
        "created_at": "2023-09-14T16:58:42Z",
        "user": "primemp",
        "labels": []
    },
    {
        "number": 3169,
        "title": "In windows10 ,When cmake and cuBLas jointly compile the llama.cpp project, the command line prompt shown below appears, and the generated executables such as quantize.exe and main.exe run without any response.\u4ee5\u4e0b\u662f\u548ccuBLAS\u4e00\u8d77\u7f16\u8bd1\u7684\u547d\u4ee4\uff0c\u9002\u7528\u4e8eNVIDIA\u76f8\u5173GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3169",
        "state": "open",
        "created_at": "2023-09-14T14:51:15Z",
        "user": "kaixuanjames",
        "labels": []
    },
    {
        "number": 3154,
        "title": "Questions about llm inference thread",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3154",
        "state": "open",
        "created_at": "2023-09-13T02:00:43Z",
        "user": "AndreaChiChengdu",
        "labels": []
    },
    {
        "number": 3153,
        "title": "Outputs garbage on WSL when compiled with cuBLAS",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3153",
        "state": "open",
        "created_at": "2023-09-12T22:18:46Z",
        "user": "MystCaster",
        "labels": []
    },
    {
        "number": 3149,
        "title": "Is anyone working on 'conditional' logit biasing? (How to potentially improve repetition penalty?)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3149",
        "state": "open",
        "created_at": "2023-09-12T21:12:14Z",
        "user": "kalomaze",
        "labels": []
    },
    {
        "number": 3148,
        "title": "Certain 70B Q4_0 quants outputting gibberish (other quant formats unaffected)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3148",
        "state": "open",
        "created_at": "2023-09-12T19:51:19Z",
        "user": "TheBloke",
        "labels": []
    },
    {
        "number": 3141,
        "title": "Error with chat-persistent on Windows.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3141",
        "state": "open",
        "created_at": "2023-09-12T12:26:37Z",
        "user": "simeoncapy",
        "labels": []
    },
    {
        "number": 3138,
        "title": "Adding Codcov badge",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3138",
        "state": "open",
        "created_at": "2023-09-12T08:05:23Z",
        "user": "alonfaraj",
        "labels": []
    },
    {
        "number": 3135,
        "title": "Freeze after offloading layers to GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3135",
        "state": "open",
        "created_at": "2023-09-12T03:22:42Z",
        "user": "CRD716",
        "labels": []
    },
    {
        "number": 3133,
        "title": "[Feature Request] Support InternLM Deploy",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3133",
        "state": "open",
        "created_at": "2023-09-12T02:08:17Z",
        "user": "vansinhu",
        "labels": []
    },
    {
        "number": 3129,
        "title": "Intel CPU and Graphics card Macbook pro: failed to create context with model './models/model.q4_k_s.gguf'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3129",
        "state": "open",
        "created_at": "2023-09-11T21:04:11Z",
        "user": "Bateoriginal",
        "labels": []
    },
    {
        "number": 3126,
        "title": "AttributeError: /home/developer/mambaforge/envs/CodeLlama/lib/libllama.so: undefined symbol: llama_n_ctx_train",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3126",
        "state": "open",
        "created_at": "2023-09-11T18:48:44Z",
        "user": "phalexo",
        "labels": []
    },
    {
        "number": 3125,
        "title": "Win 11 ROCm dual GPU models loads then server just shuts down",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3125",
        "state": "open",
        "created_at": "2023-09-11T15:38:55Z",
        "user": "ccbadd",
        "labels": []
    },
    {
        "number": 3124,
        "title": "Is it possible to implement Operator Fusion in ggml?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3124",
        "state": "open",
        "created_at": "2023-09-11T14:46:10Z",
        "user": "bobqianic",
        "labels": []
    },
    {
        "number": 3120,
        "title": "Faster multi-gpu strategy?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3120",
        "state": "open",
        "created_at": "2023-09-11T08:24:11Z",
        "user": "calvintwr",
        "labels": []
    },
    {
        "number": 3119,
        "title": "llama_model_loader prints entire tensor",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3119",
        "state": "open",
        "created_at": "2023-09-11T04:20:35Z",
        "user": "phdykd",
        "labels": []
    },
    {
        "number": 3109,
        "title": "How can I use huggingface model?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3109",
        "state": "open",
        "created_at": "2023-09-10T11:17:12Z",
        "user": "turk",
        "labels": []
    },
    {
        "number": 3102,
        "title": "Don't highlight console session as Java.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3102",
        "state": "open",
        "created_at": "2023-09-09T20:21:14Z",
        "user": "remexre",
        "labels": []
    },
    {
        "number": 3099,
        "title": "llama.cpp compiled and run with mpi outputs garbage. ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3099",
        "state": "open",
        "created_at": "2023-09-09T17:45:24Z",
        "user": "cheburakshu",
        "labels": []
    },
    {
        "number": 3094,
        "title": "Text file summarization ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3094",
        "state": "open",
        "created_at": "2023-09-09T09:52:05Z",
        "user": "slavag",
        "labels": []
    },
    {
        "number": 3093,
        "title": "Adding SqueezeLLM Support",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3093",
        "state": "open",
        "created_at": "2023-09-09T04:37:56Z",
        "user": "chooper1",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 3090,
        "title": "[Oversight]  -> Ideal Rope for CodeLLama 2 based models differs vastly from LLama 2.  ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3090",
        "state": "open",
        "created_at": "2023-09-09T01:27:01Z",
        "user": "SabinStargem",
        "labels": []
    },
    {
        "number": 3080,
        "title": "Add a simpler main example",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3080",
        "state": "open",
        "created_at": "2023-09-08T07:22:43Z",
        "user": "KerfuffleV2",
        "labels": []
    },
    {
        "number": 3077,
        "title": "\u91cf\u5316\u4e0e\u90e8\u7f72\u7684\u95ee\u9898",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3077",
        "state": "open",
        "created_at": "2023-09-08T03:52:21Z",
        "user": "kaixuanjames",
        "labels": []
    },
    {
        "number": 3067,
        "title": "(Falcon 180B) ARM - gibberish output with k-quants",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3067",
        "state": "open",
        "created_at": "2023-09-07T19:11:52Z",
        "user": "BarfingLemurs",
        "labels": []
    },
    {
        "number": 3051,
        "title": "Multi-GPU support for AMD?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3051",
        "state": "open",
        "created_at": "2023-09-07T00:15:22Z",
        "user": "ElliottDyson",
        "labels": []
    },
    {
        "number": 3046,
        "title": "Zig build configuration on windows requires adjustments",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3046",
        "state": "open",
        "created_at": "2023-09-06T10:14:10Z",
        "user": "flameoftheforest",
        "labels": []
    },
    {
        "number": 3039,
        "title": "small optimization for q4_0 to q5_1 quants in ggml.c",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3039",
        "state": "open",
        "created_at": "2023-09-06T00:50:18Z",
        "user": "Swight1423",
        "labels": []
    },
    {
        "number": 3032,
        "title": "Problem with  compilation with gpu support ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3032",
        "state": "open",
        "created_at": "2023-09-05T18:51:51Z",
        "user": "vulkomilev",
        "labels": []
    },
    {
        "number": 3030,
        "title": "llama.cpp output vs huggingface output",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3030",
        "state": "open",
        "created_at": "2023-09-05T18:17:34Z",
        "user": "rjtshrm",
        "labels": []
    },
    {
        "number": 3025,
        "title": "Model magic",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3025",
        "state": "open",
        "created_at": "2023-09-05T11:14:01Z",
        "user": "jboero",
        "labels": []
    },
    {
        "number": 3022,
        "title": "How GGML is different from ONNX",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3022",
        "state": "open",
        "created_at": "2023-09-05T06:33:02Z",
        "user": "biswaroop1547",
        "labels": []
    },
    {
        "number": 3012,
        "title": "metal : bug with ggml_cont",
        "url": "https://github.com/ggerganov/llama.cpp/pull/3012",
        "state": "open",
        "created_at": "2023-09-04T16:48:26Z",
        "user": "ggerganov",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 3001,
        "title": " Lack of Japanese Language Support in GPT-4-ALL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3001",
        "state": "open",
        "created_at": "2023-09-04T04:18:12Z",
        "user": "raj20023",
        "labels": []
    },
    {
        "number": 3000,
        "title": "[macos] AMD GPU using mul_mm in metal",
        "url": "https://github.com/ggerganov/llama.cpp/issues/3000",
        "state": "open",
        "created_at": "2023-09-04T02:36:03Z",
        "user": "sukualam",
        "labels": []
    },
    {
        "number": 2979,
        "title": "Convert huggingface format, e.g., multiple .bin files, back to fb torch format, e.g., multiple .pth files",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2979",
        "state": "open",
        "created_at": "2023-09-03T04:30:31Z",
        "user": "machilusZ",
        "labels": []
    },
    {
        "number": 2975,
        "title": "[C0ffymachyne] was -gqa argument removed from the \"main\"  ? What is a replacement and how to load 70B model without it ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2975",
        "state": "open",
        "created_at": "2023-09-02T21:07:07Z",
        "user": "c0ffymachyne",
        "labels": []
    },
    {
        "number": 2970,
        "title": "[Bug] Suggested Fixes for mathematical inaccuracy in llama_sample_repetition_penalty function",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2970",
        "state": "open",
        "created_at": "2023-09-02T17:08:10Z",
        "user": "tysam-code",
        "labels": []
    },
    {
        "number": 2968,
        "title": "ROCm: garbade output with low ngl",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2968",
        "state": "open",
        "created_at": "2023-09-02T13:24:20Z",
        "user": "Jipok",
        "labels": []
    },
    {
        "number": 2965,
        "title": "How to run in AMD GPU with macos (with mps)?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2965",
        "state": "open",
        "created_at": "2023-09-02T08:55:01Z",
        "user": "sukualam",
        "labels": []
    },
    {
        "number": 2962,
        "title": "Adding a ToT feature?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2962",
        "state": "open",
        "created_at": "2023-09-01T17:20:39Z",
        "user": "itsPreto",
        "labels": []
    },
    {
        "number": 2958,
        "title": "Unable to activate the Cublas while running a python file which implements the llamachain using the llamacpp ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2958",
        "state": "open",
        "created_at": "2023-09-01T15:12:24Z",
        "user": "Llama-activator",
        "labels": []
    },
    {
        "number": 2947,
        "title": "[User] supporting code llama",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2947",
        "state": "open",
        "created_at": "2023-09-01T01:34:44Z",
        "user": "leejw51",
        "labels": []
    },
    {
        "number": 2941,
        "title": "OSError: exception: access violation writing 0x0000000000000380 unable to query two querys at once  the LLMchain(llm=LLamaCPP())) ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2941",
        "state": "open",
        "created_at": "2023-08-31T19:20:57Z",
        "user": "Revanth-guduru-balaji",
        "labels": []
    },
    {
        "number": 2932,
        "title": "alex5956 ican't succeed in converting my llama model ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2932",
        "state": "open",
        "created_at": "2023-08-31T13:55:00Z",
        "user": "aragon5956",
        "labels": []
    },
    {
        "number": 2925,
        "title": "Infinite context",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2925",
        "state": "open",
        "created_at": "2023-08-31T08:00:08Z",
        "user": "logikstate",
        "labels": []
    },
    {
        "number": 2923,
        "title": "llama : combined beam search + grammar sampling strategy",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2923",
        "state": "open",
        "created_at": "2023-08-31T06:29:29Z",
        "user": "ggerganov",
        "labels": [
            "good first issue",
            "generation quality",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 2919,
        "title": "Quantizations for Llama-2-7B-32K-Instruct (32K context)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2919",
        "state": "open",
        "created_at": "2023-08-31T01:48:05Z",
        "user": "rozek",
        "labels": []
    },
    {
        "number": 2907,
        "title": "Discussion: Requirements for NVLink of two 3090Ti for pooled 48gb VRAM",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2907",
        "state": "open",
        "created_at": "2023-08-30T14:53:52Z",
        "user": "ghost",
        "labels": []
    },
    {
        "number": 2894,
        "title": "[User] \u201c'token_embd.weight' has wrong shape\u201d when loading WizardLM-Uncensored-Falcon-40b",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2894",
        "state": "open",
        "created_at": "2023-08-30T05:48:10Z",
        "user": "rlanday",
        "labels": []
    },
    {
        "number": 2891,
        "title": "metal: template for mat-vec multiplication kernels",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2891",
        "state": "open",
        "created_at": "2023-08-30T03:04:47Z",
        "user": "lshzh-ww",
        "labels": [
            "performance"
        ]
    },
    {
        "number": 2890,
        "title": "SEGV happen  when I start run main.exe",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2890",
        "state": "open",
        "created_at": "2023-08-30T02:45:16Z",
        "user": "willwilling",
        "labels": []
    },
    {
        "number": 2888,
        "title": "using the negate character (^) in a grammar with a sequence",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2888",
        "state": "open",
        "created_at": "2023-08-30T01:45:18Z",
        "user": "pacmanincarnate",
        "labels": []
    },
    {
        "number": 2883,
        "title": "release : use clang instead of msvc",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2883",
        "state": "open",
        "created_at": "2023-08-29T16:42:17Z",
        "user": "ivanstepanovftw",
        "labels": []
    },
    {
        "number": 2880,
        "title": "[User] does opencl part use usm or not?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2880",
        "state": "open",
        "created_at": "2023-08-29T16:17:31Z",
        "user": "congzhangzh",
        "labels": []
    },
    {
        "number": 2877,
        "title": "Please add support for kfkas llama-2-ko-7b-chat",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2877",
        "state": "open",
        "created_at": "2023-08-29T15:20:21Z",
        "user": "kurugai",
        "labels": [
            "model"
        ]
    },
    {
        "number": 2868,
        "title": "Please support the also official Falcon-rw-1b and Falcon-rw-7b model variants",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2868",
        "state": "open",
        "created_at": "2023-08-29T07:51:18Z",
        "user": "maddes8cht",
        "labels": [
            "good first issue",
            "model"
        ]
    },
    {
        "number": 2866,
        "title": "[linux] [amd64] [CUDA] -ngl hangs in interactive mode with 70B llama2 if mmap is enabled",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2866",
        "state": "open",
        "created_at": "2023-08-29T07:38:31Z",
        "user": "vfhbg",
        "labels": []
    },
    {
        "number": 2860,
        "title": "[Falcon] `convert-falcon-hf-to-gguf.py` has some missing dependencies ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2860",
        "state": "open",
        "created_at": "2023-08-28T22:14:02Z",
        "user": "akawrykow",
        "labels": []
    },
    {
        "number": 2844,
        "title": "Performance issues on high performance computer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2844",
        "state": "open",
        "created_at": "2023-08-28T00:36:13Z",
        "user": "GoComputing",
        "labels": []
    },
    {
        "number": 2843,
        "title": "Windows ROCm Build.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2843",
        "state": "open",
        "created_at": "2023-08-27T19:15:53Z",
        "user": "LFL38",
        "labels": []
    },
    {
        "number": 2841,
        "title": "has anybody already converted Together.AI's LLaMA-2 variant with 32k context?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2841",
        "state": "open",
        "created_at": "2023-08-27T18:19:00Z",
        "user": "rozek",
        "labels": []
    },
    {
        "number": 2838,
        "title": "CUDA non-determinism on identical requests",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2838",
        "state": "open",
        "created_at": "2023-08-27T17:38:30Z",
        "user": "phiharri",
        "labels": [
            "bug",
            "good first issue"
        ]
    },
    {
        "number": 2818,
        "title": "Enhancement: Codellama FIM tokenization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2818",
        "state": "open",
        "created_at": "2023-08-26T17:29:09Z",
        "user": "apaz-cli",
        "labels": []
    },
    {
        "number": 2815,
        "title": "CUDA does not build when QK_K = 64",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2815",
        "state": "open",
        "created_at": "2023-08-26T15:22:33Z",
        "user": "ikawrakow",
        "labels": []
    },
    {
        "number": 2809,
        "title": "cuda: 1.2x faster dequantization kernel",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2809",
        "state": "open",
        "created_at": "2023-08-26T12:56:21Z",
        "user": "li-plus",
        "labels": []
    },
    {
        "number": 2803,
        "title": "Support IDEFICS models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2803",
        "state": "open",
        "created_at": "2023-08-26T06:43:35Z",
        "user": "generalsvr",
        "labels": []
    },
    {
        "number": 2794,
        "title": "small esthetic Idea for main",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2794",
        "state": "open",
        "created_at": "2023-08-25T17:13:09Z",
        "user": "ghost",
        "labels": []
    },
    {
        "number": 2789,
        "title": "Generate multiple outputs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2789",
        "state": "open",
        "created_at": "2023-08-25T15:34:12Z",
        "user": "SimonBenhamou",
        "labels": []
    },
    {
        "number": 2785,
        "title": "[User] Mac with Intel CPU + AMD GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2785",
        "state": "open",
        "created_at": "2023-08-25T14:47:54Z",
        "user": "chigkim",
        "labels": []
    },
    {
        "number": 2783,
        "title": "llama : tool for evaluating quantization results per layer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2783",
        "state": "open",
        "created_at": "2023-08-25T10:02:47Z",
        "user": "ggerganov",
        "labels": [
            "enhancement",
            "generation quality"
        ]
    },
    {
        "number": 2781,
        "title": "Merge the enhanced index.html for the server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2781",
        "state": "open",
        "created_at": "2023-08-25T09:32:35Z",
        "user": "SlyEcho",
        "labels": [
            "server/webui"
        ]
    },
    {
        "number": 2778,
        "title": "[User] latest ggml-alloc not support as Xcode app package",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2778",
        "state": "open",
        "created_at": "2023-08-25T06:22:02Z",
        "user": "h4rk8s",
        "labels": []
    },
    {
        "number": 2745,
        "title": "llama_log_internal_v does not handle -1 returned from vsnprintf ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2745",
        "state": "open",
        "created_at": "2023-08-23T16:11:59Z",
        "user": "stelf",
        "labels": []
    },
    {
        "number": 2742,
        "title": "[movediedi] After building with cmake and MSVC toolchain with optimizations enabled for aarch64, main.exe produces gibberish",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2742",
        "state": "open",
        "created_at": "2023-08-23T13:19:11Z",
        "user": "movediedi",
        "labels": []
    },
    {
        "number": 2730,
        "title": "Fix docs",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2730",
        "state": "open",
        "created_at": "2023-08-23T01:09:04Z",
        "user": "akawrykow",
        "labels": []
    },
    {
        "number": 2728,
        "title": "Add date and commit hash to gguf metadata",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2728",
        "state": "open",
        "created_at": "2023-08-23T00:33:01Z",
        "user": "akawrykow",
        "labels": []
    },
    {
        "number": 2706,
        "title": "Gpt neox model that was converted and quantized to gguf refused to run",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2706",
        "state": "open",
        "created_at": "2023-08-22T09:56:30Z",
        "user": "JohnClaw",
        "labels": []
    },
    {
        "number": 2700,
        "title": "Skip computation of much of last layer & unused logits during prompt eval / large N",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2700",
        "state": "open",
        "created_at": "2023-08-22T00:27:11Z",
        "user": "ochafik",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 2696,
        "title": "The Best combinated parameters now is:",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2696",
        "state": "open",
        "created_at": "2023-08-21T22:14:57Z",
        "user": "SilvaRaulEnriqueCJM",
        "labels": []
    },
    {
        "number": 2693,
        "title": "Analyze file",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2693",
        "state": "open",
        "created_at": "2023-08-21T18:02:35Z",
        "user": "kirkog86",
        "labels": []
    },
    {
        "number": 2687,
        "title": "will this project support on device npu like qualcomm hexagon\uff1f",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2687",
        "state": "open",
        "created_at": "2023-08-21T06:29:48Z",
        "user": "AndreaChiChengdu",
        "labels": []
    },
    {
        "number": 2676,
        "title": "Compilation with CLBlast in onboard graphics",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2676",
        "state": "open",
        "created_at": "2023-08-19T21:39:02Z",
        "user": "SilvaRaulEnriqueCJM",
        "labels": []
    },
    {
        "number": 2673,
        "title": "Android instructions failure ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2673",
        "state": "open",
        "created_at": "2023-08-19T18:17:36Z",
        "user": "Jkintree2",
        "labels": []
    },
    {
        "number": 2672,
        "title": "[Feature] Add MoE Loras in ggml",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2672",
        "state": "open",
        "created_at": "2023-08-19T16:54:03Z",
        "user": "BarfingLemurs",
        "labels": []
    },
    {
        "number": 2662,
        "title": "wierd ram behavior ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2662",
        "state": "open",
        "created_at": "2023-08-18T22:08:36Z",
        "user": "clemens98",
        "labels": []
    },
    {
        "number": 2653,
        "title": "[Feature] I am wondering whether there will be a huge crowd expecting to see some rest APIs for these projects as well.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2653",
        "state": "open",
        "created_at": "2023-08-18T06:56:52Z",
        "user": "vithushanms",
        "labels": []
    },
    {
        "number": 2642,
        "title": "[ISSUE] Loading of  4 bit quantized llama2-70B model on the gpu machine(Tesla T4), it is not happening.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2642",
        "state": "open",
        "created_at": "2023-08-17T08:35:38Z",
        "user": "apoorv-98",
        "labels": []
    },
    {
        "number": 2641,
        "title": "A100 GPU generate 11.30 tokens/s with llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2641",
        "state": "open",
        "created_at": "2023-08-17T06:48:48Z",
        "user": "bigmover",
        "labels": []
    },
    {
        "number": 2638,
        "title": "[User] GPU and CPU resources underutilized, where's the bottleneck?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2638",
        "state": "open",
        "created_at": "2023-08-17T01:08:53Z",
        "user": "gschadow",
        "labels": []
    },
    {
        "number": 2636,
        "title": "[Bug] `FORTIFY: read: count 18446744071941467070 > SSIZE_MAX` on Pixel 7 (Android 13)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2636",
        "state": "open",
        "created_at": "2023-08-16T19:08:30Z",
        "user": "BioGeek",
        "labels": []
    },
    {
        "number": 2631,
        "title": "llama : add test for saving/loading sessions to the CI",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2631",
        "state": "open",
        "created_at": "2023-08-16T14:42:01Z",
        "user": "ggerganov",
        "labels": [
            "good first issue",
            "testing"
        ]
    },
    {
        "number": 2625,
        "title": "Enhancement request: prevent user entry from disappearing from Web client entry text field when submitted",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2625",
        "state": "open",
        "created_at": "2023-08-15T19:44:16Z",
        "user": "lutusp",
        "labels": []
    },
    {
        "number": 2624,
        "title": "Math library related errors \"undefined reference to ...\" when running `cmake --build`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2624",
        "state": "open",
        "created_at": "2023-08-15T15:16:49Z",
        "user": "nichoio",
        "labels": []
    },
    {
        "number": 2623,
        "title": "one can now specify where ggml-metal.metal file is with en variable GGML_METAL_PATH",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2623",
        "state": "open",
        "created_at": "2023-08-15T14:51:10Z",
        "user": "xvolks",
        "labels": []
    },
    {
        "number": 2621,
        "title": "Bug: Gibberish output when using ngl more than 0. [Linux VM AG100]",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2621",
        "state": "open",
        "created_at": "2023-08-15T07:19:30Z",
        "user": "fizahkhalid",
        "labels": []
    },
    {
        "number": 2617,
        "title": "[User] Feature Request: Prompt Caching Server Support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2617",
        "state": "open",
        "created_at": "2023-08-14T21:30:26Z",
        "user": "nova706",
        "labels": []
    },
    {
        "number": 2609,
        "title": "[User] Logprobs different with unquantized models when compared with pytorch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2609",
        "state": "open",
        "created_at": "2023-08-14T11:26:07Z",
        "user": "ssemeniuta",
        "labels": []
    },
    {
        "number": 2606,
        "title": "'Llama' object has no attribute 'model'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2606",
        "state": "open",
        "created_at": "2023-08-14T09:40:00Z",
        "user": "Giulianini",
        "labels": []
    },
    {
        "number": 2599,
        "title": "reason only last? increase 2048 tokens?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2599",
        "state": "open",
        "created_at": "2023-08-13T18:26:12Z",
        "user": "SilvaRaulEnriqueCJM",
        "labels": []
    },
    {
        "number": 2598,
        "title": "[User] `--reverse-prompt` no longer echoes in the console",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2598",
        "state": "open",
        "created_at": "2023-08-13T14:28:30Z",
        "user": "44670",
        "labels": []
    },
    {
        "number": 2593,
        "title": "Implementation of a sequence repetition penalty sampler",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2593",
        "state": "open",
        "created_at": "2023-08-12T20:39:25Z",
        "user": "KerfuffleV2",
        "labels": [
            "enhancement",
            "generation quality",
            "need feedback"
        ]
    },
    {
        "number": 2582,
        "title": "Inconsistent Embedding Results on Different OS Platforms",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2582",
        "state": "open",
        "created_at": "2023-08-11T01:17:45Z",
        "user": "saddam213",
        "labels": []
    },
    {
        "number": 2580,
        "title": "Don't crash when prompt cannot be tokenized",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2580",
        "state": "open",
        "created_at": "2023-08-10T18:50:10Z",
        "user": "jrudolph",
        "labels": []
    },
    {
        "number": 2577,
        "title": "DYLD_ROOT_PATH not set for simulator program",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2577",
        "state": "open",
        "created_at": "2023-08-10T09:29:06Z",
        "user": "Leli1024",
        "labels": []
    },
    {
        "number": 2576,
        "title": "Unable to inference on Quantized 70B Model using llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2576",
        "state": "open",
        "created_at": "2023-08-10T08:57:27Z",
        "user": "vatsarishabh22",
        "labels": []
    },
    {
        "number": 2572,
        "title": "POST to server takes forever",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2572",
        "state": "open",
        "created_at": "2023-08-10T00:28:51Z",
        "user": "YerongLi",
        "labels": []
    },
    {
        "number": 2555,
        "title": "Add support for AMX instructions (bf16 and/or int8)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2555",
        "state": "open",
        "created_at": "2023-08-08T16:20:40Z",
        "user": "WilliamTambellini",
        "labels": []
    },
    {
        "number": 2545,
        "title": "Having trouble converting llama2Chinese model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2545",
        "state": "open",
        "created_at": "2023-08-07T19:52:50Z",
        "user": "SolarTorch",
        "labels": []
    },
    {
        "number": 2526,
        "title": "Enhancement for java bindings.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2526",
        "state": "open",
        "created_at": "2023-08-05T14:33:31Z",
        "user": "codeastra2",
        "labels": []
    },
    {
        "number": 2522,
        "title": "Incoherent and Empty Output with 4-bit Quantized Vicuna v1.5",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2522",
        "state": "open",
        "created_at": "2023-08-05T05:28:41Z",
        "user": "chand1012",
        "labels": []
    },
    {
        "number": 2518,
        "title": "QuiP - 2 bit Quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2518",
        "state": "open",
        "created_at": "2023-08-04T19:11:34Z",
        "user": "Jipok",
        "labels": []
    },
    {
        "number": 2516,
        "title": "Convert.py issue",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2516",
        "state": "open",
        "created_at": "2023-08-04T18:03:29Z",
        "user": "okanduzyel",
        "labels": []
    },
    {
        "number": 2509,
        "title": "LoRA GGML is 2x the size of the original LoRA when including all modules",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2509",
        "state": "open",
        "created_at": "2023-08-04T01:09:48Z",
        "user": "kallewoof",
        "labels": []
    },
    {
        "number": 2507,
        "title": "Regression in interactive mode",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2507",
        "state": "open",
        "created_at": "2023-08-03T21:29:35Z",
        "user": "aragula12",
        "labels": []
    },
    {
        "number": 2496,
        "title": "3090 with CUDA (24GB GRAM): Best command line for more fast execution a 3090 with CUDA",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2496",
        "state": "open",
        "created_at": "2023-08-02T22:05:56Z",
        "user": "SilvaRaulEnriqueCJM",
        "labels": []
    },
    {
        "number": 2481,
        "title": "[Nem_pickaxe] ICall to Undeclared Functions and Implicit Function Declarations",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2481",
        "state": "open",
        "created_at": "2023-08-01T15:16:22Z",
        "user": "Nempickaxe",
        "labels": []
    },
    {
        "number": 2479,
        "title": "Prompt cache output mismatch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2479",
        "state": "open",
        "created_at": "2023-08-01T12:11:16Z",
        "user": "ivanstepanovftw",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 2474,
        "title": "Generation stops after a few token when discrete_distribution begin value hits 1",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2474",
        "state": "open",
        "created_at": "2023-07-31T20:50:01Z",
        "user": "anujcb",
        "labels": []
    },
    {
        "number": 2469,
        "title": "[User] Insert summary of your issue or enhancement..    slow speed on bette device .",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2469",
        "state": "open",
        "created_at": "2023-07-31T11:20:17Z",
        "user": "Fcucgvhhhvjv",
        "labels": []
    },
    {
        "number": 2454,
        "title": "Validate-Params: refactoring-gpt-params-parse",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2454",
        "state": "open",
        "created_at": "2023-07-30T14:10:01Z",
        "user": "maddes8cht",
        "labels": []
    },
    {
        "number": 2445,
        "title": "Assertion ggml_nelements(a) == ne0*ne1*ne2 when loading TheBloke/Llama-2-70B-GGML/llama-2-70b.ggmlv3.q2_K.bin",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2445",
        "state": "open",
        "created_at": "2023-07-29T14:33:35Z",
        "user": "xvolks",
        "labels": []
    },
    {
        "number": 2444,
        "title": "Speed too slow ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2444",
        "state": "open",
        "created_at": "2023-07-29T03:00:07Z",
        "user": "Fcucgvhhhvjv",
        "labels": []
    },
    {
        "number": 2434,
        "title": "ggml : PoC for normalizing weights for better quantization packing",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2434",
        "state": "open",
        "created_at": "2023-07-28T08:07:12Z",
        "user": "ggerganov",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 2432,
        "title": "MultiGPU memory allocation fail",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2432",
        "state": "open",
        "created_at": "2023-07-28T07:17:58Z",
        "user": "alex1milhalevich",
        "labels": []
    },
    {
        "number": 2430,
        "title": "Build fail in mac A2 with \"LLAMA_METAL=1 make\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2430",
        "state": "open",
        "created_at": "2023-07-28T05:12:32Z",
        "user": "ilisin",
        "labels": []
    },
    {
        "number": 2428,
        "title": "[Prompt Processing] Is there a way to speed up prompt processing for Metal? (M1/M2)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2428",
        "state": "open",
        "created_at": "2023-07-27T22:00:17Z",
        "user": "Jchang4",
        "labels": []
    },
    {
        "number": 2422,
        "title": "[Bug] llama_set_state_data() does not work correctly with offloaded GPU Layers (kv cache)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2422",
        "state": "open",
        "created_at": "2023-07-27T09:59:44Z",
        "user": "WeirdConstructor",
        "labels": []
    },
    {
        "number": 2408,
        "title": "Create example bash script for LlaMa 2 Chat",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2408",
        "state": "open",
        "created_at": "2023-07-26T19:35:20Z",
        "user": "lionelchg",
        "labels": []
    },
    {
        "number": 2407,
        "title": "Metal uses Intel HD instead of AMD GPU on Intel iMac5k ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2407",
        "state": "open",
        "created_at": "2023-07-26T18:45:20Z",
        "user": "furu00",
        "labels": []
    },
    {
        "number": 2402,
        "title": "Lack of documentation regarding RoPE scaling",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2402",
        "state": "open",
        "created_at": "2023-07-26T14:10:02Z",
        "user": "maddes8cht",
        "labels": []
    },
    {
        "number": 2396,
        "title": "I can't run llama v2 13B on GPU after  make LLAMA_CUBLAS=1",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2396",
        "state": "open",
        "created_at": "2023-07-25T23:23:17Z",
        "user": "gabrielcustodio",
        "labels": []
    },
    {
        "number": 2390,
        "title": "export PKG_CONFIG_PATH on Android to compile without NDK",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2390",
        "state": "open",
        "created_at": "2023-07-25T16:36:32Z",
        "user": "Morakhiyasaiyam",
        "labels": []
    },
    {
        "number": 2388,
        "title": "Cannot convert the gpt4all-lora-quantized.bin into ggml",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2388",
        "state": "open",
        "created_at": "2023-07-25T14:12:09Z",
        "user": "shreesha345",
        "labels": []
    },
    {
        "number": 2383,
        "title": "Use conversation template for api proxy, fix eventsource format",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2383",
        "state": "open",
        "created_at": "2023-07-25T10:26:53Z",
        "user": "zeyugao",
        "labels": []
    },
    {
        "number": 2381,
        "title": "Added Dockerfile for server",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2381",
        "state": "open",
        "created_at": "2023-07-25T00:36:56Z",
        "user": "johnjones4",
        "labels": []
    },
    {
        "number": 2378,
        "title": "CLBlast: Segmentation fault when trying to offload last layer of Llama-2-13b to GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2378",
        "state": "open",
        "created_at": "2023-07-24T18:49:14Z",
        "user": "Kolyan1414",
        "labels": []
    },
    {
        "number": 2361,
        "title": "quantize exiting prematurely without error msg",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2361",
        "state": "open",
        "created_at": "2023-07-24T07:24:50Z",
        "user": "anujcb",
        "labels": []
    },
    {
        "number": 2360,
        "title": "CLBlast: OpenCL error: clEnqueueNDRangeKernel: -54",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2360",
        "state": "open",
        "created_at": "2023-07-24T06:55:20Z",
        "user": "Freed-Wu",
        "labels": []
    },
    {
        "number": 2344,
        "title": "Building llama.cpp or building libllama.so on a virtualized Linux on Apple silicon does not work.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2344",
        "state": "open",
        "created_at": "2023-07-23T13:11:30Z",
        "user": "AndreasKunar",
        "labels": []
    },
    {
        "number": 2343,
        "title": "ggml_opencl: device FP16 support: false (dGPU), true (iGPU)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2343",
        "state": "open",
        "created_at": "2023-07-23T13:04:45Z",
        "user": "Lhasrt",
        "labels": []
    },
    {
        "number": 2341,
        "title": "IOT instruction (core dumped)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2341",
        "state": "open",
        "created_at": "2023-07-23T12:17:37Z",
        "user": "Freed-Wu",
        "labels": []
    },
    {
        "number": 2340,
        "title": "Segmentation fault: address not mapped to object at address (nil)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2340",
        "state": "open",
        "created_at": "2023-07-23T12:02:44Z",
        "user": "Freed-Wu",
        "labels": []
    },
    {
        "number": 2332,
        "title": "all llamacpp versions losing 20% performance after \"stand by\" ( sleep ) windows 11   ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2332",
        "state": "open",
        "created_at": "2023-07-22T21:16:38Z",
        "user": "mirek190",
        "labels": []
    },
    {
        "number": 2324,
        "title": "Investigate PG-TD (Planning-Guided Transformer Decoding) sampling",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2324",
        "state": "open",
        "created_at": "2023-07-22T15:27:27Z",
        "user": "walking-octopus",
        "labels": []
    },
    {
        "number": 2318,
        "title": "[Feature Request] train-text-from-scratch.cpp : GPU support and fine tuning",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2318",
        "state": "open",
        "created_at": "2023-07-22T07:27:53Z",
        "user": "YerongLi",
        "labels": []
    },
    {
        "number": 2305,
        "title": "tensor 'layers.7.attention_norm.weight' is missing from model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2305",
        "state": "open",
        "created_at": "2023-07-21T05:05:11Z",
        "user": "tonny2v",
        "labels": []
    },
    {
        "number": 2302,
        "title": "GPU Inference error -  Cannot find lcublasLt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2302",
        "state": "open",
        "created_at": "2023-07-21T01:16:06Z",
        "user": "gabrielcustodio",
        "labels": []
    },
    {
        "number": 2296,
        "title": "Consider adding repo-review badge to README",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2296",
        "state": "open",
        "created_at": "2023-07-20T15:26:58Z",
        "user": "repo-reviews",
        "labels": []
    },
    {
        "number": 2292,
        "title": "Use date as version",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2292",
        "state": "open",
        "created_at": "2023-07-20T12:20:42Z",
        "user": "Freed-Wu",
        "labels": []
    },
    {
        "number": 2288,
        "title": "Problem with gpu (help)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2288",
        "state": "open",
        "created_at": "2023-07-20T09:41:52Z",
        "user": "xajanix",
        "labels": []
    },
    {
        "number": 2283,
        "title": "Llama 2 and server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2283",
        "state": "open",
        "created_at": "2023-07-20T03:26:20Z",
        "user": "x4080",
        "labels": []
    },
    {
        "number": 2274,
        "title": "GPU offloading,  but there's no response output",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2274",
        "state": "open",
        "created_at": "2023-07-19T11:14:17Z",
        "user": "luoweb",
        "labels": []
    },
    {
        "number": 2273,
        "title": "Support for Baichuan-13B on GPU inference. (CUDA version of ALiBi OP)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2273",
        "state": "open",
        "created_at": "2023-07-19T08:53:58Z",
        "user": "LiuKai22",
        "labels": []
    },
    {
        "number": 2269,
        "title": "CUDA Error 400: Invalid Resource Handle when Running on Single GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2269",
        "state": "open",
        "created_at": "2023-07-19T03:47:47Z",
        "user": "michelg10",
        "labels": []
    },
    {
        "number": 2255,
        "title": "undefined reference to `__imp_*' error",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2255",
        "state": "open",
        "created_at": "2023-07-17T17:17:05Z",
        "user": "chujb",
        "labels": []
    },
    {
        "number": 2254,
        "title": "newer version-GPTQ to GGML convert failure",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2254",
        "state": "open",
        "created_at": "2023-07-17T16:11:43Z",
        "user": "joshuachris2001",
        "labels": []
    },
    {
        "number": 2251,
        "title": "[User] Huge Page Support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2251",
        "state": "open",
        "created_at": "2023-07-17T14:47:28Z",
        "user": "Micraow",
        "labels": []
    },
    {
        "number": 2249,
        "title": "GPU installation for Windows failing with multiple syntax errors",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2249",
        "state": "open",
        "created_at": "2023-07-17T12:09:15Z",
        "user": "FernandoFerrerTDK",
        "labels": []
    },
    {
        "number": 2247,
        "title": "FPE in llama_eval_internal when load a model.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2247",
        "state": "open",
        "created_at": "2023-07-17T03:38:20Z",
        "user": "360AIVul",
        "labels": []
    },
    {
        "number": 2246,
        "title": "Llama training finetuning interface",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2246",
        "state": "open",
        "created_at": "2023-07-17T01:50:08Z",
        "user": "howard0su",
        "labels": []
    },
    {
        "number": 2233,
        "title": "Add flag to make reverse prompt case insensitive",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2233",
        "state": "open",
        "created_at": "2023-07-15T20:00:10Z",
        "user": "dewijones92",
        "labels": []
    },
    {
        "number": 2229,
        "title": "[Requirement] Implement Metal support for Q5_0",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2229",
        "state": "open",
        "created_at": "2023-07-15T07:00:27Z",
        "user": "Kaguya-19",
        "labels": []
    },
    {
        "number": 2227,
        "title": "[User] Metal build not working",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2227",
        "state": "open",
        "created_at": "2023-07-15T06:19:11Z",
        "user": "benjaminhuo",
        "labels": []
    },
    {
        "number": 2223,
        "title": "CuBLAS version on Windows uses excessive ram",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2223",
        "state": "open",
        "created_at": "2023-07-14T17:29:55Z",
        "user": "Rotatingxenomorph",
        "labels": []
    },
    {
        "number": 2217,
        "title": "Add CFG to server",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2217",
        "state": "open",
        "created_at": "2023-07-13T19:55:29Z",
        "user": "SlyEcho",
        "labels": []
    },
    {
        "number": 2213,
        "title": "How to build a libllama.dylib file with \"arm64\" architecture?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2213",
        "state": "open",
        "created_at": "2023-07-13T09:56:55Z",
        "user": "realcarlos",
        "labels": []
    },
    {
        "number": 2209,
        "title": "Using MPI w/ 65b model but each node uses the full RAM.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2209",
        "state": "open",
        "created_at": "2023-07-13T02:57:34Z",
        "user": "magnusviri",
        "labels": [
            "help wanted"
        ]
    },
    {
        "number": 2205,
        "title": "[User] Insert summary of your issue or enhancement..",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2205",
        "state": "open",
        "created_at": "2023-07-12T22:40:40Z",
        "user": "MasterDimmy",
        "labels": []
    },
    {
        "number": 2204,
        "title": "Fix regression of model loading performance when using mlock.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2204",
        "state": "open",
        "created_at": "2023-07-12T22:29:57Z",
        "user": "spencersutton",
        "labels": []
    },
    {
        "number": 2200,
        "title": "Division by zero withing llama_calc_tensor_size when LLAMA_K_QUANTS is OFF.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2200",
        "state": "open",
        "created_at": "2023-07-12T17:44:44Z",
        "user": "zaciam",
        "labels": []
    },
    {
        "number": 2196,
        "title": "self.ctx = llama_cpp.llama_new_context_with_model(self.model, self.params)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2196",
        "state": "open",
        "created_at": "2023-07-12T15:03:06Z",
        "user": "pradeepdev-1995",
        "labels": []
    },
    {
        "number": 2194,
        "title": "Server suggestion: add HTTP basic access authentication",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2194",
        "state": "open",
        "created_at": "2023-07-12T14:51:34Z",
        "user": "csdvrx",
        "labels": []
    },
    {
        "number": 2185,
        "title": "[requirement] Support baichuan 13B model.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2185",
        "state": "open",
        "created_at": "2023-07-12T03:15:38Z",
        "user": "riverzhou",
        "labels": []
    },
    {
        "number": 2172,
        "title": "How to get encodings on each position?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2172",
        "state": "open",
        "created_at": "2023-07-11T03:51:52Z",
        "user": "Alexander-Jin",
        "labels": []
    },
    {
        "number": 2169,
        "title": "Share a simpler Cmake methd to compile and run GPU accelerated version(openBLAS and CLBlast) on android Qualcomm Adreno device.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2169",
        "state": "open",
        "created_at": "2023-07-11T02:10:25Z",
        "user": "hchenphd",
        "labels": []
    },
    {
        "number": 2168,
        "title": "Porting MPI PR to Darwin OpenMPI",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2168",
        "state": "open",
        "created_at": "2023-07-10T22:52:00Z",
        "user": "chadbrewbaker",
        "labels": []
    },
    {
        "number": 2165,
        "title": "Can't we use multiple GPUs independently?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2165",
        "state": "open",
        "created_at": "2023-07-10T20:15:48Z",
        "user": "gotzmann",
        "labels": []
    },
    {
        "number": 2164,
        "title": "mpi : attempt inference of 65B LLaMA on a cluster of Raspberry Pis",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2164",
        "state": "open",
        "created_at": "2023-07-10T16:12:22Z",
        "user": "ggerganov",
        "labels": [
            "help wanted",
            "\ud83e\udd99.",
            "hardware",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 2163,
        "title": "Sharing a link to some research on quantization referencing llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2163",
        "state": "open",
        "created_at": "2023-07-10T12:44:22Z",
        "user": "snichols",
        "labels": []
    },
    {
        "number": 2162,
        "title": "Makefile : LLAMA_FAST is incompatible with LLAMA_CUBLAS",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2162",
        "state": "open",
        "created_at": "2023-07-10T08:32:55Z",
        "user": "d-takemori",
        "labels": []
    },
    {
        "number": 2158,
        "title": "server options",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2158",
        "state": "open",
        "created_at": "2023-07-09T16:06:35Z",
        "user": "EdwardDali",
        "labels": []
    },
    {
        "number": 2156,
        "title": "C based server options missing",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2156",
        "state": "open",
        "created_at": "2023-07-09T14:17:58Z",
        "user": "aiaicode",
        "labels": []
    },
    {
        "number": 2149,
        "title": "FP32 matmult is not supported by CLBLAST and GPU based on Android phone?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2149",
        "state": "open",
        "created_at": "2023-07-08T14:13:15Z",
        "user": "hchenphd",
        "labels": []
    },
    {
        "number": 2141,
        "title": "violent crash on Mac Mini M2 8GB RAM when trying to use GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2141",
        "state": "open",
        "created_at": "2023-07-07T21:42:22Z",
        "user": "siddhsql",
        "labels": []
    },
    {
        "number": 2134,
        "title": "Large sched_yield() and threading overhead (+25-40% perf boost)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2134",
        "state": "open",
        "created_at": "2023-07-07T14:31:45Z",
        "user": "AndreiLux",
        "labels": []
    },
    {
        "number": 2133,
        "title": "Huge perplexity score generated by CLBLAST based on GPU of Android phone?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2133",
        "state": "open",
        "created_at": "2023-07-07T14:23:44Z",
        "user": "hchenphd",
        "labels": []
    },
    {
        "number": 2124,
        "title": "[User] FPE happened when load model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2124",
        "state": "open",
        "created_at": "2023-07-06T11:19:06Z",
        "user": "360AIVul",
        "labels": []
    },
    {
        "number": 2121,
        "title": "Met \"Segmentation fault\" issue when do the inference based via LLama ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2121",
        "state": "open",
        "created_at": "2023-07-06T01:23:43Z",
        "user": "JustinZou1",
        "labels": []
    },
    {
        "number": 2118,
        "title": "Help me understand the memory usage situation when using GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2118",
        "state": "open",
        "created_at": "2023-07-05T21:11:55Z",
        "user": "JianbangZ",
        "labels": []
    },
    {
        "number": 2117,
        "title": "Would it be possible to use llama.cpp for ARC, HellaSwag, MMLU and TruthfulQA benchmarking?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2117",
        "state": "open",
        "created_at": "2023-07-05T19:32:51Z",
        "user": "TyraVex",
        "labels": []
    },
    {
        "number": 2111,
        "title": "Add FlanT5 architecture to support true inference on the edge",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2111",
        "state": "open",
        "created_at": "2023-07-05T11:30:45Z",
        "user": "AmgadHasan",
        "labels": []
    },
    {
        "number": 2109,
        "title": "Compatibility with aws inf1/2 instances",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2109",
        "state": "open",
        "created_at": "2023-07-05T08:58:40Z",
        "user": "sonic182",
        "labels": []
    },
    {
        "number": 2108,
        "title": "Simultaneous interactive and instruction mode to extend/improve model?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2108",
        "state": "open",
        "created_at": "2023-07-05T06:16:37Z",
        "user": "FrankDMartinez",
        "labels": []
    },
    {
        "number": 2093,
        "title": "add support of Aquila 7B models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2093",
        "state": "open",
        "created_at": "2023-07-04T05:57:35Z",
        "user": "ftgreat",
        "labels": []
    },
    {
        "number": 2090,
        "title": "illegal instrution",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2090",
        "state": "open",
        "created_at": "2023-07-03T17:15:27Z",
        "user": "adaaaaaa",
        "labels": []
    },
    {
        "number": 2073,
        "title": "Add BPE dropout support, use it in training.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2073",
        "state": "open",
        "created_at": "2023-07-02T14:57:26Z",
        "user": "howard0su",
        "labels": []
    },
    {
        "number": 2072,
        "title": "[llama] Add resegment post processing of tokenizer",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2072",
        "state": "open",
        "created_at": "2023-07-02T14:06:41Z",
        "user": "howard0su",
        "labels": []
    },
    {
        "number": 2069,
        "title": "Avoid allocating beyond MTLDevice.recommendedMaxWorkingSetSize",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2069",
        "state": "open",
        "created_at": "2023-07-02T04:51:55Z",
        "user": "kiltyj",
        "labels": []
    },
    {
        "number": 2053,
        "title": "convert.py xgen support",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2053",
        "state": "open",
        "created_at": "2023-06-30T02:34:58Z",
        "user": "tmm1",
        "labels": [
            "model"
        ]
    },
    {
        "number": 2052,
        "title": "Pool Android performance and GPU not used at all when built with OpenCL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2052",
        "state": "open",
        "created_at": "2023-06-29T18:40:30Z",
        "user": "JianbangZ",
        "labels": []
    },
    {
        "number": 2049,
        "title": "Training Custom using train-text-from-scratch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2049",
        "state": "open",
        "created_at": "2023-06-29T15:21:57Z",
        "user": "archit0",
        "labels": [
            "training"
        ]
    },
    {
        "number": 2047,
        "title": "Question: How to access feature vector of the intermediate layer of network?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2047",
        "state": "open",
        "created_at": "2023-06-29T06:31:32Z",
        "user": "sohta94",
        "labels": []
    },
    {
        "number": 2037,
        "title": "try to fix compile warnings on macOS, address issue #2036",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2037",
        "state": "open",
        "created_at": "2023-06-28T10:32:41Z",
        "user": "mqy",
        "labels": []
    },
    {
        "number": 2036,
        "title": "[User] make prints warnings on macOS",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2036",
        "state": "open",
        "created_at": "2023-06-28T10:17:11Z",
        "user": "mqy",
        "labels": []
    },
    {
        "number": 2034,
        "title": "Question: Use cmake to compile CLBAST on windows",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2034",
        "state": "open",
        "created_at": "2023-06-28T09:46:25Z",
        "user": "mikeyang01",
        "labels": []
    },
    {
        "number": 2029,
        "title": "Avoid unused constant warnings",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2029",
        "state": "open",
        "created_at": "2023-06-28T01:05:13Z",
        "user": "set-soft",
        "labels": []
    },
    {
        "number": 2026,
        "title": "Example work stealing chunked task allocator for issue #291",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2026",
        "state": "open",
        "created_at": "2023-06-27T20:21:10Z",
        "user": "mqy",
        "labels": []
    },
    {
        "number": 2021,
        "title": "Support for large context sizes",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2021",
        "state": "open",
        "created_at": "2023-06-27T14:11:25Z",
        "user": "ikawrakow",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 2014,
        "title": "[User] Make error \"error: no matching function for call to 'ggml_rope_inplace'\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2014",
        "state": "open",
        "created_at": "2023-06-26T22:04:28Z",
        "user": "yeasy",
        "labels": []
    },
    {
        "number": 2011,
        "title": "metal : try to utilize more of the shared memory using smaller views",
        "url": "https://github.com/ggerganov/llama.cpp/pull/2011",
        "state": "open",
        "created_at": "2023-06-26T19:24:19Z",
        "user": "ggerganov",
        "labels": []
    },
    {
        "number": 2004,
        "title": "builds on jetson orin failure",
        "url": "https://github.com/ggerganov/llama.cpp/issues/2004",
        "state": "open",
        "created_at": "2023-06-26T13:44:50Z",
        "user": "malv-c",
        "labels": []
    },
    {
        "number": 1994,
        "title": "[dejimarquis] Convert.py ''unknown format: models/open_llama_7b/pytorch_model-00001-of-00002.bin\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1994",
        "state": "open",
        "created_at": "2023-06-25T16:48:06Z",
        "user": "dejimarquis",
        "labels": []
    },
    {
        "number": 1986,
        "title": "Draft: #1776 making bos and eos available for user input",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1986",
        "state": "open",
        "created_at": "2023-06-24T23:54:43Z",
        "user": "HashemAlsaket",
        "labels": []
    },
    {
        "number": 1984,
        "title": "Are there any plans to support GPU offloading with LORAs?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1984",
        "state": "open",
        "created_at": "2023-06-24T14:48:47Z",
        "user": "l3utterfly",
        "labels": []
    },
    {
        "number": 1976,
        "title": "it is usefull",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1976",
        "state": "open",
        "created_at": "2023-06-23T15:18:38Z",
        "user": "newjcj",
        "labels": []
    },
    {
        "number": 1955,
        "title": "Investigate PagedAttention KV-cache memory management for faster inference",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1955",
        "state": "open",
        "created_at": "2023-06-20T22:47:57Z",
        "user": "Azeirah",
        "labels": []
    },
    {
        "number": 1948,
        "title": "[User] 65B models on CUBLAS/cuda bugged when prompts approach model's max context size",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1948",
        "state": "open",
        "created_at": "2023-06-20T06:23:56Z",
        "user": "ycros",
        "labels": []
    },
    {
        "number": 1938,
        "title": "Can we finetune existing models via the ideas of QLORA",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1938",
        "state": "open",
        "created_at": "2023-06-19T13:48:02Z",
        "user": "howard0su",
        "labels": []
    },
    {
        "number": 1927,
        "title": "make is error ( k_quants.c)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1927",
        "state": "open",
        "created_at": "2023-06-18T14:12:02Z",
        "user": "zhp8341",
        "labels": []
    },
    {
        "number": 1909,
        "title": "[User] nonsense responses with q2_k llama in Termux when using GPU",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1909",
        "state": "open",
        "created_at": "2023-06-17T13:48:07Z",
        "user": "ghost",
        "labels": []
    },
    {
        "number": 1906,
        "title": "Use llama.cpp to load the model on Android Termux",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1906",
        "state": "open",
        "created_at": "2023-06-17T05:54:35Z",
        "user": "TangYuanMaster",
        "labels": []
    },
    {
        "number": 1900,
        "title": "Question about new models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1900",
        "state": "open",
        "created_at": "2023-06-16T20:38:58Z",
        "user": "djtech-dev",
        "labels": []
    },
    {
        "number": 1897,
        "title": "Disable _O_WTEXT when using main in MinGW",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1897",
        "state": "open",
        "created_at": "2023-06-16T16:10:46Z",
        "user": "asctime",
        "labels": []
    },
    {
        "number": 1890,
        "title": "Apple Studio m1 max out of memory?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1890",
        "state": "open",
        "created_at": "2023-06-16T11:21:14Z",
        "user": "JasonOSX",
        "labels": []
    },
    {
        "number": 1885,
        "title": "Cuda runtime error and slow eval",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1885",
        "state": "open",
        "created_at": "2023-06-16T04:02:19Z",
        "user": "huichen",
        "labels": []
    },
    {
        "number": 1883,
        "title": "Performance Disparity between arm64 and x86_64 Initialization Times",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1883",
        "state": "open",
        "created_at": "2023-06-16T01:25:20Z",
        "user": "albertstarfield",
        "labels": []
    },
    {
        "number": 1882,
        "title": "Question : Perplexity computation of short (very short) prompt.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1882",
        "state": "open",
        "created_at": "2023-06-15T18:52:32Z",
        "user": "ZergCitizen",
        "labels": []
    },
    {
        "number": 1877,
        "title": "perplexity mismatch with GPTQ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1877",
        "state": "open",
        "created_at": "2023-06-15T16:14:19Z",
        "user": "JianbangZ",
        "labels": []
    },
    {
        "number": 1870,
        "title": "[User] GPU Memory problem on Apple M2 Max 64GB ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1870",
        "state": "open",
        "created_at": "2023-06-15T10:45:55Z",
        "user": "CyborgArmy83",
        "labels": []
    },
    {
        "number": 1869,
        "title": "train-text-from-scratch.exe stop after \"begin training\" (tensor->src0 is null)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1869",
        "state": "open",
        "created_at": "2023-06-15T07:26:00Z",
        "user": "Entretoize",
        "labels": []
    },
    {
        "number": 1868,
        "title": "How do I install with Make?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1868",
        "state": "open",
        "created_at": "2023-06-15T07:06:17Z",
        "user": "chrisbward",
        "labels": []
    },
    {
        "number": 1866,
        "title": "CUDA out of memory - but there's plenty of memory",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1866",
        "state": "open",
        "created_at": "2023-06-15T03:46:02Z",
        "user": "Energiz3r",
        "labels": []
    },
    {
        "number": 1865,
        "title": "[IDEA] Global token enhancement/depression",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1865",
        "state": "open",
        "created_at": "2023-06-15T02:24:07Z",
        "user": "pauldog",
        "labels": [
            "help wanted",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 1859,
        "title": "Can't seem to load 65B Q4K_M models with Metal on Apple Max silicon with 64GB memory",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1859",
        "state": "open",
        "created_at": "2023-06-14T17:51:13Z",
        "user": "Xonar92",
        "labels": []
    },
    {
        "number": 1846,
        "title": "Applying lora with CUDA crashes with failed assertion",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1846",
        "state": "open",
        "created_at": "2023-06-14T06:29:48Z",
        "user": "d-takemori",
        "labels": [
            "wontfix"
        ]
    },
    {
        "number": 1845,
        "title": "Is it possible to load Excel files for data analysis or upload code for conversion via CLI?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1845",
        "state": "open",
        "created_at": "2023-06-14T02:22:55Z",
        "user": "breisig",
        "labels": []
    },
    {
        "number": 1844,
        "title": "After Converting my LLaMa file to HF, zip file error trying to convert to GGML. ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1844",
        "state": "open",
        "created_at": "2023-06-14T00:57:35Z",
        "user": "langem",
        "labels": []
    },
    {
        "number": 1843,
        "title": "[User] Training examples sometimes gets broken when training data is in Japanese",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1843",
        "state": "open",
        "created_at": "2023-06-13T20:05:50Z",
        "user": "Igoorx",
        "labels": []
    },
    {
        "number": 1841,
        "title": "[Contributor] Proposal - adding node bindings, similar to whisper, a la examples/addon.node",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1841",
        "state": "open",
        "created_at": "2023-06-13T19:09:05Z",
        "user": "yacineMTB",
        "labels": []
    },
    {
        "number": 1837,
        "title": "No longer runs on a Telsa P40 (Pascal, sm61)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1837",
        "state": "open",
        "created_at": "2023-06-13T15:12:43Z",
        "user": "csdvrx",
        "labels": []
    },
    {
        "number": 1835,
        "title": "Suggestions for Contributing as non-C developer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1835",
        "state": "open",
        "created_at": "2023-06-13T11:09:14Z",
        "user": "robinnarsinghranabhat",
        "labels": []
    },
    {
        "number": 1834,
        "title": "Added Arbitrary mixed quantization",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1834",
        "state": "open",
        "created_at": "2023-06-13T09:11:49Z",
        "user": "Milkdrop",
        "labels": [
            "research \ud83d\udd2c",
            "Less than 4 bits"
        ]
    },
    {
        "number": 1833,
        "title": "[User] bad magic",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1833",
        "state": "open",
        "created_at": "2023-06-13T08:25:09Z",
        "user": "vinitran",
        "labels": []
    },
    {
        "number": 1832,
        "title": "CMake Picks Up Wrong `nvcc` Path During Build",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1832",
        "state": "open",
        "created_at": "2023-06-13T05:49:01Z",
        "user": "bhy",
        "labels": []
    },
    {
        "number": 1829,
        "title": "It does not compile with OpenBlas as include path is not correctly set",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1829",
        "state": "open",
        "created_at": "2023-06-13T04:09:10Z",
        "user": "okigan",
        "labels": []
    },
    {
        "number": 1814,
        "title": "What is the input embedding of LlaMa? Is it trainable?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1814",
        "state": "open",
        "created_at": "2023-06-12T07:12:59Z",
        "user": "magic-YuanTian",
        "labels": []
    },
    {
        "number": 1813,
        "title": "GGML_ASSERT: /home/circleci/project/gpt4all-backend/llama.cpp-230511/ggml.c:4411: ctx->mem_buffer != NULL",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1813",
        "state": "open",
        "created_at": "2023-06-12T05:59:38Z",
        "user": "khalidalmannai",
        "labels": []
    },
    {
        "number": 1812,
        "title": "Vicuna 1.1 / special_tokens_map.json support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1812",
        "state": "open",
        "created_at": "2023-06-12T05:53:50Z",
        "user": "anon998",
        "labels": []
    },
    {
        "number": 1806,
        "title": "Issue : Instructions of using GPT4ALL models are unclear",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1806",
        "state": "open",
        "created_at": "2023-06-11T20:34:11Z",
        "user": "eshaanagarwal",
        "labels": []
    },
    {
        "number": 1799,
        "title": "Honest Llama",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1799",
        "state": "open",
        "created_at": "2023-06-11T08:10:57Z",
        "user": "m1chae1bx",
        "labels": []
    },
    {
        "number": 1793,
        "title": "Program hangs after some time",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1793",
        "state": "open",
        "created_at": "2023-06-10T18:07:33Z",
        "user": "siddhsql",
        "labels": []
    },
    {
        "number": 1788,
        "title": "[build] Compilation error on build: k_quants.h:26:15: error: expected declaration specifiers or '...' before 'sizeof'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1788",
        "state": "open",
        "created_at": "2023-06-10T13:04:25Z",
        "user": "ya-ds-web",
        "labels": []
    },
    {
        "number": 1786,
        "title": "Generation using GPU offloading is much slower than without",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1786",
        "state": "open",
        "created_at": "2023-06-10T07:58:45Z",
        "user": "Barafu",
        "labels": []
    },
    {
        "number": 1781,
        "title": "Is there any LLAMA.CPP .exe package that can be used for the inference process in CLI",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1781",
        "state": "open",
        "created_at": "2023-06-09T16:09:19Z",
        "user": "ekolawole",
        "labels": []
    },
    {
        "number": 1780,
        "title": "[Feature] Support resetting the status of llama_context",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1780",
        "state": "open",
        "created_at": "2023-06-09T16:07:18Z",
        "user": "AsakusaRinne",
        "labels": []
    },
    {
        "number": 1776,
        "title": "Eos and bos tokens can be redefined as additional tokens with other ids",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1776",
        "state": "open",
        "created_at": "2023-06-09T08:15:53Z",
        "user": "vvasily",
        "labels": [
            "enhancement",
            "good first issue"
        ]
    },
    {
        "number": 1771,
        "title": "Server slows down after each question (with gpu).",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1771",
        "state": "open",
        "created_at": "2023-06-09T03:38:46Z",
        "user": "msj121",
        "labels": []
    },
    {
        "number": 1766,
        "title": "Seeking Recommendations for Optimal AWS EC2 Instance for Hosting llama.cpp (CPU)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1766",
        "state": "open",
        "created_at": "2023-06-08T21:30:10Z",
        "user": "tgcandido",
        "labels": []
    },
    {
        "number": 1765,
        "title": "feature request - disabling tokenizer in conversion / inference",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1765",
        "state": "open",
        "created_at": "2023-06-08T20:14:33Z",
        "user": "genenwoochoi",
        "labels": [
            "enhancement",
            "help wanted",
            "good first issue"
        ]
    },
    {
        "number": 1764,
        "title": "feature request - support huggingface tokenizer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1764",
        "state": "open",
        "created_at": "2023-06-08T19:34:25Z",
        "user": "keunwoochoi",
        "labels": []
    },
    {
        "number": 1761,
        "title": "Unable to use Intel UHD GPU acceleration with BLAS ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1761",
        "state": "open",
        "created_at": "2023-06-08T14:46:33Z",
        "user": "Foul-Tarnished",
        "labels": []
    },
    {
        "number": 1760,
        "title": "[Feature Request] Restart a fresh conversation while using `interactive-first`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1760",
        "state": "open",
        "created_at": "2023-06-08T14:24:45Z",
        "user": "Foul-Tarnished",
        "labels": []
    },
    {
        "number": 1754,
        "title": "batch inference",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1754",
        "state": "open",
        "created_at": "2023-06-08T10:02:33Z",
        "user": "liuxiaohao-xn",
        "labels": []
    },
    {
        "number": 1744,
        "title": "[User] Generating embeddings is not using GPU when built with LLAMA_METAL=ON",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1744",
        "state": "open",
        "created_at": "2023-06-07T19:12:58Z",
        "user": "afg1",
        "labels": []
    },
    {
        "number": 1743,
        "title": "I am trying to load  vicunlocked-65b.ggmlv3.q2_K.bin  but nothing happening just quit without any error.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1743",
        "state": "open",
        "created_at": "2023-06-07T14:58:29Z",
        "user": "mirek190",
        "labels": []
    },
    {
        "number": 1742,
        "title": "API reference?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1742",
        "state": "open",
        "created_at": "2023-06-07T14:22:44Z",
        "user": "jerrimus",
        "labels": []
    },
    {
        "number": 1739,
        "title": "Feature request: documented machine-readable output as a top tier feature",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1739",
        "state": "open",
        "created_at": "2023-06-07T09:24:55Z",
        "user": "simonw",
        "labels": []
    },
    {
        "number": 1728,
        "title": "I get the following error when compiling with make LLAMA_CUBLAS=1 :",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1728",
        "state": "open",
        "created_at": "2023-06-07T04:13:00Z",
        "user": "xjDUAN184",
        "labels": []
    },
    {
        "number": 1725,
        "title": "CLBlast build failing on q3 model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1725",
        "state": "open",
        "created_at": "2023-06-06T23:52:55Z",
        "user": "TheTerrasque",
        "labels": []
    },
    {
        "number": 1721,
        "title": "Broken build on and after tag `master-99009e7`",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1721",
        "state": "open",
        "created_at": "2023-06-06T20:16:01Z",
        "user": "rbrisita",
        "labels": []
    },
    {
        "number": 1719,
        "title": "VERY VERY Slow on the rtx 4050 and i5-12455 and 16 gb ram",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1719",
        "state": "open",
        "created_at": "2023-06-06T18:13:49Z",
        "user": "Asory2010",
        "labels": []
    },
    {
        "number": 1718,
        "title": "[User] Increasing ngl parameter decreases performance in Termux",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1718",
        "state": "open",
        "created_at": "2023-06-06T16:44:24Z",
        "user": "ghost",
        "labels": []
    },
    {
        "number": 1717,
        "title": "trying to do GPU acceleration but stuck on CC not available.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1717",
        "state": "open",
        "created_at": "2023-06-06T16:20:19Z",
        "user": "mberman84",
        "labels": []
    },
    {
        "number": 1714,
        "title": "Support CoreML like whisper.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1714",
        "state": "open",
        "created_at": "2023-06-06T09:23:08Z",
        "user": "realcarlos",
        "labels": [
            "help wanted",
            "performance",
            "macos"
        ]
    },
    {
        "number": 1713,
        "title": "[Feature request] AWQ (activation-aware weight quantization) 4-bit quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1713",
        "state": "open",
        "created_at": "2023-06-06T08:50:48Z",
        "user": "hfassold",
        "labels": []
    },
    {
        "number": 1704,
        "title": "Server example not working - failing on Runtime Error unexpected EOF",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1704",
        "state": "open",
        "created_at": "2023-06-05T18:53:49Z",
        "user": "BartlomiejLewandowski",
        "labels": []
    },
    {
        "number": 1694,
        "title": "Technical details about quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1694",
        "state": "open",
        "created_at": "2023-06-05T02:48:17Z",
        "user": "Tunaaaaa",
        "labels": []
    },
    {
        "number": 1692,
        "title": "Added tensor layer numbers",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1692",
        "state": "open",
        "created_at": "2023-06-04T19:07:27Z",
        "user": "dkun7944",
        "labels": []
    },
    {
        "number": 1687,
        "title": "Unlimited context length now possible ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1687",
        "state": "open",
        "created_at": "2023-06-03T23:29:11Z",
        "user": "ekolawole",
        "labels": []
    },
    {
        "number": 1686,
        "title": "WIP: New python based entry point for containers",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1686",
        "state": "open",
        "created_at": "2023-06-03T20:43:38Z",
        "user": "jpodivin",
        "labels": []
    },
    {
        "number": 1685,
        "title": "AWQ: Activation-aware Weight Quantization???",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1685",
        "state": "open",
        "created_at": "2023-06-03T15:42:42Z",
        "user": "UnsGentoals",
        "labels": []
    },
    {
        "number": 1676,
        "title": "60f8c36 commit has issues with gpu offload (low speed, out of memory",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1676",
        "state": "open",
        "created_at": "2023-06-02T16:05:51Z",
        "user": "Harlok13",
        "labels": [
            "need more info"
        ]
    },
    {
        "number": 1668,
        "title": "ggml_cl_pool_malloc potential issue",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1668",
        "state": "open",
        "created_at": "2023-06-01T17:05:31Z",
        "user": "LostRuins",
        "labels": []
    },
    {
        "number": 1663,
        "title": "it\u2019s so long a lime to wait while using a server",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1663",
        "state": "open",
        "created_at": "2023-06-01T08:38:02Z",
        "user": "niubi-AI",
        "labels": []
    },
    {
        "number": 1660,
        "title": "Llama cpp low level python bindings",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1660",
        "state": "open",
        "created_at": "2023-06-01T06:18:02Z",
        "user": "dmahurin",
        "labels": []
    },
    {
        "number": 1656,
        "title": "Output of quantized Vicuna is so inappropriate that I can't use it",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1656",
        "state": "open",
        "created_at": "2023-05-31T12:09:49Z",
        "user": "JerryYao80",
        "labels": []
    },
    {
        "number": 1655,
        "title": "[User] error: inlining failed in call to 'always_inline' 'vfmaq_f16': target specific option mismatch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1655",
        "state": "open",
        "created_at": "2023-05-31T04:12:19Z",
        "user": "Kangmo",
        "labels": []
    },
    {
        "number": 1651,
        "title": "How does \"-ins\"/Instruct/chat mode remembers contexts? How to properly feed it back as plain text?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1651",
        "state": "open",
        "created_at": "2023-05-30T14:43:25Z",
        "user": "aleksusklim",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 1643,
        "title": "[Bastian1110] tTrouble using convert.py with meta provided models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1643",
        "state": "open",
        "created_at": "2023-05-29T18:36:43Z",
        "user": "Bastian1110",
        "labels": []
    },
    {
        "number": 1633,
        "title": "[User] GCC 7.5.0 compilation failure fix",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1633",
        "state": "open",
        "created_at": "2023-05-29T03:03:25Z",
        "user": "wyklq",
        "labels": []
    },
    {
        "number": 1622,
        "title": "[User] How to specify which cuda device to use programmably",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1622",
        "state": "open",
        "created_at": "2023-05-28T07:43:56Z",
        "user": "huichen",
        "labels": []
    },
    {
        "number": 1620,
        "title": "cuBLAS fails on same model - terminate called after throwing an instance of 'std::runtime_error'   what():  unexpectedly reached end of file",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1620",
        "state": "open",
        "created_at": "2023-05-28T05:25:42Z",
        "user": "dynamite9999",
        "labels": []
    },
    {
        "number": 1603,
        "title": "JSON parser differential ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1603",
        "state": "open",
        "created_at": "2023-05-26T18:34:31Z",
        "user": "suhacker1",
        "labels": []
    },
    {
        "number": 1591,
        "title": "Programs released by the release page are slower than those built locally.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1591",
        "state": "open",
        "created_at": "2023-05-25T07:02:41Z",
        "user": "rankaiyx",
        "labels": []
    },
    {
        "number": 1583,
        "title": "Cmake file always assumes AVX2 support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1583",
        "state": "open",
        "created_at": "2023-05-24T03:47:38Z",
        "user": "diwu1989",
        "labels": [
            "bug",
            "high priority",
            "build"
        ]
    },
    {
        "number": 1581,
        "title": "MinGW: clblast \"variable length arrays are not supported\", unicode issues in common.c",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1581",
        "state": "open",
        "created_at": "2023-05-24T01:59:59Z",
        "user": "asctime",
        "labels": []
    },
    {
        "number": 1575,
        "title": "Extend ggml format to include a description of the model.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1575",
        "state": "open",
        "created_at": "2023-05-23T16:39:04Z",
        "user": "darxkies",
        "labels": [
            "enhancement",
            "help wanted"
        ]
    },
    {
        "number": 1552,
        "title": "[Feature] Is there a way to get response with embeddings as input?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1552",
        "state": "open",
        "created_at": "2023-05-21T13:32:32Z",
        "user": "AsakusaRinne",
        "labels": []
    },
    {
        "number": 1551,
        "title": "benchmark-matmult broken when building without BLAS",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1551",
        "state": "open",
        "created_at": "2023-05-21T13:21:34Z",
        "user": "stsydow",
        "labels": []
    },
    {
        "number": 1537,
        "title": "Feature: Short but helpful incompatibility information on ggml file version",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1537",
        "state": "open",
        "created_at": "2023-05-20T10:51:19Z",
        "user": "dronus",
        "labels": []
    },
    {
        "number": 1535,
        "title": "API function to check if llama version supports given file",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1535",
        "state": "open",
        "created_at": "2023-05-20T09:09:18Z",
        "user": "niansa",
        "labels": []
    },
    {
        "number": 1515,
        "title": "Slower Response on large context size",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1515",
        "state": "open",
        "created_at": "2023-05-18T12:58:30Z",
        "user": "iamirulofficial",
        "labels": []
    },
    {
        "number": 1507,
        "title": "ggml : spread compute across threads in chunks",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1507",
        "state": "open",
        "created_at": "2023-05-17T17:12:10Z",
        "user": "ggerganov",
        "labels": [
            "threading",
            "demo"
        ]
    },
    {
        "number": 1505,
        "title": "ci: add linux binaries to release build",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1505",
        "state": "open",
        "created_at": "2023-05-17T16:50:39Z",
        "user": "Green-Sky",
        "labels": []
    },
    {
        "number": 1504,
        "title": "Upgrade v1/v2 format to v3 by leveraging quantize",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1504",
        "state": "open",
        "created_at": "2023-05-17T15:44:26Z",
        "user": "howard0su",
        "labels": []
    },
    {
        "number": 1501,
        "title": "[Feature request] How to support special tokens in tokenizer of llama_cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1501",
        "state": "open",
        "created_at": "2023-05-17T10:35:22Z",
        "user": "Snowdar",
        "labels": []
    },
    {
        "number": 1499,
        "title": "[Feature request] Any plans for AMD XDNA AI Engine support on Ryzen 7x40 processors?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1499",
        "state": "open",
        "created_at": "2023-05-17T09:57:42Z",
        "user": "KarmaMonk",
        "labels": []
    },
    {
        "number": 1494,
        "title": "[Enhancement] Simultaneous CLBLAS/CUBLAS instances. ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1494",
        "state": "open",
        "created_at": "2023-05-17T03:14:18Z",
        "user": "AlphaAtlas",
        "labels": []
    },
    {
        "number": 1492,
        "title": "Feature - Internal ggml precision GGML_TYPE_F16 support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1492",
        "state": "open",
        "created_at": "2023-05-17T02:15:17Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 1489,
        "title": "[Feature request] Add a way to pipe text into a running session",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1489",
        "state": "open",
        "created_at": "2023-05-17T01:09:16Z",
        "user": "skidd-level-100",
        "labels": []
    },
    {
        "number": 1488,
        "title": "Hundreds of \"undefined\" errors when compiling with LLAMA_CUBLAS=1",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1488",
        "state": "open",
        "created_at": "2023-05-16T22:38:55Z",
        "user": "Dragons10523",
        "labels": []
    },
    {
        "number": 1484,
        "title": "Readme: add akx/ggify to tools",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1484",
        "state": "open",
        "created_at": "2023-05-16T16:19:59Z",
        "user": "akx",
        "labels": []
    },
    {
        "number": 1472,
        "title": "[Research] Steering vectors",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1472",
        "state": "open",
        "created_at": "2023-05-16T00:14:50Z",
        "user": "SlyEcho",
        "labels": [
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 1471,
        "title": "[User] Insert summary of your issue or enhancement..",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1471",
        "state": "open",
        "created_at": "2023-05-15T21:33:27Z",
        "user": "adityachallapally",
        "labels": []
    },
    {
        "number": 1465,
        "title": "mac M1: very low memory usage on 7B but 10x higher on 13B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1465",
        "state": "open",
        "created_at": "2023-05-15T09:27:34Z",
        "user": "DKormann",
        "labels": []
    },
    {
        "number": 1464,
        "title": "Error to loading the latest Chinese LLaMa Alpaca model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1464",
        "state": "open",
        "created_at": "2023-05-15T08:05:58Z",
        "user": "done434",
        "labels": []
    },
    {
        "number": 1460,
        "title": "Investigate and play with \"steering vectors\" post (paper)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1460",
        "state": "open",
        "created_at": "2023-05-14T21:36:55Z",
        "user": "Azeirah",
        "labels": []
    },
    {
        "number": 1457,
        "title": "Stops Mid-Prompt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1457",
        "state": "open",
        "created_at": "2023-05-14T18:47:05Z",
        "user": "Asory2010",
        "labels": []
    },
    {
        "number": 1451,
        "title": "60f8c36  commit has issues with gpu offload ?  (low speed, out of memory)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1451",
        "state": "open",
        "created_at": "2023-05-14T14:06:55Z",
        "user": "drimeF0",
        "labels": []
    },
    {
        "number": 1449,
        "title": "How to output the ggml file with Lora merged?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1449",
        "state": "open",
        "created_at": "2023-05-14T09:47:04Z",
        "user": "FNsi",
        "labels": []
    },
    {
        "number": 1448,
        "title": "Can I offload the workload on GPU instead of CPU?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1448",
        "state": "open",
        "created_at": "2023-05-14T09:19:48Z",
        "user": "m0sh1x2",
        "labels": []
    },
    {
        "number": 1447,
        "title": "[Feature request] add openCL acceleration support?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1447",
        "state": "open",
        "created_at": "2023-05-14T07:35:02Z",
        "user": "skidd-level-100",
        "labels": []
    },
    {
        "number": 1444,
        "title": "Final trailing LF stripped off prompt when using --file",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1444",
        "state": "open",
        "created_at": "2023-05-14T04:15:21Z",
        "user": "rmc135",
        "labels": []
    },
    {
        "number": 1441,
        "title": "HuggingFaceH4/starchat-alpha CPP LLM",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1441",
        "state": "open",
        "created_at": "2023-05-14T00:52:32Z",
        "user": "ekolawole",
        "labels": []
    },
    {
        "number": 1437,
        "title": "LLaMA NUMA could be better",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1437",
        "state": "open",
        "created_at": "2023-05-13T20:49:45Z",
        "user": "zrm",
        "labels": [
            "performance"
        ]
    },
    {
        "number": 1433,
        "title": "How to implement CLBLAST ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1433",
        "state": "open",
        "created_at": "2023-05-13T16:18:48Z",
        "user": "LeLaboDuGame",
        "labels": []
    },
    {
        "number": 1408,
        "title": " this format is no longer supported",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1408",
        "state": "open",
        "created_at": "2023-05-11T22:15:12Z",
        "user": "apcameron",
        "labels": []
    },
    {
        "number": 1402,
        "title": "llama.cpp runs WizardLM 7B",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1402",
        "state": "open",
        "created_at": "2023-05-11T10:56:50Z",
        "user": "rozek",
        "labels": []
    },
    {
        "number": 1400,
        "title": "[Feature request] make model combination possible (long shot!)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1400",
        "state": "open",
        "created_at": "2023-05-11T00:57:29Z",
        "user": "skidd-level-100",
        "labels": []
    },
    {
        "number": 1398,
        "title": "[Feature Request] --prompt-cache-all + user input",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1398",
        "state": "open",
        "created_at": "2023-05-11T00:38:58Z",
        "user": "skidd-level-100",
        "labels": []
    },
    {
        "number": 1395,
        "title": "MacOS M2 8GB - Really slow response",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1395",
        "state": "open",
        "created_at": "2023-05-10T20:17:06Z",
        "user": "NxRoot",
        "labels": []
    },
    {
        "number": 1392,
        "title": "[Enhancement]: Global probability maximization with backtracking for better results. (aka Beam search)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1392",
        "state": "open",
        "created_at": "2023-05-10T16:54:24Z",
        "user": "pauldog",
        "labels": []
    },
    {
        "number": 1386,
        "title": "Does llama.cpp support gpt-2-q4?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1386",
        "state": "open",
        "created_at": "2023-05-10T02:42:35Z",
        "user": "realcarlos",
        "labels": []
    },
    {
        "number": 1385,
        "title": "[issue] [enhancement] add OpenCL guide and distro guides",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1385",
        "state": "open",
        "created_at": "2023-05-09T22:59:43Z",
        "user": "skidd-level-100",
        "labels": []
    },
    {
        "number": 1382,
        "title": "multiline-input",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1382",
        "state": "open",
        "created_at": "2023-05-09T17:07:27Z",
        "user": "mirek190",
        "labels": []
    },
    {
        "number": 1381,
        "title": "error loading model: missing tok_embeddings.weight",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1381",
        "state": "open",
        "created_at": "2023-05-09T16:58:32Z",
        "user": "realcarlos",
        "labels": []
    },
    {
        "number": 1362,
        "title": "[User] Support for newline reverse prompt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1362",
        "state": "open",
        "created_at": "2023-05-08T03:38:54Z",
        "user": "MrJackSpade",
        "labels": []
    },
    {
        "number": 1354,
        "title": "[User] Chinese alpaca need more REQ memory",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1354",
        "state": "open",
        "created_at": "2023-05-07T13:18:14Z",
        "user": "morningtzh",
        "labels": []
    },
    {
        "number": 1337,
        "title": "Implement Together Computer's Red Pajama 3B Base/Chat model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1337",
        "state": "open",
        "created_at": "2023-05-06T01:48:53Z",
        "user": "Jimexist",
        "labels": [
            "model"
        ]
    },
    {
        "number": 1319,
        "title": "[User] Unable to use nvidia gpu",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1319",
        "state": "open",
        "created_at": "2023-05-04T13:27:36Z",
        "user": "tchereau",
        "labels": []
    },
    {
        "number": 1306,
        "title": "cc1plus: warning: switch \u2018-mcpu=ares+crypto+ssbs+noprofile\u2019 conflicts with \u2018-march=armv8-a\u2019 switch",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1306",
        "state": "open",
        "created_at": "2023-05-03T21:27:29Z",
        "user": "hughobrien",
        "labels": []
    },
    {
        "number": 1299,
        "title": "Test replit-code-v1-3b model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1299",
        "state": "open",
        "created_at": "2023-05-03T15:47:01Z",
        "user": "abetlen",
        "labels": [
            "help wanted",
            "model"
        ]
    },
    {
        "number": 1295,
        "title": "RPTQ state of the art quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1295",
        "state": "open",
        "created_at": "2023-05-03T03:23:53Z",
        "user": "bennmann",
        "labels": [
            "generation quality",
            "research \ud83d\udd2c",
            "Less than 4 bits"
        ]
    },
    {
        "number": 1291,
        "title": "Try whether OpenLLaMa works",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1291",
        "state": "open",
        "created_at": "2023-05-02T21:53:20Z",
        "user": "prusnak",
        "labels": [
            "\ud83e\udd99.",
            "model"
        ]
    },
    {
        "number": 1278,
        "title": "Implement get_num_physical_cores() for Windows",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1278",
        "state": "open",
        "created_at": "2023-05-02T14:25:53Z",
        "user": "DannyDaemonic",
        "labels": []
    },
    {
        "number": 1274,
        "title": "Is it possible to increase the num_beams parameter?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1274",
        "state": "open",
        "created_at": "2023-05-02T08:09:13Z",
        "user": "WonpangNew",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 1269,
        "title": "cuBLAS: keep the weights in VRAM when possible",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1269",
        "state": "open",
        "created_at": "2023-05-01T16:52:50Z",
        "user": "slaren",
        "labels": [
            "enhancement",
            "performance"
        ]
    },
    {
        "number": 1243,
        "title": "Is it still make sense to align structs for AVX ?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1243",
        "state": "open",
        "created_at": "2023-04-29T21:48:01Z",
        "user": "gotzmann",
        "labels": [
            "enhancement",
            "performance"
        ]
    },
    {
        "number": 1222,
        "title": "conversational memory in disk?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1222",
        "state": "open",
        "created_at": "2023-04-29T05:41:23Z",
        "user": "yesbroc",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 1209,
        "title": "Wizardllm Model Does not let user type after first prompt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1209",
        "state": "open",
        "created_at": "2023-04-27T21:05:04Z",
        "user": "Asory2010",
        "labels": []
    },
    {
        "number": 1204,
        "title": "Create run.py",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1204",
        "state": "open",
        "created_at": "2023-04-27T14:39:49Z",
        "user": "jdpsl",
        "labels": []
    },
    {
        "number": 1198,
        "title": "Getting started documentation",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1198",
        "state": "open",
        "created_at": "2023-04-26T21:26:33Z",
        "user": "TheNotary",
        "labels": []
    },
    {
        "number": 1194,
        "title": "perf(cuBLAS): store device pointers in ggml_tensor",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1194",
        "state": "open",
        "created_at": "2023-04-26T18:28:20Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 1193,
        "title": "Tokenization Example",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1193",
        "state": "open",
        "created_at": "2023-04-26T17:43:16Z",
        "user": "rozek",
        "labels": []
    },
    {
        "number": 1192,
        "title": "perf(CuBLAS): explore reduction in launch overhead via CUDA graphs",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1192",
        "state": "open",
        "created_at": "2023-04-26T17:19:37Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 1190,
        "title": "fix(LoRA): debugging",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1190",
        "state": "open",
        "created_at": "2023-04-26T15:33:52Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 1189,
        "title": "fix (perf/UX): get physical cores for Windows",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1189",
        "state": "open",
        "created_at": "2023-04-26T14:41:54Z",
        "user": "jon-chuang",
        "labels": [
            "enhancement",
            "hardware",
            "windows",
            "threading"
        ]
    },
    {
        "number": 1188,
        "title": "[Suggestion] Add parameter for setting openblas threads",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1188",
        "state": "open",
        "created_at": "2023-04-26T13:24:17Z",
        "user": "klosax",
        "labels": [
            "enhancement",
            "threading"
        ]
    },
    {
        "number": 1186,
        "title": "[Suggestion] Reduce API surface of ggml.h",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1186",
        "state": "open",
        "created_at": "2023-04-26T01:23:21Z",
        "user": "howard0su",
        "labels": []
    },
    {
        "number": 1175,
        "title": "Can't convert LoRA to ggml on windows, files in correct structure No such file or directory: 'lora/llama-oasst-lora-13B\\\\adapter_model.bin'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1175",
        "state": "open",
        "created_at": "2023-04-25T17:10:30Z",
        "user": "captainzero93",
        "labels": []
    },
    {
        "number": 1158,
        "title": "\"Press Ctrl+C to interject at any time\" is a bad chice - use something else, maybe [Esc]",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1158",
        "state": "open",
        "created_at": "2023-04-24T18:29:00Z",
        "user": "maddes8cht",
        "labels": []
    },
    {
        "number": 1135,
        "title": "Feature Request: Alternate Interrupt method in instruct mode",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1135",
        "state": "open",
        "created_at": "2023-04-23T04:19:32Z",
        "user": "CRD716",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 1133,
        "title": "Bug: Empty response in interactive mode or incomplete answer",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1133",
        "state": "open",
        "created_at": "2023-04-22T22:16:11Z",
        "user": "batmanonline",
        "labels": []
    },
    {
        "number": 1132,
        "title": "main: add pledge call on OpenBSD",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1132",
        "state": "open",
        "created_at": "2023-04-22T21:13:22Z",
        "user": "codesoap",
        "labels": []
    },
    {
        "number": 1130,
        "title": "The same context, supplied  differently, lead to different outcome",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1130",
        "state": "open",
        "created_at": "2023-04-22T16:08:47Z",
        "user": "sergedc",
        "labels": []
    },
    {
        "number": 1129,
        "title": "Performance issues with cuBLAS and a bug",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1129",
        "state": "open",
        "created_at": "2023-04-22T15:26:30Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 1116,
        "title": "[Binding/UI] Node-RED node and flow for LLaMA",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1116",
        "state": "open",
        "created_at": "2023-04-22T04:54:09Z",
        "user": "rozek",
        "labels": []
    },
    {
        "number": 1112,
        "title": "Error when using Zig : \"no field or member function named 'standardReleaseOptions' in 'Build'\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1112",
        "state": "open",
        "created_at": "2023-04-21T20:59:41Z",
        "user": "TheGreat-Chain",
        "labels": []
    },
    {
        "number": 1103,
        "title": "llama : quantize attention results",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1103",
        "state": "open",
        "created_at": "2023-04-21T14:45:59Z",
        "user": "ggerganov",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 1092,
        "title": "cuBLAS - windows - static not compiling",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1092",
        "state": "open",
        "created_at": "2023-04-20T21:28:05Z",
        "user": "cmp-nct",
        "labels": [
            "bug",
            "build",
            "windows"
        ]
    },
    {
        "number": 1084,
        "title": "'adapter_config.json' and 'adapter_model.bin'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1084",
        "state": "open",
        "created_at": "2023-04-20T17:12:16Z",
        "user": "Dave86ch",
        "labels": []
    },
    {
        "number": 1079,
        "title": "[Bug(CMake 3.17)] nvcc fatal : Unknown option '-mf16c'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1079",
        "state": "open",
        "created_at": "2023-04-20T10:57:18Z",
        "user": "fumiama",
        "labels": []
    },
    {
        "number": 1078,
        "title": "[Bug(CMake 3.17)] CUDA::cublasLt not found but can be specified absolutely",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1078",
        "state": "open",
        "created_at": "2023-04-20T10:52:42Z",
        "user": "fumiama",
        "labels": []
    },
    {
        "number": 1069,
        "title": "Support for Minigpt-4?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1069",
        "state": "open",
        "created_at": "2023-04-19T19:08:38Z",
        "user": "spikespiegel",
        "labels": []
    },
    {
        "number": 1058,
        "title": "Add a option to force the token end of text apears even on interative, and also shows loading porcentage",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1058",
        "state": "open",
        "created_at": "2023-04-19T12:09:09Z",
        "user": "jeffersoncgo",
        "labels": []
    },
    {
        "number": 1048,
        "title": "convert.py fails on the dolly model with KeyError: ('torch', 'ByteStorage')",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1048",
        "state": "open",
        "created_at": "2023-04-18T21:26:39Z",
        "user": "devkral",
        "labels": []
    },
    {
        "number": 1038,
        "title": "config.json for 4bit quantized files",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1038",
        "state": "open",
        "created_at": "2023-04-18T05:59:26Z",
        "user": "philwee",
        "labels": []
    },
    {
        "number": 1022,
        "title": "Add command mode to interactive mode.",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1022",
        "state": "open",
        "created_at": "2023-04-17T07:31:53Z",
        "user": "wbpxre150",
        "labels": []
    },
    {
        "number": 1015,
        "title": "Add mmap pages stats (disabled by default)",
        "url": "https://github.com/ggerganov/llama.cpp/pull/1015",
        "state": "open",
        "created_at": "2023-04-16T16:24:11Z",
        "user": "prusnak",
        "labels": []
    },
    {
        "number": 1010,
        "title": "[User] Interactive mode immediately exits on Windows with Zig",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1010",
        "state": "open",
        "created_at": "2023-04-16T03:52:41Z",
        "user": "karashiiro",
        "labels": []
    },
    {
        "number": 1000,
        "title": "KeyError: 'model.embed_tokens.weight'     when converting .safetensors to ggml",
        "url": "https://github.com/ggerganov/llama.cpp/issues/1000",
        "state": "open",
        "created_at": "2023-04-15T15:19:53Z",
        "user": "Jake36921",
        "labels": []
    },
    {
        "number": 998,
        "title": "accuracy: Q4 matrix multiply error is very bad for small K",
        "url": "https://github.com/ggerganov/llama.cpp/issues/998",
        "state": "open",
        "created_at": "2023-04-15T14:25:56Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 994,
        "title": "testing: allclose operator",
        "url": "https://github.com/ggerganov/llama.cpp/issues/994",
        "state": "open",
        "created_at": "2023-04-15T12:18:27Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 968,
        "title": "Speed differences between vicuna models and the others.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/968",
        "state": "open",
        "created_at": "2023-04-14T14:42:44Z",
        "user": "wro52",
        "labels": []
    },
    {
        "number": 967,
        "title": "Add a param to force the [end of text] to show, even in interactive mode",
        "url": "https://github.com/ggerganov/llama.cpp/issues/967",
        "state": "open",
        "created_at": "2023-04-14T14:00:00Z",
        "user": "jeffersoncgo",
        "labels": []
    },
    {
        "number": 964,
        "title": "Tracking: LoRA",
        "url": "https://github.com/ggerganov/llama.cpp/issues/964",
        "state": "open",
        "created_at": "2023-04-14T09:57:38Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 956,
        "title": "perf(ggml): tall and skinny GEMM for LoRA: F32 `mul_mat([16 X 5120], [16 X 5120])` takes 120ms - 24x slower than expected",
        "url": "https://github.com/ggerganov/llama.cpp/issues/956",
        "state": "open",
        "created_at": "2023-04-14T04:01:56Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 955,
        "title": "Is there any scripts that can convert custom vicuna model?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/955",
        "state": "open",
        "created_at": "2023-04-14T02:12:02Z",
        "user": "yzxyzh",
        "labels": []
    },
    {
        "number": 954,
        "title": "convert-gptq-to-ggml:KeyError: 'model.layers.0.self_attn.q_proj.zeros'",
        "url": "https://github.com/ggerganov/llama.cpp/issues/954",
        "state": "open",
        "created_at": "2023-04-14T01:22:32Z",
        "user": "iMountTai",
        "labels": []
    },
    {
        "number": 946,
        "title": "Question regarding distributed computing...",
        "url": "https://github.com/ggerganov/llama.cpp/issues/946",
        "state": "open",
        "created_at": "2023-04-13T14:03:55Z",
        "user": "snapo",
        "labels": []
    },
    {
        "number": 942,
        "title": "start to present code automatically in interactive mode.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/942",
        "state": "open",
        "created_at": "2023-04-13T08:42:49Z",
        "user": "4t8dd",
        "labels": []
    },
    {
        "number": 941,
        "title": "MSYS2/MinGW64 Text generation continues even when reverse prompts are specified.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/941",
        "state": "open",
        "created_at": "2023-04-13T08:37:24Z",
        "user": "1OOyan",
        "labels": []
    },
    {
        "number": 931,
        "title": "Discussion: Investigate Perf Boosts Through Pruning (DeepSparse)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/931",
        "state": "open",
        "created_at": "2023-04-13T02:05:10Z",
        "user": "MillionthOdin16",
        "labels": []
    },
    {
        "number": 930,
        "title": "discussion: expanding the use-case of llama.cpp - embedded LLM toolchain",
        "url": "https://github.com/ggerganov/llama.cpp/issues/930",
        "state": "open",
        "created_at": "2023-04-13T01:47:05Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 905,
        "title": "feature: linearly interpolating one or multiple LoRA with base model",
        "url": "https://github.com/ggerganov/llama.cpp/issues/905",
        "state": "open",
        "created_at": "2023-04-12T03:04:33Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 904,
        "title": "feature: interactively exporting loaded model to binfile",
        "url": "https://github.com/ggerganov/llama.cpp/issues/904",
        "state": "open",
        "created_at": "2023-04-12T03:02:33Z",
        "user": "jon-chuang",
        "labels": []
    },
    {
        "number": 899,
        "title": "[User] Embedding doesn't seem to work?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/899",
        "state": "open",
        "created_at": "2023-04-11T17:22:57Z",
        "user": "shi-yan",
        "labels": [
            "generation quality"
        ]
    },
    {
        "number": 897,
        "title": "[Enhancement] Implement alternate method in 'timing' to improve 10.7-10.9 compatibility",
        "url": "https://github.com/ggerganov/llama.cpp/issues/897",
        "state": "open",
        "created_at": "2023-04-11T15:24:39Z",
        "user": "Errand24",
        "labels": []
    },
    {
        "number": 895,
        "title": "Why does my program have no output after quantization",
        "url": "https://github.com/ggerganov/llama.cpp/issues/895",
        "state": "open",
        "created_at": "2023-04-11T14:28:46Z",
        "user": "iMountTai",
        "labels": []
    },
    {
        "number": 894,
        "title": "The procedure entry point PrefetchVirtualMemory could not be located in the dynamic link library KERNEL32.dll",
        "url": "https://github.com/ggerganov/llama.cpp/issues/894",
        "state": "open",
        "created_at": "2023-04-11T13:42:44Z",
        "user": "moon91210",
        "labels": []
    },
    {
        "number": 893,
        "title": "[Feature Request] Add batch processing for input prompt data in embedding mode.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/893",
        "state": "open",
        "created_at": "2023-04-11T08:15:29Z",
        "user": "AL-Kost",
        "labels": []
    },
    {
        "number": 881,
        "title": "alloca is not standard c",
        "url": "https://github.com/ggerganov/llama.cpp/issues/881",
        "state": "open",
        "created_at": "2023-04-10T19:15:16Z",
        "user": "Rhialto",
        "labels": []
    },
    {
        "number": 878,
        "title": "[User] Random seed not working correctly, \"too many inputs\"?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/878",
        "state": "open",
        "created_at": "2023-04-10T18:02:58Z",
        "user": "captainzero93",
        "labels": []
    },
    {
        "number": 859,
        "title": "Bug with instruct mode, somehow \"forks\" in background (Windows 11 - Powershell) and makes the shell unuseable",
        "url": "https://github.com/ggerganov/llama.cpp/issues/859",
        "state": "open",
        "created_at": "2023-04-09T00:08:07Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 851,
        "title": "Use Threadpool to schedule the work",
        "url": "https://github.com/ggerganov/llama.cpp/pull/851",
        "state": "open",
        "created_at": "2023-04-08T14:19:55Z",
        "user": "howard0su",
        "labels": [
            "threading"
        ]
    },
    {
        "number": 850,
        "title": "Run several single thread operators parellel",
        "url": "https://github.com/ggerganov/llama.cpp/pull/850",
        "state": "open",
        "created_at": "2023-04-08T13:44:50Z",
        "user": "howard0su",
        "labels": [
            "threading"
        ]
    },
    {
        "number": 849,
        "title": "[ALPACA Q4] assert n_dims in (1, 2) when using migrate-ggml-2023-03-30-pr613.py after convert-gpt4all-to-ggml.py",
        "url": "https://github.com/ggerganov/llama.cpp/issues/849",
        "state": "open",
        "created_at": "2023-04-08T11:14:33Z",
        "user": "DanielWicz",
        "labels": []
    },
    {
        "number": 846,
        "title": "llama : add RWKV models support",
        "url": "https://github.com/ggerganov/llama.cpp/issues/846",
        "state": "open",
        "created_at": "2023-04-08T06:32:31Z",
        "user": "multimediaconverter",
        "labels": [
            "help wanted",
            "good first issue",
            "model"
        ]
    },
    {
        "number": 843,
        "title": "Input Chinese Its talk with itself",
        "url": "https://github.com/ggerganov/llama.cpp/issues/843",
        "state": "open",
        "created_at": "2023-04-08T04:24:28Z",
        "user": "huanxiang666",
        "labels": []
    },
    {
        "number": 842,
        "title": "Performance e-core bug(?) -  only 50% CPU utilization when using all threads -  (Win11, Intel 13900k)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/842",
        "state": "open",
        "created_at": "2023-04-08T03:07:10Z",
        "user": "cmp-nct",
        "labels": []
    },
    {
        "number": 841,
        "title": "How do i stop the ai from talking to itself??",
        "url": "https://github.com/ggerganov/llama.cpp/issues/841",
        "state": "open",
        "created_at": "2023-04-07T23:26:31Z",
        "user": "Asory2010",
        "labels": []
    },
    {
        "number": 837,
        "title": "[Feature Request] Dawn C++ WebGPU backend",
        "url": "https://github.com/ggerganov/llama.cpp/issues/837",
        "state": "open",
        "created_at": "2023-04-07T16:10:18Z",
        "user": "loretoparisi",
        "labels": [
            "enhancement",
            "performance"
        ]
    },
    {
        "number": 835,
        "title": "Q4_0 scale selection using RMSE",
        "url": "https://github.com/ggerganov/llama.cpp/pull/835",
        "state": "open",
        "created_at": "2023-04-07T14:43:55Z",
        "user": "sw",
        "labels": [
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 830,
        "title": "Intermittent segmentation faults in llama_sample_top_p_top_k()",
        "url": "https://github.com/ggerganov/llama.cpp/issues/830",
        "state": "open",
        "created_at": "2023-04-07T12:33:14Z",
        "user": "rlanday",
        "labels": []
    },
    {
        "number": 829,
        "title": "Port to Google Tensor/T2 on Pixel phones",
        "url": "https://github.com/ggerganov/llama.cpp/issues/829",
        "state": "open",
        "created_at": "2023-04-07T10:57:33Z",
        "user": "teaalltr",
        "labels": []
    },
    {
        "number": 828,
        "title": "Do not recreate context while LLama is writing",
        "url": "https://github.com/ggerganov/llama.cpp/issues/828",
        "state": "open",
        "created_at": "2023-04-07T09:41:24Z",
        "user": "janekb04",
        "labels": []
    },
    {
        "number": 826,
        "title": "problem with the ggml files when using the docker images",
        "url": "https://github.com/ggerganov/llama.cpp/issues/826",
        "state": "open",
        "created_at": "2023-04-07T07:16:21Z",
        "user": "dnalexen",
        "labels": []
    },
    {
        "number": 813,
        "title": "Optimize locking behavior",
        "url": "https://github.com/ggerganov/llama.cpp/pull/813",
        "state": "open",
        "created_at": "2023-04-06T14:19:10Z",
        "user": "janekb04",
        "labels": [
            "threading"
        ]
    },
    {
        "number": 811,
        "title": "[Enhancement]: Implement optimizations used in CTranslate2",
        "url": "https://github.com/ggerganov/llama.cpp/issues/811",
        "state": "open",
        "created_at": "2023-04-06T14:13:22Z",
        "user": "janekb04",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 799,
        "title": "What would it take to 100x the context window?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/799",
        "state": "open",
        "created_at": "2023-04-06T03:18:40Z",
        "user": "awfulcooking",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 790,
        "title": "Prompt eval time is counted twice",
        "url": "https://github.com/ggerganov/llama.cpp/issues/790",
        "state": "open",
        "created_at": "2023-04-05T19:59:57Z",
        "user": "sw",
        "labels": [
            "bug"
        ]
    },
    {
        "number": 788,
        "title": "Compilation failed on macOS 10.7-8-9: 'clock_gettime' produce warnings and errors",
        "url": "https://github.com/ggerganov/llama.cpp/issues/788",
        "state": "open",
        "created_at": "2023-04-05T19:16:14Z",
        "user": "Errand24",
        "labels": [
            "bug",
            "build",
            "macos"
        ]
    },
    {
        "number": 787,
        "title": "Changing default repeat_last_n value to current context size?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/787",
        "state": "open",
        "created_at": "2023-04-05T18:18:14Z",
        "user": "rmn20",
        "labels": [
            "enhancement",
            "generation quality"
        ]
    },
    {
        "number": 778,
        "title": "Demo usage of Flash Attention",
        "url": "https://github.com/ggerganov/llama.cpp/pull/778",
        "state": "open",
        "created_at": "2023-04-05T15:44:12Z",
        "user": "ggerganov",
        "labels": [
            "demo"
        ]
    },
    {
        "number": 764,
        "title": "Feature to Discard Last Generated Message in Interactive Chat Mode?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/764",
        "state": "open",
        "created_at": "2023-04-04T13:50:05Z",
        "user": "chigkim",
        "labels": [
            "duplicate",
            "enhancement"
        ]
    },
    {
        "number": 744,
        "title": "Add \"-e\"/\"--eval-threads\" to distinguish thread counts for single-token eval and prompt eval",
        "url": "https://github.com/ggerganov/llama.cpp/pull/744",
        "state": "open",
        "created_at": "2023-04-03T18:19:37Z",
        "user": "MagisterLuddite",
        "labels": [
            "threading"
        ]
    },
    {
        "number": 742,
        "title": "Pythia Support?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/742",
        "state": "open",
        "created_at": "2023-04-03T16:51:43Z",
        "user": "lodorg",
        "labels": []
    },
    {
        "number": 735,
        "title": "I'm pegging CPU (`./examples/chat.sh` works very slowly) on a 5800X3D / u22 linux, anything that can be done?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/735",
        "state": "open",
        "created_at": "2023-04-03T05:38:52Z",
        "user": "robbintt",
        "labels": []
    },
    {
        "number": 727,
        "title": "Using Repeat_last_n and Repeat_penalty To Avoid Going Back and Repeating Itself?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/727",
        "state": "open",
        "created_at": "2023-04-03T00:58:19Z",
        "user": "chigkim",
        "labels": []
    },
    {
        "number": 725,
        "title": "[Feature Request] Support for Filter Assisted Decoding/Constrained Text Generation",
        "url": "https://github.com/ggerganov/llama.cpp/issues/725",
        "state": "open",
        "created_at": "2023-04-02T21:58:38Z",
        "user": "Hellisotherpeople",
        "labels": []
    },
    {
        "number": 714,
        "title": "Add support to FMA3/FMA4 instructions ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/714",
        "state": "open",
        "created_at": "2023-04-02T14:57:14Z",
        "user": "teaalltr",
        "labels": []
    },
    {
        "number": 712,
        "title": "Running llama.cpp on android just prints out the question",
        "url": "https://github.com/ggerganov/llama.cpp/issues/712",
        "state": "open",
        "created_at": "2023-04-02T14:16:00Z",
        "user": "Shreyas-ITB",
        "labels": [
            "android"
        ]
    },
    {
        "number": 707,
        "title": "Convert pytorch-based models to work with llama.cpp?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/707",
        "state": "open",
        "created_at": "2023-04-02T10:53:58Z",
        "user": "IngwiePhoenix",
        "labels": []
    },
    {
        "number": 705,
        "title": "Windows page fault disk i/o slow on first load",
        "url": "https://github.com/ggerganov/llama.cpp/issues/705",
        "state": "open",
        "created_at": "2023-04-02T10:04:24Z",
        "user": "x02Sylvie",
        "labels": [
            "performance",
            "windows"
        ]
    },
    {
        "number": 699,
        "title": "WIndows build fails with -DBUILD_SHARED_LIBS=ON ",
        "url": "https://github.com/ggerganov/llama.cpp/issues/699",
        "state": "open",
        "created_at": "2023-04-02T05:05:28Z",
        "user": "trrahul",
        "labels": [
            "bug",
            "build"
        ]
    },
    {
        "number": 692,
        "title": "How to make llama.cpp return control to add additional context?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/692",
        "state": "open",
        "created_at": "2023-04-01T22:20:36Z",
        "user": "simplejackcoder",
        "labels": [
            "enhancement",
            "generation quality"
        ]
    },
    {
        "number": 676,
        "title": "[Feature Suggestion] Halide-lang codegen / generators",
        "url": "https://github.com/ggerganov/llama.cpp/issues/676",
        "state": "open",
        "created_at": "2023-04-01T11:18:04Z",
        "user": "eszdman",
        "labels": []
    },
    {
        "number": 673,
        "title": "[Feature Suggestion] Dynamic prompt",
        "url": "https://github.com/ggerganov/llama.cpp/issues/673",
        "state": "open",
        "created_at": "2023-04-01T08:05:43Z",
        "user": "edwios",
        "labels": []
    },
    {
        "number": 668,
        "title": "Text Generation Effects in Different Models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/668",
        "state": "open",
        "created_at": "2023-04-01T03:26:38Z",
        "user": "asaka003",
        "labels": []
    },
    {
        "number": 667,
        "title": "[User] examples/chat-13B.sh sometimes continues my question instead of answering",
        "url": "https://github.com/ggerganov/llama.cpp/issues/667",
        "state": "open",
        "created_at": "2023-04-01T02:42:42Z",
        "user": "patrakov",
        "labels": []
    },
    {
        "number": 660,
        "title": "Variable density context windows?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/660",
        "state": "open",
        "created_at": "2023-03-31T20:39:41Z",
        "user": "GeorgeUCB",
        "labels": []
    },
    {
        "number": 633,
        "title": "~2x perf improvement on Apple Silicon by changing state_shared.has_work access from atomic to mutex/conditional",
        "url": "https://github.com/ggerganov/llama.cpp/issues/633",
        "state": "open",
        "created_at": "2023-03-30T19:18:14Z",
        "user": "gjmulder",
        "labels": [
            "enhancement",
            "performance"
        ]
    },
    {
        "number": 630,
        "title": "Combine large LLM with small LLM for faster inference",
        "url": "https://github.com/ggerganov/llama.cpp/issues/630",
        "state": "open",
        "created_at": "2023-03-30T17:54:01Z",
        "user": "ggerganov",
        "labels": [
            "question",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 627,
        "title": "How to activate BLAS?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/627",
        "state": "open",
        "created_at": "2023-03-30T16:56:25Z",
        "user": "BadisG",
        "labels": [
            "need more info"
        ]
    },
    {
        "number": 618,
        "title": "Making a \"quantize-ggml_16bit-to-gptq.py\" script?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/618",
        "state": "open",
        "created_at": "2023-03-30T07:23:54Z",
        "user": "BadisG",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 604,
        "title": "Reverting generated output/user input!",
        "url": "https://github.com/ggerganov/llama.cpp/issues/604",
        "state": "open",
        "created_at": "2023-03-29T18:51:38Z",
        "user": "niansa",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 587,
        "title": "User should be able to return control without inserting a newline",
        "url": "https://github.com/ggerganov/llama.cpp/issues/587",
        "state": "open",
        "created_at": "2023-03-29T04:40:03Z",
        "user": "ScarletEmerald",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 578,
        "title": "|BUG] ggml spawns threads even BLAS is used",
        "url": "https://github.com/ggerganov/llama.cpp/issues/578",
        "state": "open",
        "created_at": "2023-03-28T15:02:01Z",
        "user": "linouxis9",
        "labels": [
            "bug",
            "performance"
        ]
    },
    {
        "number": 557,
        "title": "Perplexity test stopping before the supposed end.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/557",
        "state": "open",
        "created_at": "2023-03-27T15:49:15Z",
        "user": "BadisG",
        "labels": [
            "need more info"
        ]
    },
    {
        "number": 537,
        "title": "Docker Issus ''Illegal instruction''",
        "url": "https://github.com/ggerganov/llama.cpp/issues/537",
        "state": "open",
        "created_at": "2023-03-26T19:18:11Z",
        "user": "Netsuno",
        "labels": [
            "bug",
            "hardware"
        ]
    },
    {
        "number": 534,
        "title": "Build your windows binaries with Clang and not MSVC.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/534",
        "state": "open",
        "created_at": "2023-03-26T17:12:25Z",
        "user": "BadisG",
        "labels": [
            "enhancement",
            "build",
            "windows"
        ]
    },
    {
        "number": 533,
        "title": "Is it possible to avoid printing input when using Alpaca models and prompt from file?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/533",
        "state": "open",
        "created_at": "2023-03-26T16:26:02Z",
        "user": "DanielWicz",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 532,
        "title": "[Feature Suggestion] Load/Save current conversation's tokens into file",
        "url": "https://github.com/ggerganov/llama.cpp/issues/532",
        "state": "open",
        "created_at": "2023-03-26T15:49:50Z",
        "user": "x02Sylvie",
        "labels": [
            "duplicate",
            "enhancement"
        ]
    },
    {
        "number": 528,
        "title": "add support for llama adapters",
        "url": "https://github.com/ggerganov/llama.cpp/issues/528",
        "state": "open",
        "created_at": "2023-03-26T14:28:49Z",
        "user": "redthing1",
        "labels": [
            "enhancement",
            "model"
        ]
    },
    {
        "number": 513,
        "title": "Fails to run inside Docker from Ubuntu 22.04",
        "url": "https://github.com/ggerganov/llama.cpp/issues/513",
        "state": "open",
        "created_at": "2023-03-25T22:42:37Z",
        "user": "tadasgedgaudas",
        "labels": [
            "bug",
            "build"
        ]
    },
    {
        "number": 507,
        "title": "Comparison of Windows Build VS Unix Build (through WSL2)",
        "url": "https://github.com/ggerganov/llama.cpp/issues/507",
        "state": "open",
        "created_at": "2023-03-25T20:09:51Z",
        "user": "BadisG",
        "labels": [
            "question",
            "build"
        ]
    },
    {
        "number": 466,
        "title": "How do we finetune the model with new data?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/466",
        "state": "open",
        "created_at": "2023-03-24T16:12:02Z",
        "user": "ekolawole",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 452,
        "title": "Add support for running bloom models",
        "url": "https://github.com/ggerganov/llama.cpp/issues/452",
        "state": "open",
        "created_at": "2023-03-24T02:26:13Z",
        "user": "bil-ash",
        "labels": [
            "enhancement",
            "model"
        ]
    },
    {
        "number": 413,
        "title": "Mismatch in Vocabulary Size: Investigating Inconsistencies between Token-to-ID and ID-to-Token Dictionaries",
        "url": "https://github.com/ggerganov/llama.cpp/issues/413",
        "state": "open",
        "created_at": "2023-03-23T02:05:32Z",
        "user": "nullhook",
        "labels": [
            "question"
        ]
    },
    {
        "number": 367,
        "title": "The initial token is always empty.",
        "url": "https://github.com/ggerganov/llama.cpp/issues/367",
        "state": "open",
        "created_at": "2023-03-21T19:48:37Z",
        "user": "BadisG",
        "labels": [
            "need more info",
            "generation quality"
        ]
    },
    {
        "number": 364,
        "title": "In interactive/chat mode, sometimes User: does not appear and I need to manually type in my nickname",
        "url": "https://github.com/ggerganov/llama.cpp/issues/364",
        "state": "open",
        "created_at": "2023-03-21T17:24:39Z",
        "user": "x02Sylvie",
        "labels": [
            "generation quality"
        ]
    },
    {
        "number": 359,
        "title": "Converting GGML Q4_0 back to Torch checkpoint for HuggingFace/Pytorch consumption/training/finetuning",
        "url": "https://github.com/ggerganov/llama.cpp/issues/359",
        "state": "open",
        "created_at": "2023-03-21T15:58:07Z",
        "user": "ductai199x",
        "labels": [
            "enhancement",
            "need more info"
        ]
    },
    {
        "number": 353,
        "title": "Improve the Chat Mode with some tricks and considerations",
        "url": "https://github.com/ggerganov/llama.cpp/issues/353",
        "state": "open",
        "created_at": "2023-03-21T13:44:45Z",
        "user": "Belluxx",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 288,
        "title": "RISC-V (TH1520&D1) benchmark and hack for <1GB DDR device",
        "url": "https://github.com/ggerganov/llama.cpp/issues/288",
        "state": "open",
        "created_at": "2023-03-19T10:14:34Z",
        "user": "Zepan",
        "labels": [
            "enhancement",
            "need more info",
            "hardware"
        ]
    },
    {
        "number": 247,
        "title": "How to use ggml for Flan-T5",
        "url": "https://github.com/ggerganov/llama.cpp/issues/247",
        "state": "open",
        "created_at": "2023-03-17T22:38:08Z",
        "user": "i-am-neo",
        "labels": [
            "enhancement",
            "model",
            "generation quality"
        ]
    },
    {
        "number": 231,
        "title": "Study how LM Evaluation Harness works and try to implement it",
        "url": "https://github.com/ggerganov/llama.cpp/issues/231",
        "state": "open",
        "created_at": "2023-03-17T08:32:33Z",
        "user": "ggerganov",
        "labels": [
            "enhancement",
            "high priority",
            "generation quality",
            "research \ud83d\udd2c"
        ]
    },
    {
        "number": 101,
        "title": "M1 Max + GNU coreutils: \"Your arch is announced as x86_64, but it seems to actually be ARM64\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/101",
        "state": "open",
        "created_at": "2023-03-13T19:57:53Z",
        "user": "Birch-san",
        "labels": [
            "bug",
            "hardware",
            "build"
        ]
    },
    {
        "number": 97,
        "title": "WebAssembly and emscripten headers",
        "url": "https://github.com/ggerganov/llama.cpp/issues/97",
        "state": "open",
        "created_at": "2023-03-13T17:27:58Z",
        "user": "loretoparisi",
        "labels": [
            "enhancement"
        ]
    },
    {
        "number": 52,
        "title": "Segmentation Fault Error \"not enough space in the context's memory pool\"",
        "url": "https://github.com/ggerganov/llama.cpp/issues/52",
        "state": "open",
        "created_at": "2023-03-12T16:05:03Z",
        "user": "cannin",
        "labels": [
            "bug",
            "need more info"
        ]
    },
    {
        "number": 34,
        "title": "benchmarks?",
        "url": "https://github.com/ggerganov/llama.cpp/issues/34",
        "state": "open",
        "created_at": "2023-03-12T05:20:58Z",
        "user": "brappier",
        "labels": [
            "documentation",
            "question"
        ]
    },
    {
        "number": 23,
        "title": "Ability for `./main` to keep the model in memory and pass it more text",
        "url": "https://github.com/ggerganov/llama.cpp/issues/23",
        "state": "open",
        "created_at": "2023-03-11T21:00:25Z",
        "user": "simonw",
        "labels": [
            "enhancement"
        ]
    }
]